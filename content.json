{"meta":{"title":"Hexo","subtitle":"","description":"记录我的工作、学习、生活相关的日志，包括linux、k8s、黑苹果、pulsar、Java、golang","author":"Falser","url":"https://falser101.github.io","root":"/"},"pages":[{"title":"","date":"2025-06-16T07:17:39.756Z","updated":"2025-06-16T07:17:39.756Z","comments":true,"path":"404.html","permalink":"https://falser101.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2025-06-16T07:17:39.758Z","updated":"2025-06-16T07:17:39.758Z","comments":true,"path":"about/index.html","permalink":"https://falser101.github.io/about/index.html","excerpt":"","text":"在职程序员，喜欢研究linux、k8s、做一些工具！"},{"title":"所有分类","date":"2025-06-16T07:17:39.758Z","updated":"2025-06-16T07:17:39.758Z","comments":true,"path":"categories/index.html","permalink":"https://falser101.github.io/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2025-06-16T07:17:39.774Z","updated":"2025-06-16T07:17:39.774Z","comments":true,"path":"tags/index.html","permalink":"https://falser101.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"kde全局菜单","slug":"2025/kde全局菜单","date":"2025-06-25T05:58:18.000Z","updated":"2025-06-25T06:08:55.076Z","comments":true,"path":"2025/06/25/2025/kde全局菜单/","permalink":"https://falser101.github.io/2025/06/25/2025/kde%E5%85%A8%E5%B1%80%E8%8F%9C%E5%8D%95/","excerpt":"","text":"Zen-Browser开启全局菜单新建标签页输入 1about:config 搜索widget.gtk.global-menu. 将widget.gtk.global-menu.wayland.enabled和widget.gtk.global-menu.enabled这两个选项修改为true，然后重启浏览器即可看见全局菜单了。","categories":[{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/categories/linux/"}],"tags":[{"name":"kde","slug":"kde","permalink":"https://falser101.github.io/tags/kde/"}]},{"title":"2025技术学习目标","slug":"2025/2025技术学习目标","date":"2025-06-18T08:49:44.000Z","updated":"2025-06-18T08:57:04.288Z","comments":true,"path":"2025/06/18/2025/2025技术学习目标/","permalink":"https://falser101.github.io/2025/06/18/2025/2025%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87/","excerpt":"","text":"2025快过一半了，这半年没什么动力学习，公司股票被ST了，年终奖也不发。简单的定一个下半年的学习目标吧，看看有没有新的机会。 1. 深入Kubernetes学习 存储csi的实现原理 istio基本的应用 2. nixos用了一年多的archlinux，最近看到nixos，感觉挺有意思的，想尝试一下。","categories":[],"tags":[]},{"title":"2024个人年度总结","slug":"2024/2024个人年度总结","date":"2024-12-27T08:44:53.000Z","updated":"2025-06-16T07:17:39.756Z","comments":true,"path":"2024/12/27/2024/2024个人年度总结/","permalink":"https://falser101.github.io/2024/12/27/2024/2024%E4%B8%AA%E4%BA%BA%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/","excerpt":"","text":"生活3月3月底，去了射洪参加大学同学的婚礼，爱情长跑6-7年修成了正果。不得不感叹时间过得真快啊，2020年毕业到现在，工作4年了！难得的能够聚齐耍得好的大学同学些，然后打麻将打的很晚，第二天回去的路上不知道咋回事儿，突然咳嗽了一下，导致我喉咙难受了大半个月。 4-8月去乐山耍了一圈，看了乐山大佛，吃了乐山美食看了巅峰现场演唱会（张碧晨、林宥嘉、王力宏）看了许嵩的演唱会，好多紫色妹妹去了贵州小七孔风景还不错，还有黄果树大瀑布，倒霉的就是把手机搞丢了 10月每年国庆都是和朋友们自驾游，去年去了青海甘肃大环线，风景很好，还可以飙车，开到了170！今年去了贵州赤水大瀑布、广西阳朔竹筏漂流，最值的去的，山水画的感觉，凤凰古城（极其不推荐）,比较感叹的是现在的车机系统确实不错，龙儿开车，我们竟然在车上看完了一部电视剧（天才基本法） 时隔七年再次去重庆，看了邓紫棋的演唱会，去的下午发烧了，然后坚持看完了！ 11月抢到了mate70pro+，之前手机掉了用的女朋友的旧苹果12,然后就升级了next,确实很丝滑，虽然软件适配不是很完善，但对我来说日常够用了，这个月股票涨了很多，最高赚了4个。 12月和朋友们自驾去了毕棚沟，风景甚好，就是有点冷。坐车上山，然后下来选择了走一段路，走了一个小时，不过路上的风景还是不错的。这个月股市不太行，收益掉到了2个多，不过这几天月底又回到了3个多了，再接再厉过个好年 工作之前把我的笔记本刷了个黑苹果，但是风扇声音太大且操作起来感觉到明显的掉帧，我就入手了一台迷你主机，想着用下linux系统，在尝试了并且搜了好几个发行版后选择了archlinux + kde作为主力系统，后面又看到了hyprland,平铺加上好看的动画，直接转成hyprland了。 中间3500块买了台macmini m4试用了下，确实很精致，试用了几天发现已经习惯了linux的开放，然后就退了，以后大概率也不会用mac了。 而且今年感国产软件适配linux越来越好(主要就是腾讯系的软件)，微信（终于用了统一的架构了）、腾讯会议（开源大佬开发的）在wayland下都可以用了;现在就差企业微信了，现在用的steam proton问题还挺多，基本只能用聊天功能，司马腾讯！ 总结生活方面在女朋友的带领下，今年去了很多地方;工作方面，算是按部就班吧，希望明年能多参加点开源贡献。把烂点的牙齿拔了，安个新牙齿！继续操作下股票吧，多搞点钱准备看看车！","categories":[],"tags":[]},{"title":"hyprland下使用steam安装企业微信去掉阴影窗口","slug":"2024/03-hyprland下使用steam安装企业微信去掉阴影窗口","date":"2024-12-23T16:00:00.000Z","updated":"2025-06-18T08:11:24.684Z","comments":true,"path":"2024/12/24/2024/03-hyprland下使用steam安装企业微信去掉阴影窗口/","permalink":"https://falser101.github.io/2024/12/24/2024/03-hyprland%E4%B8%8B%E4%BD%BF%E7%94%A8steam%E5%AE%89%E8%A3%85%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E5%8E%BB%E6%8E%89%E9%98%B4%E5%BD%B1%E7%AA%97%E5%8F%A3/","excerpt":"","text":"使用脚本来获取窗口创建事件，根据窗口的class和宽度高度筛选阴影窗口，调用unmap方法隐藏阴影窗口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566from Xlib import X, displayfrom Xlib.Xatom import WM_NAME, WM_CLASS# 定义需要隐藏的窗口尺寸sizes_to_hide = [ (491, 583), (491, 585), (491, 625), (411, 601), (411, 600), (260, 147), (1045, 745), (502, 406), (1317, 1413), (1273, 38), (192, 24), (466, 616), (1, 1), (795, 821), (1792, 1385), (1686, 1385), (461, 610) ]def handle_event(event): &quot;&quot;&quot;处理窗口创建事件&quot;&quot;&quot; if event.type == X.MapNotify: window_id = event.window window = d.create_resource_object(&#x27;window&#x27;, window_id.id) print(f&quot;处理窗口创建事件, 窗口ID: &#123;window_id&#125;&quot;) # 确保窗口有效性 if window: # 获取窗口类名 wmclass = window.get_full_property(WM_CLASS, 0) if wmclass: wmclass = wmclass.value.decode(&#x27;utf-8&#x27;, &#x27;ignore&#x27;) else: wmclass = &quot;无类名&quot; if wmclass.find(&quot;steam_app_0&quot;) != -1: # 获取窗口的几何信息 try: geometry = window.get_geometry() print(f&quot;窗口宽度: &#123;geometry.width&#125;, 高度: &#123;geometry.height&#125;, x: &#123;geometry.x&#125;, y: &#123;geometry.y&#125;&quot;) for width, height in sizes_to_hide: if geometry.width == width and geometry.height == height: # 关闭窗口 try: window.unmap() print(f&quot;窗口 &#123;window_id&#125; 已关闭&quot;) except Exception as e: print(f&quot;关闭窗口时出错: &#123;e&#125;&quot;) except Exception as e: print(f&quot;获取几何信息时出错: &#123;e&#125;&quot;)# 设置显示和根窗口d = display.Display()root = d.screen().root# 选择事件root.change_attributes(event_mask=X.SubstructureNotifyMask)# 进入事件循环print(&quot;监听新的窗口创建...&quot;)while True: event = d.next_event() handle_event(event)","categories":[{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/tags/linux/"}],"author":"falser101"},{"title":"消息重试的三种方式解析","slug":"2024/消息重试的三种方式解析","date":"2024-11-13T02:14:10.000Z","updated":"2025-06-18T07:00:25.735Z","comments":true,"path":"2024/11/13/2024/消息重试的三种方式解析/","permalink":"https://falser101.github.io/2024/11/13/2024/%E6%B6%88%E6%81%AF%E9%87%8D%E8%AF%95%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F%E8%A7%A3%E6%9E%90/","excerpt":"","text":"要利用消息重新交付，您需要在Broke重新发送Apache Pulsar客户端中未确认的消息之前启用此机制。 可以使用三种方法激活Apache Pulsar中的消息重新传递机制。 Negative Acknowledgment -&gt; 否定确认 Acknowledgment Timeout -&gt; 确认超时 Retry letter topic -&gt; 重试消费 主动重试重试信主题允许您存储未能使用的消息，并在以后重试使用它们。使用此方法，可以自定义重新传递消息的间隔。 原始主题的消费者也会自动订阅重试信主题。一旦达到最大重试次数，未使用的消息将被移动到死信主题进行手动处理。 重试信主题的功能由使用者实现。 使用方式123456789101112131415161718192021PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(&quot;pulsar://localhost:6650&quot;).build();Consumer&lt;byte[]&gt; consumer = pulsarClient.newConsumer() // .topic(&quot;persistent://my-tenant/my-ns/my-topic&quot;) // .enableRetry(true) .subscriptionName(&quot;my-subscription-name&quot;).subscribe();Message&lt;byte[]&gt; msg = null;for (int i = 0; i &lt; 100; i++) &#123; msg = consumer.receive();&#125;// Acknowledge the consumption of all messages at oncetry &#123; consumer.acknowledgeCumulative(msg);&#125; catch (Exception e) &#123; consumer.reconsumeLater(msg, 10, TimeUnit.SECONDS);&#125;pulsarClient.close(); 具体实现Consumer接口 1void reconsumeLater(Message&lt;?&gt; message, long delayTime, TimeUnit unit) throws PulsarClientException; 抽象类ConsumerBase 实现reconsumeLater接口 123456789101112131415161718192021222324252627282930313233343536373839@Overridepublic void reconsumeLater(Message&lt;?&gt; message, long delayTime, TimeUnit unit) throws PulsarClientException &#123; reconsumeLater(message, null, delayTime, unit);&#125;@Overridepublic void reconsumeLater(Message&lt;?&gt; message, Map&lt;String, String&gt; customProperties, long delayTime, TimeUnit unit) throws PulsarClientException &#123; if (!conf.isRetryEnable()) &#123; throw new PulsarClientException(RECONSUME_LATER_ERROR_MSG); &#125; try &#123; reconsumeLaterAsync(message, customProperties, delayTime, unit).get(); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); throw PulsarClientException.unwrap(e); &#125; catch (ExecutionException e) &#123; throw PulsarClientException.unwrap(e); &#125;&#125;@Overridepublic CompletableFuture&lt;Void&gt; reconsumeLaterAsync( Message&lt;?&gt; message, Map&lt;String, String&gt; customProperties, long delayTime, TimeUnit unit) &#123; if (!conf.isRetryEnable()) &#123; return FutureUtil.failedFuture(new PulsarClientException(RECONSUME_LATER_ERROR_MSG)); &#125; try &#123; validateMessageId(message); &#125; catch (PulsarClientException e) &#123; return FutureUtil.failedFuture(e); &#125; return doReconsumeLater(message, AckType.Individual, customProperties, delayTime, unit);&#125;protected abstract CompletableFuture&lt;Void&gt; doReconsumeLater(Message&lt;?&gt; message, AckType ackType, Map&lt;String, String&gt; customProperties, long delayTime, TimeUnit unit); 单分区ConsumerImpl 实现doReconsumeLater抽象方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137protected CompletableFuture&lt;Void&gt; doReconsumeLater(Message&lt;?&gt; message, AckType ackType, Map&lt;String, String&gt; customProperties, long delayTime, TimeUnit unit) &#123; MessageId messageId = message.getMessageId(); if (messageId == null) &#123; return FutureUtil.failedFuture(new PulsarClientException .InvalidMessageException(&quot;Cannot handle message with null messageId&quot;)); &#125; // 检查消费者的状态 if (getState() != State.Ready &amp;&amp; getState() != State.Connecting) &#123; stats.incrementNumAcksFailed(); PulsarClientException exception = new PulsarClientException(&quot;Consumer not ready. State: &quot; + getState()); if (AckType.Individual.equals(ackType)) &#123; onAcknowledge(messageId, exception); &#125; else if (AckType.Cumulative.equals(ackType)) &#123; onAcknowledgeCumulative(messageId, exception); &#125; return FutureUtil.failedFuture(exception); &#125; if (delayTime &lt; 0) &#123; delayTime = 0; &#125; if (retryLetterProducer == null) &#123; createProducerLock.writeLock().lock(); try &#123; // 如果重试生产者为空 创建重试消息生产者 if (retryLetterProducer == null) &#123; retryLetterProducer = client.newProducer(Schema.AUTO_PRODUCE_BYTES(schema)) .topic(this.deadLetterPolicy.getRetryLetterTopic()) .enableBatching(false) .enableChunking(true) .blockIfQueueFull(false) .create(); stats.setRetryLetterProducerStats(retryLetterProducer.getStats()); &#125; &#125; catch (Exception e) &#123; log.error(&quot;Create retry letter producer exception with topic: &#123;&#125;&quot;, deadLetterPolicy.getRetryLetterTopic(), e); return FutureUtil.failedFuture(e); &#125; finally &#123; createProducerLock.writeLock().unlock(); &#125; &#125; CompletableFuture&lt;Void&gt; result = new CompletableFuture&lt;&gt;(); if (retryLetterProducer != null) &#123; try &#123; // 构建重试消息 MessageImpl&lt;T&gt; retryMessage = (MessageImpl&lt;T&gt;) getMessageImpl(message); // 提取消息的原始 messageId String originMessageIdStr = message.getMessageId().toString(); // 主题名（originTopicNameStr） String originTopicNameStr = getOriginTopicNameStr(message); // 并将它们作为属性添加到重试消息中。 SortedMap&lt;String, String&gt; propertiesMap = getPropertiesMap(message, originMessageIdStr, originTopicNameStr); if (customProperties != null) &#123; propertiesMap.putAll(customProperties); &#125; // 并将它们作为属性添加到重试消息中。 int reconsumeTimes = 1; // 如果消息已经包含了 RECONSUMETIMES 属性，说明这是重新消费的消息，增加其计数。 if (propertiesMap.containsKey(RetryMessageUtil.SYSTEM_PROPERTY_RECONSUMETIMES)) &#123; reconsumeTimes = Integer.parseInt( propertiesMap.get(RetryMessageUtil.SYSTEM_PROPERTY_RECONSUMETIMES)); reconsumeTimes = reconsumeTimes + 1; &#125; propertiesMap.put(RetryMessageUtil.SYSTEM_PROPERTY_RECONSUMETIMES, String.valueOf(reconsumeTimes)); propertiesMap.put(RetryMessageUtil.SYSTEM_PROPERTY_DELAY_TIME, String.valueOf(unit.toMillis(delayTime))); MessageId finalMessageId = messageId; if (reconsumeTimes &gt; this.deadLetterPolicy.getMaxRedeliverCount() &amp;&amp; StringUtils.isNotBlank(deadLetterPolicy.getDeadLetterTopic())) &#123; // 超过最大重试次数并发送到死信队列 initDeadLetterProducerIfNeeded(); deadLetterProducer.thenAcceptAsync(dlqProducer -&gt; &#123; TypedMessageBuilder&lt;byte[]&gt; typedMessageBuilderNew = dlqProducer.newMessage(Schema.AUTO_PRODUCE_BYTES(retryMessage.getReaderSchema().get())) .value(retryMessage.getData()) .properties(propertiesMap); typedMessageBuilderNew.sendAsync().thenAccept(msgId -&gt; &#123; consumerDlqMessagesCounter.increment(); doAcknowledge(finalMessageId, ackType, Collections.emptyMap(), null).thenAccept(v -&gt; &#123; result.complete(null); &#125;).exceptionally(ex -&gt; &#123; result.completeExceptionally(ex); return null; &#125;); &#125;).exceptionally(ex -&gt; &#123; result.completeExceptionally(ex); return null; &#125;); &#125;, internalPinnedExecutor).exceptionally(ex -&gt; &#123; result.completeExceptionally(ex); deadLetterProducer = null; return null; &#125;); &#125; else &#123; // 如果消息的重试次数未达到最大限制，则将消息发送到重试队列 assert retryMessage != null; TypedMessageBuilder&lt;byte[]&gt; typedMessageBuilderNew = retryLetterProducer .newMessage(Schema.AUTO_PRODUCE_BYTES(message.getReaderSchema().get())) .value(retryMessage.getData()) .properties(propertiesMap); if (delayTime &gt; 0) &#123; typedMessageBuilderNew.deliverAfter(delayTime, unit); &#125; if (message.hasKey()) &#123; typedMessageBuilderNew.key(message.getKey()); &#125; typedMessageBuilderNew.sendAsync() .thenCompose(__ -&gt; doAcknowledge(finalMessageId, ackType, Collections.emptyMap(), null)) .thenAccept(v -&gt; result.complete(null)) .exceptionally(ex -&gt; &#123; result.completeExceptionally(ex); return null; &#125;); &#125; &#125; catch (Exception e) &#123; result.completeExceptionally(e); &#125; &#125; MessageId finalMessageId = messageId; result.exceptionally(ex -&gt; &#123; log.error(&quot;Send to retry letter topic exception with topic: &#123;&#125;, messageId: &#123;&#125;&quot;, retryLetterProducer.getTopic(), finalMessageId, ex); Set&lt;MessageId&gt; messageIds = Collections.singleton(finalMessageId); unAckedMessageTracker.remove(finalMessageId); redeliverUnacknowledgedMessages(messageIds); return null; &#125;); return result;&#125; 多分区MultiTopicsConsumerImpl实现doReconsumeLater抽象方法 12345678910111213141516171819202122232425262728293031323334@Overrideprotected CompletableFuture&lt;Void&gt; doReconsumeLater(Message&lt;?&gt; message, AckType ackType, Map&lt;String, String&gt; customProperties, long delayTime, TimeUnit unit) &#123; MessageId messageId = message.getMessageId(); if (messageId == null) &#123; return FutureUtil.failedFuture(new PulsarClientException .InvalidMessageException(&quot;Cannot handle message with null messageId&quot;)); &#125; if (!(messageId instanceof TopicMessageId)) &#123; return FutureUtil.failedFuture(new PulsarClientException.NotAllowedException( &quot;Only TopicMessageId is allowed for reconsumeLater for a multi-topics consumer, while messageId is &quot; + message.getClass().getName())); &#125; TopicMessageId topicMessageId = (TopicMessageId) messageId; if (getState() != State.Ready) &#123; return FutureUtil.failedFuture(new PulsarClientException(&quot;Consumer already closed&quot;)); &#125; if (ackType == AckType.Cumulative) &#123; Consumer&lt;T&gt; individualConsumer = consumers.get(topicMessageId.getOwnerTopic()); if (individualConsumer != null) &#123; return individualConsumer.reconsumeLaterCumulativeAsync(message, delayTime, unit); &#125; else &#123; return FutureUtil.failedFuture(new PulsarClientException.NotConnectedException()); &#125; &#125; else &#123; // 实际也是调用单分区的doReconsumeLater方法 ConsumerImpl&lt;T&gt; consumer = consumers.get(topicMessageId.getOwnerTopic()); return consumer.doReconsumeLater(message, ackType, customProperties, delayTime, unit) .thenRun(() -&gt;unAckedMessageTracker.remove(topicMessageId)); &#125;&#125; 否定确认否定确认机制允许您向Broker发送通知，指出消费者没有处理消息。当消费者未能消费消息并需要重新消费它时，消费者向Broker发送否定确认（nack），触发Broker将此消息重新传递给消费者。 在Exclusive和Exclusive订阅类型中，消费者只对他们收到的最后一条消息进行否定确认。 在Shared和Key_Shared订阅类型中，消费者可以分别否定确认消息。 如果要对消息使用否定确认，请确保在确认超时之前进行否定确认 使用方式12345678910111213141516171819202122Consumer&lt;byte[]&gt; consumer = pulsarClient.newConsumer() .topic(topic) .subscriptionName(&quot;sub-negative-ack&quot;) .subscriptionInitialPosition(SubscriptionInitialPosition.Earliest) // 以不同的延迟来 .negativeAckRedeliveryBackoff(MultiplierRedeliveryBackoff.builder() .minDelayMs(1000) .maxDelayMs(60 * 1000) .multiplier(2) .build()) .subscribe();while (true) &#123; Message&lt;String&gt; msg = consumer.receive(); try &#123; // Process message... consumer.acknowledge(msg); &#125; catch (Throwable t) &#123; log.warn(&quot;Failed to process message&quot;); consumer.negativeAcknowledge(msg); &#125;&#125; 具体实现Consumer接口 1void negativeAcknowledge(Message&lt;?&gt; message); ConsumerImpl实现 123456789@Overridepublic void negativeAcknowledge(MessageId messageId) &#123; consumerNacksCounter.increment(); // 添加到否定确认跟踪器中 negativeAcksTracker.add(messageId); // Ensure the message is not redelivered for ack-timeout, since we did receive an &quot;ack&quot; unAckedMessageTracker.remove(messageId);&#125; NegativeAcksTracker主要实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485class NegativeAcksTracker implements Closeable &#123; private static final Logger log = LoggerFactory.getLogger(NegativeAcksTracker.class); private HashMap&lt;MessageId, Long&gt; nackedMessages = null; private final ConsumerBase&lt;?&gt; consumer; private final Timer timer; private final long nackDelayNanos; private final long timerIntervalNanos; private final RedeliveryBackoff negativeAckRedeliveryBackoff; private Timeout timeout; // Set a min delay to allow for grouping nacks within a single batch private static final long MIN_NACK_DELAY_NANOS = TimeUnit.MILLISECONDS.toNanos(100); public NegativeAcksTracker(ConsumerBase&lt;?&gt; consumer, ConsumerConfigurationData&lt;?&gt; conf) &#123; this.consumer = consumer; this.timer = consumer.getClient().timer(); this.nackDelayNanos = Math.max(TimeUnit.MICROSECONDS.toNanos(conf.getNegativeAckRedeliveryDelayMicros()), MIN_NACK_DELAY_NANOS); this.negativeAckRedeliveryBackoff = conf.getNegativeAckRedeliveryBackoff(); if (negativeAckRedeliveryBackoff != null) &#123; this.timerIntervalNanos = Math.max( TimeUnit.MILLISECONDS.toNanos(negativeAckRedeliveryBackoff.next(0)), MIN_NACK_DELAY_NANOS) / 3; &#125; else &#123; this.timerIntervalNanos = nackDelayNanos / 3; &#125; &#125; private synchronized void triggerRedelivery(Timeout t) &#123; // 检查是否有消息需要重新投递。如果没有，设置timeout为null并退出 if (nackedMessages.isEmpty()) &#123; this.timeout = null; return; &#125; // Group all the nacked messages into one single re-delivery request Set&lt;MessageId&gt; messagesToRedeliver = new HashSet&lt;&gt;(); long now = System.nanoTime(); nackedMessages.forEach((msgId, timestamp) -&gt; &#123; // 检查每条消息的时间戳是否已到。如果已到，将其添加到messagesToRedeliver集合中 if (timestamp &lt; now) &#123; addChunkedMessageIdsAndRemoveFromSequenceMap(msgId, messagesToRedeliver, this.consumer); messagesToRedeliver.add(msgId); &#125; &#125;); // 移除并调用consumer.onNegativeAcksSend(messagesToRedeliver)和consumer.redeliverUnacknowledgedMessages(messagesToRedeliver)来处理重发 if (!messagesToRedeliver.isEmpty()) &#123; messagesToRedeliver.forEach(nackedMessages::remove); consumer.onNegativeAcksSend(messagesToRedeliver); log.info(&quot;[&#123;&#125;] &#123;&#125; messages will be re-delivered&quot;, consumer, messagesToRedeliver.size()); // 调用重新投递方法 consumer.redeliverUnacknowledgedMessages(messagesToRedeliver); &#125; this.timeout = timer.newTimeout(this::triggerRedelivery, timerIntervalNanos, TimeUnit.NANOSECONDS); &#125; private synchronized void add(MessageId messageId, int redeliveryCount) &#123; if (nackedMessages == null) &#123; nackedMessages = new HashMap&lt;&gt;(); &#125; long backoffNs; if (negativeAckRedeliveryBackoff != null) &#123; backoffNs = TimeUnit.MILLISECONDS.toNanos(negativeAckRedeliveryBackoff.next(redeliveryCount)); &#125; else &#123; backoffNs = nackDelayNanos; &#125; // 将消息添加到否定确认消息map中，并设置超时时间戳 nackedMessages.put(MessageIdAdvUtils.discardBatch(messageId), System.nanoTime() + backoffNs); if (this.timeout == null) &#123; // Schedule a task and group all the redeliveries for same period. Leave a small buffer to allow for // nack immediately following the current one will be batched into the same redeliver request. // 定时任务触发重新投递 this.timeout = timer.newTimeout(this::triggerRedelivery, timerIntervalNanos, TimeUnit.NANOSECONDS); &#125; &#125;&#125; 未确认消息跟踪器使用方式123456789Consumer&lt;byte[]&gt; consumer = pulsarClient.newConsumer() // .topic(&quot;persistent://my-tenant/my-ns/my-topic&quot;) // .ackTimeout(5, TimeUnit.SECONDS) .ackTimeoutRedeliveryBackoff(MultiplierRedeliveryBackoff.builder() .minDelayMs(1000) .maxDelayMs(60000) .multiplier(2) .build()) .subscriptionName(&quot;my-subscription-name&quot;).subscribe(); 以上的ackTimeoutRedeliveryBackoff配置，消息重新传递行为应如下所示 重新投递次数 重新投递延迟 1 1 seconds 2 2 seconds 3 4 seconds 4 8 seconds 5 16 seconds 6 32 seconds 7 60 seconds 8 60 seconds 每当客户端接收到消息时ConsumerImpl.messageReceived() -&gt; executeNotifyCallback() -&gt; messageProcessed() -&gt; trackMessage() -&gt; trackUnAckedMsgIfNoListener() 将消息加入未确认消息跟踪器中 在ConsumerBase中初始化unAckedMessageTracker 123456789if (conf.getAckTimeoutMillis() != 0) &#123; if (conf.getAckTimeoutRedeliveryBackoff() != null) &#123; this.unAckedMessageTracker = new UnAckedTopicMessageRedeliveryTracker(client, this, conf); &#125; else &#123; this.unAckedMessageTracker = new UnAckedTopicMessageTracker(client, this, conf); &#125;&#125; else &#123; this.unAckedMessageTracker = UnAckedMessageTracker.UNACKED_MESSAGE_TRACKER_DISABLED;&#125; 具体实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138public class UnAckedMessageTracker implements Closeable &#123; // UnAckedMessageTracker 的属性 // 消息ID-消息集（为了更好的针对消息ID索引） protected final HashMap&lt;MessageId, HashSet&lt;MessageId&gt;&gt; messageIdPartitionMap; // 双向列表 用于时间分区 protected final ArrayDeque&lt;HashSet&lt;MessageId&gt;&gt; timePartitions; // 读写锁 protected final Lock readLock; protected final Lock writeLock; // UnAckedMessageTracker 的空实现，用于非持久化消息组件 public static final UnAckedMessageTrackerDisabled UNACKED_MESSAGE_TRACKER_DISABLED = new UnAckedMessageTrackerDisabled(); // 确认超时时间 protected final long ackTimeoutMillis; // 定时器的触发间隔 protected final long tickDurationInMs; // 消费者ack超时计数器 private final Counter consumerAckTimeoutsCounter; protected Timeout timeout; public UnAckedMessageTracker(PulsarClientImpl client, ConsumerBase&lt;?&gt; consumerBase, ConsumerConfigurationData&lt;?&gt; conf) &#123; // 超时时间 this.ackTimeoutMillis = conf.getAckTimeoutMillis(); // 定时器的间隔时间 this.tickDurationInMs = Math.min(conf.getTickDurationMillis(), conf.getAckTimeoutMillis()); // 定时器的触发间隔必须大于0，并且确认超时时间大于等于单位时间 checkArgument(tickDurationInMs &gt; 0 &amp;&amp; ackTimeoutMillis &gt;= tickDurationInMs); ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock(); this.readLock = readWriteLock.readLock(); this.writeLock = readWriteLock.writeLock(); InstrumentProvider ip = client.instrumentProvider(); consumerAckTimeoutsCounter = ip.newCounter(&quot;pulsar.client.consumer.message.ack.timeout&quot;, Unit.Messages, &quot;The number of messages that were not acknowledged in the configured timeout period, hence, were &quot; + &quot;requested by the client to be redelivered&quot;, consumerBase.getTopic(), Attributes.builder().put(&quot;pulsar.subscription&quot;, consumerBase.getSubscription()).build()); // 如果不为空则初始化为UnAckedTopicMessageRedeliveryTracker if (conf.getAckTimeoutRedeliveryBackoff() == null) &#123; // 这里主要用于存储消息ID和消息的映射 this.messageIdPartitionMap = new HashMap&lt;&gt;(); // 从头部移除，尾部追加数据 this.timePartitions = new ArrayDeque&lt;&gt;(); // 设置一个时间分区，假设确认超时时间为11000ms，单位时间为2000ms，此时计算结果为5 int blankPartitions = (int) Math.ceil((double) this.ackTimeoutMillis / this.tickDurationInMs); // 创建6个时间分区容器 for (int i = 0; i &lt; blankPartitions + 1; i++) &#123; timePartitions.add(new HashSet&lt;&gt;(16, 1)); &#125; // 新增一个定时器任务，tickDurationInMs 为轮询间隔 // 也就是这里会处理消息消费超时，并通知broker进行重新投递 timeout = client.timer().newTimeout(new TimerTask() &#123; @Override public void run(Timeout t) throws Exception &#123; if (t.isCancelled()) &#123; return; &#125; Set&lt;MessageId&gt; messageIds = TL_MESSAGE_IDS_SET.get(); messageIds.clear(); writeLock.lock(); try &#123; // 取出并移除队列头部的分区 HashSet&lt;MessageId&gt; headPartition = timePartitions.removeFirst(); // 如果头部分区不为空 if (!headPartition.isEmpty()) &#123; consumerAckTimeoutsCounter.add(headPartition.size()); log.info(&quot;[&#123;&#125;] &#123;&#125; messages will be re-delivered&quot;, consumerBase, headPartition.size()); headPartition.forEach(messageId -&gt; &#123; // 分片消息 if (messageId instanceof ChunkMessageIdImpl) &#123; addChunkedMessageIdsAndRemoveFromSequenceMap(messageId, messageIds, consumerBase); &#125; else &#123; messageIds.add(messageId); &#125; // 从消息id-分区map中删除 messageIdPartitionMap.remove(messageId); &#125;); &#125; // 清空头部分区 headPartition.clear(); // 加入队列的尾部 timePartitions.addLast(headPartition); &#125; finally &#123; try &#123; timeout = client.timer().newTimeout(this, tickDurationInMs, TimeUnit.MILLISECONDS); &#125; finally &#123; writeLock.unlock(); if (!messageIds.isEmpty()) &#123; consumerBase.onAckTimeoutSend(messageIds); consumerBase.redeliverUnacknowledgedMessages(messageIds); &#125; &#125; &#125; &#125; &#125;, this.tickDurationInMs, TimeUnit.MILLISECONDS); &#125; else &#123; this.messageIdPartitionMap = null; this.timePartitions = null; &#125; &#125; /*添加逻辑分析 为什么加入尾部分区？ 最近添加的消息：未确认的消息是按照它们的添加时间进行管理的。最近添加的消息应该被放置在队列的尾部分区中，因为这些消息离超时的阈值还很远，不需要马上进行超时检查。 超时检查顺序：定时器会从头部分区开始检查未确认消息。如果新的消息被加入到尾部分区，就可以保证这些消息在未来的检查周期内才会被考虑超时，而不是立即被处理。 添加的具体操作 在 add(MessageId messageId) 方法中： 使用 timePartitions.peekLast() 获取 ArrayDeque 中的尾部分区。 将新的 MessageId 加入这个尾部分区中。 */ public boolean add(MessageId messageId) &#123; writeLock.lock(); try &#123; // 取出队列尾部的集合 HashSet&lt;MessageId&gt; partition = timePartitions.peekLast(); // 如果原map不存在映射则将消息id-分区的映射放入messageIdPartitionMap HashSet&lt;MessageId&gt; previousPartition = messageIdPartitionMap.putIfAbsent(messageId, partition); if (previousPartition == null) &#123; // 之前不存在映射，则将消息id放入队尾分区的集合中 return partition.add(messageId); &#125; else &#123; // 之前存在映射，不做处理 return false; &#125; &#125; finally &#123; writeLock.unlock(); &#125; &#125;&#125; UnAckedMessageRedeliveryTracker实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115public class UnAckedMessageRedeliveryTracker extends UnAckedMessageTracker &#123; private static final Logger log = LoggerFactory.getLogger(UnAckedMessageRedeliveryTracker.class); protected final HashMap&lt;UnackMessageIdWrapper, HashSet&lt;UnackMessageIdWrapper&gt;&gt; redeliveryMessageIdPartitionMap; protected final ArrayDeque&lt;HashSet&lt;UnackMessageIdWrapper&gt;&gt; redeliveryTimePartitions; protected final HashMap&lt;MessageId, Long&gt; ackTimeoutMessages; private final RedeliveryBackoff ackTimeoutRedeliveryBackoff; public UnAckedMessageRedeliveryTracker(PulsarClientImpl client, ConsumerBase&lt;?&gt; consumerBase, ConsumerConfigurationData&lt;?&gt; conf) &#123; super(client, consumerBase, conf); this.ackTimeoutRedeliveryBackoff = conf.getAckTimeoutRedeliveryBackoff(); this.ackTimeoutMessages = new HashMap&lt;MessageId, Long&gt;(); this.redeliveryMessageIdPartitionMap = new HashMap&lt;&gt;(); this.redeliveryTimePartitions = new ArrayDeque&lt;&gt;(); int blankPartitions = (int) Math.ceil((double) this.ackTimeoutMillis / this.tickDurationInMs); for (int i = 0; i &lt; blankPartitions + 1; i++) &#123; redeliveryTimePartitions.add(new HashSet&lt;&gt;(16, 1)); &#125; timeout = client.timer().newTimeout(new TimerTask() &#123; @Override public void run(Timeout t) throws Exception &#123; writeLock.lock(); try &#123; HashSet&lt;UnackMessageIdWrapper&gt; headPartition = redeliveryTimePartitions.removeFirst(); if (!headPartition.isEmpty()) &#123; headPartition.forEach(unackMessageIdWrapper -&gt; &#123; addAckTimeoutMessages(unackMessageIdWrapper); redeliveryMessageIdPartitionMap.remove(unackMessageIdWrapper); unackMessageIdWrapper.recycle(); &#125;); &#125; headPartition.clear(); redeliveryTimePartitions.addLast(headPartition); triggerRedelivery(consumerBase); &#125; finally &#123; writeLock.unlock(); timeout = client.timer().newTimeout(this, tickDurationInMs, TimeUnit.MILLISECONDS); &#125; &#125; &#125;, this.tickDurationInMs, TimeUnit.MILLISECONDS); &#125; // 添加超时消息 private void addAckTimeoutMessages(UnackMessageIdWrapper messageIdWrapper) &#123; writeLock.lock(); try &#123; MessageId messageId = messageIdWrapper.getMessageId(); int redeliveryCount = messageIdWrapper.getRedeliveryCount(); // 计算下次超时时间 long backoffNs = ackTimeoutRedeliveryBackoff.next(redeliveryCount); // 消息id：超时时间的映射 ackTimeoutMessages.put(messageId, System.currentTimeMillis() + backoffNs); &#125; finally &#123; writeLock.unlock(); &#125; &#125; private void triggerRedelivery(ConsumerBase&lt;?&gt; consumerBase) &#123; if (ackTimeoutMessages.isEmpty()) &#123; return; &#125; Set&lt;MessageId&gt; messageIds = TL_MESSAGE_IDS_SET.get(); messageIds.clear(); try &#123; long now = System.currentTimeMillis(); ackTimeoutMessages.forEach((messageId, timestamp) -&gt; &#123; // 时间戳小于等于当前时间则加入重投递消息集合中 if (timestamp &lt;= now) &#123; addChunkedMessageIdsAndRemoveFromSequenceMap(messageId, messageIds, consumerBase); messageIds.add(messageId); &#125; &#125;); if (!messageIds.isEmpty()) &#123; log.info(&quot;[&#123;&#125;] &#123;&#125; messages will be re-delivered&quot;, consumerBase, messageIds.size()); Iterator&lt;MessageId&gt; iterator = messageIds.iterator(); while (iterator.hasNext()) &#123; MessageId messageId = iterator.next(); ackTimeoutMessages.remove(messageId); &#125; &#125; &#125; finally &#123; if (messageIds.size() &gt; 0) &#123; consumerBase.onAckTimeoutSend(messageIds); // 触发重投递 consumerBase.redeliverUnacknowledgedMessages(messageIds); &#125; &#125; &#125; @Override public boolean add(MessageId messageId, int redeliveryCount) &#123; writeLock.lock(); try &#123; UnackMessageIdWrapper messageIdWrapper = UnackMessageIdWrapper.valueOf(messageId, redeliveryCount); HashSet&lt;UnackMessageIdWrapper&gt; partition = redeliveryTimePartitions.peekLast(); HashSet&lt;UnackMessageIdWrapper&gt; previousPartition = redeliveryMessageIdPartitionMap .putIfAbsent(messageIdWrapper, partition); if (previousPartition == null) &#123; return partition.add(messageIdWrapper); &#125; else &#123; messageIdWrapper.recycle(); return false; &#125; &#125; finally &#123; writeLock.unlock(); &#125; &#125;&#125;","categories":[],"tags":[],"author":"falser101"},{"title":"消费者接收消息","slug":"2024/消费者接收消息","date":"2024-10-30T07:25:09.000Z","updated":"2025-06-18T07:00:25.173Z","comments":true,"path":"2024/10/30/2024/消费者接收消息/","permalink":"https://falser101.github.io/2024/10/30/2024/%E6%B6%88%E8%B4%B9%E8%80%85%E6%8E%A5%E6%94%B6%E6%B6%88%E6%81%AF/","excerpt":"","text":"消费者接收消息的API非常简单，根据前面的消费者简单例子，如下： 1Message&lt;byte[]&gt; msg = consumer.receive(); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121@Overridepublic Message&lt;T&gt; receive() throws PulsarClientException &#123; if (listener != null) &#123; throw new PulsarClientException.InvalidConfigurationException( &quot;Cannot use receive() when a listener has been set&quot;); &#125; // 作一些状态检查。 verifyConsumerState(); return internalReceive();&#125;private void verifyConsumerState() throws PulsarClientException &#123; switch (getState()) &#123; case Ready: case Connecting: break; // Ok case Closing: case Closed: throw new PulsarClientException.AlreadyClosedException(&quot;Consumer already closed&quot;); case Terminated: throw new PulsarClientException.AlreadyClosedException(&quot;Topic was terminated&quot;); case Failed: case Uninitialized: throw new PulsarClientException.NotConnectedException(); default: break; &#125;&#125;// 抽象方法 直接看ConsumerImpl的实现protected abstract Message&lt;T&gt; internalReceive() throws PulsarClientException; @Overrideprotected Message&lt;T&gt; internalReceive() throws PulsarClientException &#123; Message&lt;T&gt; message; try &#123; // 如果消息队列为空 根据内存使用情况扩大receiver队列的大小 if (incomingMessages.isEmpty()) &#123; expectMoreIncomingMessages(); &#125; // 消息队列阻塞调用，等待消息到来 message = incomingMessages.take(); messageProcessed(message); return beforeConsume(message); &#125; catch (InterruptedException e) &#123; ExceptionHandler.handleInterruptedException(e); stats.incrementNumReceiveFailed(); throw PulsarClientException.unwrap(e); &#125;&#125;// 扩大receiver队列的大小protected void expectMoreIncomingMessages() &#123; if (!conf.isAutoScaledReceiverQueueSizeEnabled()) &#123; return; &#125; double usage = getMemoryLimitController().map(MemoryLimitController::currentUsagePercent).orElse(0d); if (usage &lt; MEMORY_THRESHOLD_FOR_RECEIVER_QUEUE_SIZE_EXPANSION &amp;&amp; scaleReceiverQueueHint.compareAndSet(true, false)) &#123; int oldSize = getCurrentReceiverQueueSize(); int newSize = Math.min(maxReceiverQueueSize, oldSize * 2); setCurrentReceiverQueueSize(newSize); &#125;&#125;/** * Record the event that one message has been processed by the application. * * Periodically, it sends a Flow command to notify the broker that it can push more messages */@Overrideprotected synchronized void messageProcessed(Message&lt;?&gt; msg) &#123; ClientCnx currentCnx = cnx(); ClientCnx msgCnx = ((MessageImpl&lt;?&gt;) msg).getCnx(); lastDequeuedMessageId = msg.getMessageId(); messagesPrefetchedGauge.decrement(); messagesReceivedCounter.increment(); bytesPrefetchedGauge.subtract(msg.size()); bytesReceivedCounter.add(msg.size()); if (msgCnx != currentCnx) &#123; // The processed message did belong to the old queue that was cleared after reconnection. &#125; else &#123; if (listener == null &amp;&amp; !parentConsumerHasListener) &#123; increaseAvailablePermits(currentCnx); &#125; stats.updateNumMsgsReceived(msg); trackMessage(msg); &#125; decreaseIncomingMessageSize(msg);&#125;protected void trackMessage(MessageId messageId, int redeliveryCount) &#123; if (conf.getAckTimeoutMillis() &gt; 0 &amp;&amp; messageId instanceof MessageIdImpl) &#123; MessageId id = MessageIdAdvUtils.discardBatch(messageId); if (hasParentConsumer) &#123; //TODO: check parent consumer here // we should no longer track this message, TopicsConsumer will take care from now onwards unAckedMessageTracker.remove(id); &#125; else &#123; trackUnAckedMsgIfNoListener(id, redeliveryCount); &#125; &#125;&#125;// 增加可用许可protected void increaseAvailablePermits(ClientCnx currentCnx, int delta) &#123; int available = AVAILABLE_PERMITS_UPDATER.addAndGet(this, delta); while (available &gt;= getCurrentReceiverQueueSize() / 2 &amp;&amp; !paused) &#123; if (AVAILABLE_PERMITS_UPDATER.compareAndSet(this, available, 0)) &#123; // 根据可用许可，通知 broker 推送消息 sendFlowPermitsToBroker(currentCnx, available); break; &#125; else &#123; available = AVAILABLE_PERMITS_UPDATER.get(this); &#125; &#125;&#125; 123456789101112@Overrideprotected void handleMessage(CommandMessage cmdMessage, ByteBuf headersAndPayload) &#123; checkArgument(state == State.Ready); if (log.isDebugEnabled()) &#123; log.debug(&quot;&#123;&#125; Received a message from the server: &#123;&#125;&quot;, ctx.channel(), cmdMessage); &#125; ConsumerImpl&lt;?&gt; consumer = consumers.get(cmdMessage.getConsumerId()); if (consumer != null) &#123; consumer.messageReceived(cmdMessage, headersAndPayload, this); &#125;&#125;","categories":[],"tags":[],"author":"falser101"},{"title":"prometheus告警实战","slug":"2024/prometheus告警实战","date":"2024-09-09T16:00:00.000Z","updated":"2025-06-18T07:00:26.181Z","comments":true,"path":"2024/09/10/2024/prometheus告警实战/","permalink":"https://falser101.github.io/2024/09/10/2024/prometheus%E5%91%8A%E8%AD%A6%E5%AE%9E%E6%88%98/","excerpt":"","text":"配置alertmanager配置12345678910111213141516171819202122global: resolve_timeout: 5m # 设置解决（resolve）告警的超时时间为5分钟。route: group_by: [&#x27;alertname&#x27;] # 告警按照 alertname 分组。 group_wait: 10s # 每个分组等待10秒，以便将相关的告警聚合在一起。 group_interval: 10s # 每个分组之间的间隔时间为10秒。 repeat_interval: 1h # 告警的重复间隔时间为1小时。 receiver: &#x27;webhook&#x27; # 默认的接收器是 &#x27;webhook&#x27;。 routes: - match: cluster: &#x27;tlq-cn&#x27; # 满足条件的使用这个接收器 receiver: &#x27;webhook&#x27;receivers: # 定义接收器- name: &#x27;webhook&#x27; webhook_configs: - url: &#x27;http://127.0.0.1:5001/&#x27;inhibit_rules: # 定义抑制规则，用于在某些条件下抑制告警 - source_match: severity: &#x27;critical&#x27; # 如果源告警的严重性为 &#x27;critical&#x27;。 target_match: severity: &#x27;warning&#x27; # 如果目标告警的严重性为 &#x27;warning&#x27; equal: [&#x27;alertname&#x27;, &#x27;dev&#x27;, &#x27;instance&#x27;] # 源告警和目标告警必须在这些标签上相等才会触发抑制。 Prometheus配置12345678910111213141516171819202122232425262728293031323334353637global: scrape_interval: 15s # 采集间隔为15秒。 evaluation_interval: 15s # 评估间隔为15秒。rule_files: - /home/prometheus/rule/*.yml # 下面配置的告警规则文件路径alerting: alert_relabel_configs: # : 配置了动态修改 alert 属性的规则。 - source_labels: [dc] # 使用标签 dc 作为源标签。 regex: (.+)\\d+ # 使用正则表达式从源标签中提取数据，并且去掉结尾的数字。 target_label: dc1 # 将提取的数据赋值给目标标签 dc1。 alertmanagers: - static_configs: # 静态配置，指定了 Alertmanager 的地址为 localhost:9093。 - targets: - localhost:9093scrape_configs:# 联邦集群配置- job_name: &#x27;federate&#x27; scrape_interval: 15s honor_labels: true metrics_path: &#x27;/federate&#x27; params: &#x27;match[]&#x27;: - &#x27;&#123;job=&quot;prometheus&quot;&#125;&#x27; - &#x27;&#123;job=&quot;broker&quot;&#125;&#x27; - &#x27;&#123;job=&quot;bookie&quot;&#125;&#x27; - &#x27;&#123;job=&quot;zookeeper&quot;&#125;&#x27; - &#x27;&#123;job=&quot;proxy&quot;&#125;&#x27; - &#x27;&#123;job=&quot;node-exporter&quot;&#125;&#x27; static_configs: - targets: - &#x27;10.10.22.162:52119&#x27;- job_name: &#x27;prometheus&#x27; # 任务名称。 static_configs: - targets: [&#x27;localhost:9090&#x27;] # 采集prometheus自身。- job_name: &#x27;node&#x27; # 任务名称。 static_configs: - targets: [&#x27;localhost:9100&#x27;] # 采集本机。 告警规则1234567891011groups: - name: alert.rules # 告警规则组名称 rules: - alert: InstanceDown # 告警的名称，命名为 InstanceDown expr: up == 0 # 触发告警的表达式，如果 up 的值为 0，表示实例处于宕机状态。 for: 1m # 即当满足条件并持续1分钟时触发告警 labels: # 标签，用于标识告警的一些元信息 severity: critical # 告警的严重性标签，设置为 &#x27;critical&#x27;。 annotations: # 注释，提供更详细的描述信息。 summary: &quot;&#123;&#123; $labels.instance &#125;&#125;: no data for 1 minute&quot; # 摘要信息，描述实例在1分钟内没有数据。 description: &quot;&#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125; has been down for more than 1 minute.&quot; # 描述信息，指明哪个作业的哪个实例在1分钟以上处于宕机状态。 启动服务启动 Prometheus参数--web.enable-lifecycle的作用是启用 Prometheus 的热加载功能，允许通过 HTTP 接口动态加载和卸载规则文件。当我们修改了 Prometheus 的配置文件或者规则文件，需要重新加载配置文件，可以通过 POST 请求 /-/reload 接口实现。 12# 启动服务./prometheus --config.file=prometheus.yml --web.enable-lifecycle 启动 Alertmanager12# 启动服务./alertmanager --config.file=alertmanager.yml 当我们修改了Alertmanager的配置文件，需要重新加载配置文件，可以通过 POST 请求 /-/reload 接口实现。 查看Prometheus在浏览器输入localhost:9090，即可看到Prometheus的监控页面。点击alerts，可以看到当前告警信息。如下图所示 状态值 Inactive（未激活）：Alert 的初始状态，表示规则条件尚未满足。 Pending（等待）：表示 Alert 已经被触发，但是在确认一定时间内保持在 Pending 状态，以防止短时间内的噪声或瞬时问题。 Firing（触发）：表示 Alert 已经被确认，规则条件持续满足。 热加载修改Prometheus规则文件,将expr表达式修改up&#123;job=&#39;broker&#39;&#125; 1234567891011groups: - name: alert.rules # 告警规则组名称 rules: - alert: InstanceDown # 告警的名称，命名为 InstanceDown expr: up&#123;job=&#x27;broker&#x27;&#125; == 1 # 触发告警的表达式，如果 up&#123;job=&#x27;broker&#x27;&#125; 的值为 1，表示实例处于启动状态。 for: 1m # 即当满足条件并持续1分钟时触发告警 labels: # 标签，用于标识告警的一些元信息 severity: critical # 告警的严重性标签，设置为 &#x27;critical&#x27;。 annotations: # 注释，提供更详细的描述信息。 summary: &quot;&#123;&#123; $labels.instance &#125;&#125;: no data for 1 minute&quot; # 摘要信息，描述实例在1分钟内没有数据。 description: &quot;&#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125; has been down for more than 1 minute.&quot; # 描述信息，指明哪个作业的哪个实例在1分钟以上处于宕机状态。 修改后调用POST请求http://localhost:9090/-/reload，即可更新Prometheus的规则文件。 1curl -X POST http://localhost:9090/-/reload 再次查看alerts，可以看到告警信息已经更新了。 查看Alertmanager在浏览器输入localhost:9093，即可看到Alertmanager的监控页面。点击alerts，可以看到当前告警信息。如下图所示 可以看到告警信息已经发送到Alertmanager，并且被Alertmanager处理了。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://falser101.github.io/categories/k8s/"}],"tags":[{"name":"监控告警","slug":"监控告警","permalink":"https://falser101.github.io/tags/%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/"}]},{"title":"archlinux安装waydroid","slug":"2024/01.archlinux安装waydroid","date":"2024-08-13T16:00:00.000Z","updated":"2025-06-25T06:08:33.083Z","comments":true,"path":"2024/08/14/2024/01.archlinux安装waydroid/","permalink":"https://falser101.github.io/2024/08/14/2024/01.archlinux%E5%AE%89%E8%A3%85waydroid/","excerpt":"","text":"archlinux安装waydroid 安装1paru -S waydroid 下载镜像1paru -S waydroid-image-gapps 初始化1sudo waydroid init -s GAPPS -f 连接12waydroid session startwaydroid show-full-ui 安装应用1sudo waydroid app install xx.apk QA打开浏览器发现无网络连接A: 使用iptable.service 删除掉nftables.service,并修改&#x2F;usr&#x2F;lib&#x2F;waydroid&#x2F;data&#x2F;scripts&#x2F;waydroid-net.sh文件中的LXC_USE_NFT&#x3D;”falser” 12345sudo systemctl disable nftables.servicesudo systemctl stop nftables.servicesudo systemctl enable iptables.servicesudo systemctl start iptables.servicesystemctl restart waydroid-container 获取安卓设备id执行下方命令,选择安卓版本,选择获取安卓设备id,并前往安卓设备认证,认证完成后重启waydroid再登陆google账号 12345git clone https://mirror.ghproxy.com/https://github.com/casualsnek/waydroid_scriptcd waydroid_scriptpython3 -m venv venvvenv/bin/pip install -r requirements.txtsudo venv/bin/python3 main.py","categories":[{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/tags/linux/"},{"name":"waydroid","slug":"waydroid","permalink":"https://falser101.github.io/tags/waydroid/"}],"author":"falser101"},{"title":"iptables基本概念","slug":"2024/02.iptables基本概念","date":"2024-08-13T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2024/08/14/2024/02.iptables基本概念/","permalink":"https://falser101.github.io/2024/08/14/2024/02.iptables%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"防火墙相关概念iptables其实不是真正的防火墙，我们可以把它理解成一个客户端代理，用户通过iptables这个代理，将用户的安全设定执行到对应的”安全框架”中，这个”安全框架”才是真正的防火墙，这个框架的名字叫netfilter netfilter才是防火墙真正的安全框架（framework），netfilter位于内核空间。 iptables其实是一个命令行工具，位于用户空间，我们用这个工具操作真正的框架。 netfilter&#x2F;iptables（下文中简称为iptables）组成Linux平台下的包过滤防火墙，与大多数的Linux软件一样，这个包过滤防火墙是免费的，它可以代替昂贵的商业防火墙解决方案，完成封包过滤、封包重定向和网络地址转换（NAT）等功能。 Netfilter是Linux操作系统核心层内部的一个数据包处理模块，它具有如下功能： 网络地址转换(Network Address Translate) 数据包内容修改 以及数据包过滤的防火墙功能 所以说，虽然我们使用service iptables start启动iptables”服务”，但是其实准确的来说，iptables并没有一个守护进程，所以并不能算是真正意义上的服务，而应该算是内核提供的功能。 iptables基础我们知道iptables是按照规则来办事的，我们就来说说规则（rules），规则其实就是网络管理员预定义的条件，规则一般的定义为”如果数据包头符合这样的条件，就这样处理这个数据包”。规则存储在内核空间的信息包过滤表中，这些规则分别指定了源地址、目的地址、传输协议（如TCP、UDP、ICMP）和服务类型（如HTTP、FTP和SMTP）等。当数据包与规则匹配时，iptables就根据规则所定义的方法来处理这些数据包，如放行（accept）、拒绝（reject）和丢弃（drop）等。配置防火墙的主要工作就是添加、修改和删除这些规则。 当客户端访问服务器的web服务时，客户端发送报文到网卡，而tcp&#x2F;ip协议栈是属于内核的一部分，所以，客户端的信息会通过内核的TCP协议传输到用户空间中的web服务中，而此时，客户端报文的目标终点为web服务所监听的套接字（IP：Port）上，当web服务需要响应客户端请求时，web服务发出的响应报文的目标终点则为客户端，这个时候，web服务所监听的IP与端口反而变成了原点，我们说过，netfilter才是真正的防火墙，它是内核的一部分，所以，如果我们想要防火墙能够达到”防火”的目的，则需要在内核中设置关卡，所有进出的报文都要通过这些关卡，经过检查后，符合放行条件的才能放行，符合阻拦条件的则需要被阻止，于是，就出现了input关卡和output关卡，而这些关卡在iptables中不被称为”关卡”,而被称为”链”。 其实我们上面描述的场景并不完善，因为客户端发来的报文访问的目标地址可能并不是本机，而是其他服务器，当本机的内核支持IP_FORWARD时，我们可以将报文转发给其他服务器，所以，这个时候，我们就会提到iptables中的其他”关卡”，也就是其他”链”，他们就是 “路由前”、”转发”、”路由后”，他们的英文名是 PREROUTING、FORWARD、POSTROUTING 也就是说，当我们启用了防火墙功能时，报文需要经过如下关卡，也就是说，根据实际情况的不同，报文经过”链”可能不同。如果报文需要转发，那么报文则不会经过input链发往用户空间，而是直接在内核空间中经过forward链和postrouting链转发出去的。 所以，根据上图，我们能够想象出某些常用场景中，报文的流向： 到本机某进程的报文：PREROUTING –&gt; INPUT 由本机转发的报文：PREROUTING –&gt; FORWARD –&gt; POSTROUTING 由本机的某进程发出报文（通常为响应报文）：OUTPUT –&gt; POSTROUTING 链的概念现在，我们想象一下，这些”关卡”在iptables中为什么被称作”链”呢？我们知道，防火墙的作用就在于对经过的报文匹配”规则”，然后执行对应的”动作”,所以，当报文经过这些关卡的时候，则必须匹配这个关卡上的规则，但是，这个关卡上可能不止有一条规则，而是有很多条规则，当我们把这些规则串到一个链条上的时候，就形成了”链”,所以，我们把每一个”关卡”想象成如下图中的模样 ，这样来说，把他们称为”链”更为合适，每个经过这个”关卡”的报文，都要将这条”链”上的所有规则匹配一遍，如果有符合条件的规则，则执行规则对应的动作。 表的概念我们再想想另外一个问题，我们对每个”链”上都放置了一串规则，但是这些规则有些很相似，比如，A类规则都是对IP或者端口的过滤，B类规则是修改报文，那么这个时候，我们是不是能把实现相同功能的规则放在一起呢，必须能的。 我们把具有相同功能的规则的集合叫做”表”，所以说，不同功能的规则，我们可以放置在不同的表中进行管理，而iptables已经为我们定义了4种表，每种表对应了不同的功能，而我们定义的规则也都逃脱不了这4种功能的范围，所以，学习iptables之前，我们必须先搞明白每种表 的作用。 iptables为我们提供了如下规则的分类，或者说，iptables为我们提供了如下”表” filter表：负责过滤功能，防火墙；内核模块：iptables_filter nat表：network address translation，网络地址转换功能；内核模块：iptable_nat mangle表：拆解报文，做出修改，并重新封装 的功能；iptable_mangle raw表：关闭nat表上启用的连接追踪机制；iptable_raw 也就是说，我们自定义的所有规则，都是这四种分类中的规则，或者说，所有规则都存在于这4张”表”中。 表链关系但是我们需要注意的是，某些”链”中注定不会包含”某类规则”，就像某些”关卡”天生就不具备某些功能一样，比如，A”关卡”只负责打击陆地敌人，没有防空能力，B”关卡”只负责打击空中敌人，没有防御步兵的能力，C”关卡”可能比较NB，既能防空，也能防御陆地敌人，D”关卡”最屌，海陆空都能防。 那让我们来看看，每个”关卡”都有哪些能力，或者说，让我们看看每个”链”上的规则都存在于哪些”表”中。 我们还是以图为例，先看看prerouting”链”上的规则都存在于哪些表中。 注意：下图只用于说明prerouting链上的规则存在于哪些表中，并没有描述表的顺序。 这幅图是什么意思呢？它的意思是说，prerouting”链”只拥有nat表、raw表和mangle表所对应的功能，所以，prerouting中的规则只能存放于nat表、raw表和mangle表中。 那么，根据上述思路，我们来总结一下，每个”关卡”都拥有什么功能， 或者说，每个”链”中的规则都存在于哪些”表”中。 PREROUTING 的规则可以存在于：raw表，mangle表，nat表。 INPUT 的规则可以存在于：mangle表，filter表，（centos7中还有nat表，centos6中没有）。 FORWARD 的规则可以存在于：mangle表，filter表。 OUTPUT 的规则可以存在于：raw表mangle表，nat表，filter表。 POSTROUTING 的规则可以存在于：mangle表，nat表。 但是，我们在实际的使用过程中，往往是通过”表”作为操作入口，对规则进行定义的，之所以按照上述过程介绍iptables，是因为从”关卡”的角度更容易从入门的角度理解，但是为了以便在实际使用的时候，更加顺畅的理解它们，此处我们还要将各”表”与”链”的关系罗列出来， 表（功能）&lt;–&gt; 链（钩子）： raw 表中的规则可以被哪些链使用：PREROUTING，OUTPUT mangle 表中的规则可以被哪些链使用：PREROUTING，INPUT，FORWARD，OUTPUT，POSTROUTING nat 表中的规则可以被哪些链使用：PREROUTING，OUTPUT，POSTROUTING（centos7中还有INPUT，centos6中没有） filter 表中的规则可以被哪些链使用：INPUT，FORWARD，OUTPUT 其实我们还需要注意一点，因为数据包经过一个”链”的时候，会将当前链的所有规则都匹配一遍，但是匹配时总归要有顺序，我们应该一条一条的去匹配，而且我们说过，相同功能类型的规则会汇聚在一张”表”中，那么，哪些”表”中的规则会放在”链”的最前面执行呢，这时候就需要有一个优先级的问题，我们还拿prerouting”链”做图示。 prerouting链中的规则存放于三张表中，而这三张表中的规则执行的优先级如下： raw –&gt; mangle –&gt; nat 但是我们知道，iptables为我们定义了4张”表”,当他们处于同一条”链”时，执行的优先级如下。 优先级次序（由高而低）： raw –&gt; mangle –&gt; nat –&gt; filter 但是我们前面说过，某些链天生就不能使用某些表中的规则，所以，4张表中的规则处于同一条链的目前只有output链，它就是传说中海陆空都能防守的关卡。 为了更方便的管理，我们还可以在某个表里面创建自定义链，将针对某个应用程序所设置的规则放置在这个自定义链中，但是自定义链接不能直接使用，只能被某个默认的链当做动作去调用才能起作用，我们可以这样想象，自定义链就是一段比较”短”的链子，这条”短”链子上的规则都是针对某个应用程序制定的，但是这条短的链子并不能直接使用，而是需要”焊接”在iptables默认定义链子上，才能被IPtables使用，这就是为什么默认定义的”链”需要把”自定义链”当做”动作”去引用的原因。这是后话，后面再聊，在实际使用时我们即可更加的明白。 数据经过防火墙的流程结合上述所有的描述，我们可以将数据包通过防火墙的流程总结为下图： 我们在写Iptables规则的时候，要时刻牢记这张路由次序图，灵活配置规则。 我们将经常用到的对应关系重新写在此处，方便对应图例查看。 链的规则存放于哪些表中（从链到表的对应关系）： PREROUTING 的规则可以存在于：raw表，mangle表，nat表。 INPUT 的规则可以存在于：mangle表，filter表，（centos7中还有nat表，centos6中没有）。 FORWARD 的规则可以存在于：mangle表，filter表。 OUTPUT 的规则可以存在于：raw表mangle表，nat表，filter表。 POSTROUTING 的规则可以存在于：mangle表，nat表。 表中的规则可以被哪些链使用（从表到链的对应关系）： raw 表中的规则可以被哪些链使用：PREROUTING，OUTPUT mangle 表中的规则可以被哪些链使用：PREROUTING，INPUT，FORWARD，OUTPUT，POSTROUTING nat 表中的规则可以被哪些链使用：PREROUTING，OUTPUT，POSTROUTING（centos7中还有INPUT，centos6中没有） filter 表中的规则可以被哪些链使用：INPUT，FORWARD，OUTPUT 下图中nat表在centos7中的情况就不再标明。 规则的概念说了一圈又说回来了，在上述描述中我们一直在提规则，可是没有细说，现在说说它。 先说说规则的概念，然后再通俗的解释它。 规则：根据指定的匹配条件来尝试匹配每个流经此处的报文，一旦匹配成功，则由规则后面指定的处理动作进行处理； 那么我们来通俗的解释一下什么是iptables的规则，之前打过一个比方，每条”链”都是一个”关卡”，每个通过这个”关卡”的报文都要匹配这个关卡上的规则，如果匹配，则对报文进行对应的处理，比如说，你我二人此刻就好像两个”报文”，你我二人此刻都要入关，可是城主有命，只有器宇轩昂的人才能入关，不符合此条件的人不能入关，于是守关将士按照城主制定的”规则”，开始打量你我二人，最终，你顺利入关了，而我已被拒之门外，因为你符合”器宇轩昂”的标准，所以把你”放行”了，而我不符合标准，所以没有被放行，其实，”器宇轩昂”就是一种”匹配条件”，”放行”就是一种”动作”，”匹配条件”与”动作”组成了规则。 了解了规则的概念，那我们来聊聊规则的组成部分,此处只是大概的将规则的结构列出，后面的文章中会单独对规则进行总结。 规则由匹配条件和处理动作组成。 匹配条件匹配条件分为基本匹配条件与扩展匹配条件 基本匹配条件： 源地址Source IP，目标地址 Destination IP 上述内容都可以作为基本匹配条件。 扩展匹配条件： 除了上述的条件可以用于匹配，还有很多其他的条件可以用于匹配，这些条件泛称为扩展条件，这些扩展条件其实也是netfilter中的一部分，只是以模块的形式存在，如果想要使用这些条件，则需要依赖对应的扩展模块。 源端口Source Port, 目标端口Destination Port 上述内容都可以作为扩展匹配条件 处理动作处理动作在iptables中被称为target（这样说并不准确，我们暂且这样称呼），动作也可以分为基本动作和扩展动作。 此处列出一些常用的动作，之后的文章会对它们进行详细的示例与总结： ACCEPT：允许数据包通过。 DROP：直接丢弃数据包，不给任何回应信息，这时候客户端会感觉自己的请求泥牛入海了，过了超时时间才会有反应。 REJECT：拒绝数据包通过，必要时会给数据发送端一个响应的信息，客户端刚请求就会收到拒绝的信息。 SNAT：源地址转换，解决内网用户用同一个公网地址上网的问题。 MASQUERADE：是SNAT的一种特殊形式，适用于动态的、临时会变的ip上。 DNAT：目标地址转换。 REDIRECT：在本机做端口映射。 LOG：在&#x2F;var&#x2F;log&#x2F;messages文件中记录日志信息，然后将数据包传递给下一条规则，也就是说除了记录以外不对数据包做任何其他操作，仍然让下一条规则去匹配。","categories":[{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/tags/linux/"},{"name":"iptables","slug":"iptables","permalink":"https://falser101.github.io/tags/iptables/"}],"author":"falser101"},{"title":"Kubernetes controller-manager源码学习","slug":"2024/kubernetes Controller-Manager源码分析","date":"2024-07-29T16:00:00.000Z","updated":"2025-06-18T07:00:29.631Z","comments":true,"path":"2024/07/30/2024/kubernetes Controller-Manager源码分析/","permalink":"https://falser101.github.io/2024/07/30/2024/kubernetes%20Controller-Manager%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"Kubernetes controller-manager 关于ControllerManager 官方介绍的很详细了,可查看官方文档 Controller Manager,下面是我对controller-manager源码阅读总结 入口 入口: cmd&#x2F;kube-controller-manager&#x2F;controller-manager.go 12345func main() &#123; command := app.NewControllerManagerCommand() code := cli.Run(command) os.Exit(code)&#125; 进入app.NewControllerManagerCommand()方法 12345678910111213141516171819202122232425func NewControllerManagerCommand() *cobra.Command &#123; ......//省略 RunE: func(cmd *cobra.Command, args []string) error &#123; verflag.PrintAndExitIfRequested() // Activate logging as soon as possible, after that // show flags with the final logging configuration. if err := logsapi.ValidateAndApply(s.Logs, utilfeature.DefaultFeatureGate); err != nil &#123; return err &#125; cliflag.PrintFlags(cmd.Flags()) // 创建一个controller-manager的配置,将已知的controller和默认禁用的controller传入 c, err := s.Config(KnownControllers(), ControllersDisabledByDefault(), ControllerAliases()) if err != nil &#123; return err &#125; // add feature enablement metrics fg := s.ComponentGlobalsRegistry.FeatureGateFor(utilversion.DefaultKubeComponent) fg.(featuregate.MutableFeatureGate).AddMetrics() return Run(context.Background(), c.Complete()) &#125;, ......//省略&#125; Run方法进入Run(context.Background(), c.Complete())方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// Run runs the KubeControllerManagerOptions.func Run(ctx context.Context, c *config.CompletedConfig) error &#123; ......//省略 // Start 事件处理管道。 c.EventBroadcaster.StartStructuredLogging(0) c.EventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: c.Client.CoreV1().Events(&quot;&quot;)&#125;) defer c.EventBroadcaster.Shutdown() ......//省略 // Start controller manager Http 服务 // unsecuredMux is the handler for these controller *after* authn/authz filters have been applied var unsecuredMux *mux.PathRecorderMux clientBuilder, rootClientBuilder := createClientBuilders(c) saTokenControllerDescriptor := newServiceAccountTokenControllerDescriptor(rootClientBuilder) // 启动函数 run := func(ctx context.Context, controllerDescriptors map[string]*ControllerDescriptor) &#123; controllerContext, err := CreateControllerContext(ctx, c, rootClientBuilder, clientBuilder) if err != nil &#123; logger.Error(err, &quot;Error building controller context&quot;) klog.FlushAndExit(klog.ExitFlushTimeout, 1) &#125; if err := StartControllers(ctx, controllerContext, controllerDescriptors, unsecuredMux, healthzHandler); err != nil &#123; logger.Error(err, &quot;Error starting controllers&quot;) klog.FlushAndExit(klog.ExitFlushTimeout, 1) &#125; controllerContext.InformerFactory.Start(stopCh) controllerContext.ObjectOrMetadataInformerFactory.Start(stopCh) close(controllerContext.InformersStarted) &lt;-ctx.Done() &#125; // 无选举,直接执行 if !c.ComponentConfig.Generic.LeaderElection.LeaderElect &#123; // 所有的控制器描述 controllerDescriptors := NewControllerDescriptors() controllerDescriptors[names.ServiceAccountTokenController] = saTokenControllerDescriptor run(ctx, controllerDescriptors) return nil &#125; // 选举流程&#125; 注册控制器NewControllerDescriptors()方法注册了所有的内置controller,返回命名控制器组与包含InitFunc的ControllerDescriptor包装器对象配对 map;所有的控制器都必须满足共同的约束 12345678910111213141516171819202122232425262728293031323334353637383940414243// NewControllerDescriptors is a public map of named controller groups (you can start more than one in an init func)// paired to their ControllerDescriptor wrapper object that includes InitFunc.// This allows for structured downstream composition and subdivision.func NewControllerDescriptors() map[string]*ControllerDescriptor &#123; controllers := map[string]*ControllerDescriptor&#123;&#125; aliases := sets.NewString() register := func(controllerDesc *ControllerDescriptor) &#123; if controllerDesc == nil &#123; panic(&quot;received nil controller for a registration&quot;) &#125; name := controllerDesc.Name() if len(name) == 0 &#123; panic(&quot;received controller without a name for a registration&quot;) &#125; if _, found := controllers[name]; found &#123; panic(fmt.Sprintf(&quot;controller name %q was registered twice&quot;, name)) &#125; if controllerDesc.GetInitFunc() == nil &#123; panic(fmt.Sprintf(&quot;controller %q does not have an init function&quot;, name)) &#125; for _, alias := range controllerDesc.GetAliases() &#123; if aliases.Has(alias) &#123; panic(fmt.Sprintf(&quot;controller %q has a duplicate alias %q&quot;, name, alias)) &#125; aliases.Insert(alias) &#125; controllers[name] = controllerDesc &#125; // First add &quot;special&quot; controllers that aren&#x27;t initialized normally. These controllers cannot be initialized // in the main controller loop initialization, so we add them here only for the metadata and duplication detection. // app.ControllerDescriptor#RequiresSpecialHandling should return true for such controllers // The only known special case is the ServiceAccountTokenController which *must* be started // first to ensure that the SA tokens for future controllers will exist. Think very carefully before adding new // special controllers. ......//省略 register(newJobControllerDescriptor())a return controllers&#125; 以JobController为例，其initFunc函数为startJobController，其定义如下： 1234567func newJobControllerDescriptor() *ControllerDescriptor &#123; return &amp;ControllerDescriptor&#123; name: names.JobController, aliases: []string&#123;&quot;job&quot;&#125;, initFunc: startJobController, &#125;&#125; 继续进入startJobController方法 12345678910111213func startJobController(ctx context.Context, controllerContext ControllerContext, controllerName string) (controller.Interface, bool, error) &#123; jobController, err := job.NewController( ctx, controllerContext.InformerFactory.Core().V1().Pods(), controllerContext.InformerFactory.Batch().V1().Jobs(), controllerContext.ClientBuilder.ClientOrDie(&quot;job-controller&quot;), ) if err != nil &#123; return nil, true, fmt.Errorf(&quot;creating Job controller: %v&quot;, err) &#125; go jobController.Run(ctx, int(controllerContext.ComponentConfig.JobController.ConcurrentJobSyncs)) return nil, true, nil&#125; 这里JobController的创建是通过job.NewController方法,,加载了Pod 和 Job的Informer，然后启动了JobController的Run方法，启动了一个goroutine来执行JobController的Run方法。 ControllerManager的启动流程大致就是这样，具体的实现逻辑就不再赘述了，可以自行查看源码。 总结Kubernetes controller-manager 的启动流程包括：初始化配置、注册控制器、启动事件处理管道和 HTTP 服务，并通过控制器描述对象注册和启动所有控制器。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://falser101.github.io/categories/k8s/"}],"tags":[{"name":"Kubernetes controller-manager","slug":"Kubernetes-controller-manager","permalink":"https://falser101.github.io/tags/Kubernetes-controller-manager/"}],"author":"falser101"},{"title":"Kubernetes Scheduler源码分析","slug":"2024/kubernetesScheduler源码分析","date":"2024-07-27T16:00:00.000Z","updated":"2025-06-18T07:00:29.118Z","comments":true,"path":"2024/07/28/2024/kubernetesScheduler源码分析/","permalink":"https://falser101.github.io/2024/07/28/2024/kubernetesScheduler%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"Kubernetes Scheduler调度器调度器是 Kubernetes 中的核心组件之一，负责将 Pod 调度到集群中的节点上。调度器的主要职责是根据一系列的调度策略和规则，选择一个最适合运行 Pod 的节点，并将 Pod 绑定到该节点上。 调度器的主要工作流程如下： 调度器监听集群中的 Pod 创建事件，当有新的 Pod 需要调度时，调度器会收到通知。 调度器会根据一系列的调度策略和规则，对集群中的节点进行评估，选择一个最适合运行 Pod 的节点。这些策略和规则可能包括节点的资源使用情况、节点标签、亲和性规则等。 一旦选择了合适的节点，调度器会将 Pod 绑定到该节点上，并将绑定信息更新到 Kubernetes API 服务器中。 节点上的 kubelet 组件会监听到 Pod 绑定事件，并开始在该节点上启动 Pod。 调度器会继续监听集群中的 Pod 创建事件，并重复上述过程，直到所有的 Pod 都被调度到合适的节点上。 源码分析调度器初始化调度器在 Kubernetes 集群启动时会进行初始化，主要涉及到以下几个步骤： 入口: cmd/kube-scheduler/scheduler.go 12345func main() &#123; command := app.NewSchedulerCommand() code := cli.Run(command) os.Exit(code)&#125; 进入 app.NewSchedulerCommand()方法，该方法会创建一个 *cobra.Command 对象，并设置一些默认参数和命令行标志。主要逻辑在runCommand方法中 1234567891011121314151617181920// runCommand runs the scheduler.func runCommand(cmd *cobra.Command, opts *options.Options, registryOptions ...Option) error &#123; verflag.PrintAndExitIfRequested() fg := opts.ComponentGlobalsRegistry.FeatureGateFor(utilversion.DefaultKubeComponent) // Activate logging as soon as possible, after that // show flags with the final logging configuration. if err := logsapi.ValidateAndApply(opts.Logs, fg); err != nil &#123; fmt.Fprintf(os.Stderr, &quot;%v\\n&quot;, err) os.Exit(1) &#125; ......// 省略部分代码 // cc, sched, err := Setup(ctx, opts, registryOptions...) if err != nil &#123; return err &#125; // add feature enablement metrics fg.(featuregate.MutableFeatureGate).AddMetrics() return Run(ctx, cc, sched)&#125; Setup方法主要是创建并返回一个完整的配置和调度器对象 123456789101112131415161718192021222324252627282930313233// 创建一个完整的配置和调度器对象func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) &#123; ......// 省略部分代码 // Get the completed config cc := c.Complete() ......// 省略部分代码 // Create the scheduler. sched, err := scheduler.New(ctx, cc.Client, cc.InformerFactory, cc.DynInformerFactory, recorderFactory, scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion), scheduler.WithKubeConfig(cc.KubeConfig), scheduler.WithProfiles(cc.ComponentConfig.Profiles...), scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry), scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds), scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds), scheduler.WithPodMaxInUnschedulablePodsDuration(cc.PodMaxInUnschedulablePodsDuration), scheduler.WithExtenders(cc.ComponentConfig.Extenders...), scheduler.WithParallelism(cc.ComponentConfig.Parallelism), scheduler.WithBuildFrameworkCapturer(func(profile kubeschedulerconfig.KubeSchedulerProfile) &#123; // Profiles are processed during Framework instantiation to set default plugins and configurations. Capturing them for logging completedProfiles = append(completedProfiles, profile) &#125;), ) ......//省略 return &amp;cc, sched, nil&#125; 进入scheduler.New()方法，创建一个调度器对象，并返回。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899// New returns a Schedulerfunc New(ctx context.Context, client clientset.Interface, informerFactory informers.SharedInformerFactory, dynInformerFactory dynamicinformer.DynamicSharedInformerFactory, recorderFactory profile.RecorderFactory, opts ...Option) (*Scheduler, error) &#123; ......//省略 // 注册内置插件 registry := frameworkplugins.NewInTreeRegistry() // merge 内置插件和外部注册插件 if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil &#123; return nil, err &#125; // 注册指标 metrics.Register() // 构建外部扩展器 extenders, err := buildExtenders(logger, options.extenders, options.profiles) if err != nil &#123; return nil, fmt.Errorf(&quot;couldn&#x27;t build extenders: %w&quot;, err) &#125; // 实例化 podLister 负责监控 pod 变化 podLister := informerFactory.Core().V1().Pods().Lister() // 实例化 nodeLister 负责监控 node 变化 nodeLister := informerFactory.Core().V1().Nodes().Lister() // 创建 snapshot，snapshot 作为缓存存在 snapshot := internalcache.NewEmptySnapshot() metricsRecorder := metrics.NewMetricsAsyncRecorder(1000, time.Second, stopEverything) // waitingPods holds all the pods that are in the scheduler and waiting in the permit stage // 保存调度程序中等待许可的所有 pod waitingPods := frameworkruntime.NewWaitingPodsMap() ......//省略部分代码 // 创建 profiles，profiles 中存储的是调度器框架 profiles, err := profile.NewMap(ctx, options.profiles, registry, recorderFactory, frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion), frameworkruntime.WithClientSet(client), frameworkruntime.WithKubeConfig(options.kubeConfig), frameworkruntime.WithInformerFactory(informerFactory), frameworkruntime.WithResourceClaimCache(resourceClaimCache), frameworkruntime.WithSnapshotSharedLister(snapshot), frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)), frameworkruntime.WithParallelism(int(options.parallelism)), frameworkruntime.WithExtenders(extenders), frameworkruntime.WithMetricsRecorder(metricsRecorder), frameworkruntime.WithWaitingPods(waitingPods), ) queueingHintsPerProfile[profileName], err = buildQueueingHintMap(ctx, profile.EnqueueExtensions()) // 创建优先级队列 podQueue podQueue := internalqueue.NewSchedulingQueue( profiles[options.profiles[0].SchedulerName].QueueSortFunc(), informerFactory, internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second), internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second), internalqueue.WithPodLister(podLister), internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration), internalqueue.WithPreEnqueuePluginMap(preEnqueuePluginMap), internalqueue.WithQueueingHintMapPerProfile(queueingHintsPerProfile), internalqueue.WithPluginMetricsSamplePercent(pluginMetricsSamplePercent), internalqueue.WithMetricsRecorder(*metricsRecorder), ) for _, fwk := range profiles &#123; fwk.SetPodNominator(podQueue) &#125; // 创建调度器缓存 schedulerCache schedulerCache := internalcache.New(ctx, durationToExpireAssumedPod) // 实例化调度器 Scheduler sched := &amp;Scheduler&#123; Cache: schedulerCache, client: client, nodeInfoSnapshot: snapshot, percentageOfNodesToScore: options.percentageOfNodesToScore, Extenders: extenders, StopEverything: stopEverything, SchedulingQueue: podQueue, Profiles: profiles, logger: logger, &#125; // 将队列的 Pop 方法赋值给 sched.NextPod sched.NextPod = podQueue.Pop sched.applyDefaultHandlers() if err = addAllEventHandlers(sched, informerFactory, dynInformerFactory, resourceClaimCache, unionedGVKs(queueingHintsPerProfile)); err != nil &#123; return nil, fmt.Errorf(&quot;adding event handlers: %w&quot;, err) &#125; return sched, nil&#125; scheduler.New 创建了 snapshot, eventHandler, profiles(framework) 和 cache 等对象，结合着调度框架将它们关联起来会更清晰 启动调度器 继续进入Run方法，执行根据配置生成的调度器，只在发生错误或上下文完成时返回。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// Run executes the scheduler based on the given configuration. It only returns on error or when context is done.func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error &#123; // 启动事件处理器 cc.EventBroadcaster.StartRecordingToSink(ctx.Done()) defer cc.EventBroadcaster.Shutdown() // 设置健康检查 var checks, readyzChecks []healthz.HealthChecker if cc.ComponentConfig.LeaderElection.LeaderElect &#123; checks = append(checks, cc.LeaderElection.WatchDog) readyzChecks = append(readyzChecks, cc.LeaderElection.WatchDog) &#125; readyzChecks = append(readyzChecks, healthz.NewShutdownHealthz(ctx.Done())) // 选举leader waitingForLeader := make(chan struct&#123;&#125;) isLeader := func() bool &#123; select &#123; case _, ok := &lt;-waitingForLeader: // if channel is closed, we are leading return !ok default: // channel is open, we are waiting for a leader return false &#125; &#125; ......//省略 // 运行 informer startInformersAndWaitForSync := func(ctx context.Context) &#123; // Start all informers. // 启动所有的informers cc.InformerFactory.Start(ctx.Done()) // DynInformerFactory can be nil in tests. if cc.DynInformerFactory != nil &#123; cc.DynInformerFactory.Start(ctx.Done()) &#125; // Wait for all caches to sync before scheduling. // 在调度之前等待所有的缓存同步 cc.InformerFactory.WaitForCacheSync(ctx.Done()) // DynInformerFactory can be nil in tests. if cc.DynInformerFactory != nil &#123; cc.DynInformerFactory.WaitForCacheSync(ctx.Done()) &#125; // Wait for all handlers to sync (all items in the initial list delivered) before scheduling. // 在调度之前等待所有的处理器同步 if err := sched.WaitForHandlersSync(ctx); err != nil &#123; logger.Error(err, &quot;waiting for handlers to sync&quot;) &#125; close(handlerSyncReadyCh) logger.V(3).Info(&quot;Handlers synced&quot;) &#125; if !cc.ComponentConfig.DelayCacheUntilActive || cc.LeaderElection == nil &#123; startInformersAndWaitForSync(ctx) &#125; // If leader election is enabled, runCommand via LeaderElector until done and exit. if cc.LeaderElection != nil &#123; &#125; // Leader election is disabled, so runCommand inline until done. close(waitingForLeader) sched.Run(ctx) return fmt.Errorf(&quot;finished without leader elect&quot;)&#125; Run 函数内包含三部分处理： 选举 leader 节点。如果是单节点，则跳过选举。 运行 informer，负责监控 pod 和 node 变化。 运行调度器 进入 sched.Run 查看调度器是如何运行的。 123456789101112131415// Run begins watching and scheduling. It starts scheduling and blocked until the context is done.// 开始监控和调度，它开始调度并阻塞直到上下文完成。func (sched *Scheduler) Run(ctx context.Context) &#123; // 从队列中去需要调度的 pod sched.SchedulingQueue.Run(logger) // We need to start scheduleOne loop in a dedicated goroutine, // because scheduleOne function hangs on getting the next item // from the SchedulingQueue. // If there are no new pods to schedule, it will be hanging there // and if done in this goroutine it will be blocking closing // SchedulingQueue, in effect causing a deadlock on shutdown. // 调度 pod go wait.UntilWithContext(ctx, sched.ScheduleOne, 0)&#125; sched.Run 主要做了两件事。从优先级队列中取用于调度的 pod，然后通过 sched.scheduleOne 调度该 pod。 如何取出 pod 的呢？在 sched.Run 函数中，调用了 sched.SchedulingQueue.Run(logger)。 123456789// Run starts the goroutine to pump from podBackoffQ to activeQfunc (p *PriorityQueue) Run(logger klog.Logger) &#123; go wait.Until(func() &#123; p.flushBackoffQCompleted(logger) &#125;, 1.0*time.Second, p.stop) go wait.Until(func() &#123; p.flushUnschedulablePodsLeftover(logger) &#125;, 30*time.Second, p.stop)&#125; 优先级队列由 ActiveQ，BackoffQ 和 UnschedulableQ 组成，其逻辑关系如下 在 PriorityQueue.Run 中启动两个 goroutine 分别运行 p.flushBackoffQCompleted 和 p.flushUnschedulablePodsLeftover 方法。p.flushBackoffQCompleted 将处于 BackOffQ 的 pod 移到 ActiveQ。p.flushUnschedulablePodsLeftover 将 UnschedulableQ 的 pod 移到 ActiveQ 或者 BackOffQ 接着，进入 sched.scheduleOne 查看 pod 是怎么调度的。 1234567891011121314151617181920func (sched *Scheduler) scheduleOne(ctx context.Context) &#123; ... // 获取需要调度的 pod podInfo, err := sched.NextPod(logger) ... // 进入调度循环调度 pod scheduleResult, assumedPodInfo, status := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate) if !status.IsSuccess() &#123; sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, status, scheduleResult.nominatingInfo, start) return &#125; // 进入绑定循环绑定 pod go func() &#123; ... status := sched.bindingCycle(bindingCycleCtx, state, fwk, scheduleResult, assumedPodInfo, start, podsToActivate) ... &#125;()&#125; sched.scheduleOne 主要包括三部分：获取需要调度的 pod，进入调度循环调度 pod 和进入绑定循环绑定 pod。其逻辑结构如下。 sched.NextPod 获取需要调度的 pod 12345678910111213141516171819202122func (p *PriorityQueue) Pop(logger klog.Logger) (*framework.QueuedPodInfo, error) &#123; ... for p.activeQ.Len() == 0 &#123; if p.closed &#123; logger.V(2).Info(&quot;Scheduling queue is closed&quot;) return nil, nil &#125; // 如果 activeQ 没有 pod 的话，阻塞等待 p.cond.Wait() &#125; // 从 activeQ 中取 pod obj, err := p.activeQ.Pop() if err != nil &#123; return nil, err &#125; pInfo := obj.(*framework.QueuedPodInfo) ... return pInfo, nil&#125; sched.NextPod 的逻辑主要是看 activeQ 队列中有没有 pod，如果有的话，取 pod 调度。如果没有的话，阻塞等待，直到 activeQ 中有 pod。 sched.schedulingCycle 调度 pod 1234567891011121314151617181920212223242526272829303132func (sched *Scheduler) schedulingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, podInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate,) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) &#123; ... // 调度 Pod scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod) ... assumedPodInfo := podInfo.DeepCopy() assumedPod := assumedPodInfo.Pod err = sched.assume(logger, assumedPod, scheduleResult.SuggestedHost) ... // 运行 Reserve 插件的 Reserve 方法 if sts := fwk.RunReservePluginsReserve(ctx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() &#123; ... &#125; // 运行 Permit 插件 runPermitStatus := fwk.RunPermitPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) if !runPermitStatus.IsWait() &amp;&amp; !runPermitStatus.IsSuccess() &#123; ... &#125; ... return scheduleResult, assumedPodInfo, nil&#125; sched.schedulingCycle 包含几个步骤：sched.SchedulePod 调度 Pod，将调度的还未绑定的 Pod 作为 assumedPod 添加到缓存，运行 Reserve 插件和 Permit 插件。 首先，看 sched.SchedulePod 是怎么调度 Pod 的。 123456789101112131415161718192021222324252627282930313233343536373839// schedulePod tries to schedule the given pod to one of the nodes in the node list.// If it succeeds, it will return the name of the node.// If it fails, it will return a FitError with reasons.func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) &#123; // 获取合适的节点列表 feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod) if len(feasibleNodes) == 0 &#123; return result, &amp;framework.FitError&#123; Pod: pod, NumAllNodes: sched.nodeInfoSnapshot.NumNodes(), Diagnosis: diagnosis, &#125; &#125; // When only one node after predicate, just use it. // 当只有一个节点满足条件时，直接返回 if len(feasibleNodes) == 1 &#123; return ScheduleResult&#123; SuggestedHost: feasibleNodes[0].Node().Name, EvaluatedNodes: 1 + diagnosis.NodeToStatus.Len(), FeasibleNodes: 1, &#125;, nil &#125; //否贼进行优先级排序 priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes) if err != nil &#123; return result, err &#125; host, _, err := selectHost(priorityList, numberOfHighestScoredNodesToReport) trace.Step(&quot;Prioritizing done&quot;) return ScheduleResult&#123; SuggestedHost: host, EvaluatedNodes: len(feasibleNodes) + diagnosis.NodeToStatus.Len(), FeasibleNodes: len(feasibleNodes), &#125;, err&#125; 在 sched.SchedulePod 中，sched.findNodesThatFitPod 为 Pod 寻找合适的节点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// kubernetes/pkg/scheduler/schedule_one.gofunc (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*framework.NodeInfo, framework.Diagnosis, error) &#123; ... // 从 snapshot 中取所有节点 allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List() if err != nil &#123; return nil, diagnosis, err &#125; preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod) if !s.IsSuccess() &#123; ... &#125; ... // 寻找 pod 可调用的节点 feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, &amp;diagnosis, nodes) ...&#125;// kubernetes/pkg/scheduler/schedule_one.gofunc (sched *Scheduler) findNodesThatPassFilters( ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, diagnosis *framework.Diagnosis, nodes []*framework.NodeInfo) ([]*framework.NodeInfo, error) &#123; ... checkNode := func(i int) &#123; ... status := fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo) &#125; ...&#125;// kubernetes/pkg/scheduler/framework/runtime/framework.gofunc (f *frameworkImpl) RunFilterPluginsWithNominatedPods(ctx context.Context, state *framework.CycleState, pod *v1.Pod, info *framework.NodeInfo) *framework.Status &#123; ... for i := 0; i &lt; 2; i++ &#123; ... // 运行 Filter 插件 status = f.RunFilterPlugins(ctx, stateToUse, pod, nodeInfoToUse) if !status.IsSuccess() &amp;&amp; !status.IsRejected() &#123; return status &#125; &#125; return status&#125; sched.findNodesThatFitPod 运行 Filter 插件获取可用的节点 feasibleNodes。接着，如果可用的节点只有一个，则返回调度结果。如果有多个节点则运行 priority 插件寻找最合适的节点作为调度节点。逻辑如下。 123456789101112131415161718192021222324252627282930func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) &#123; ... feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod) if err != nil &#123; return result, err &#125; ... if len(feasibleNodes) == 1 &#123; return ScheduleResult&#123; SuggestedHost: feasibleNodes[0].Node().Name, EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap), FeasibleNodes: 1, &#125;, nil &#125; priorityList, err := sched.prioritizeNodes(ctx, fwk, state, pod, feasibleNodes) if err != nil &#123; return result, err &#125; host, _, err := selectHost(priorityList, numberOfHighestScoredNodesToReport) ... return ScheduleResult&#123; SuggestedHost: host, EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap), FeasibleNodes: len(feasibleNodes), &#125;, err&#125; 获得调度结果 scheduleResult 后，在 sched.schedulingCycle 中的 sched.assume 将 assumePod 的 NodeName 更新为调度的节点 host，并且将 assumePod 添加到缓存中。缓存允许运行假定的操作，该操作将 Pod 临时存储在缓存中，使得 Pod 看起来像已经在快照的所有消费者的指定节点上运行那样。假定操作忽视了 kube-apiserver 和 Pod 实际更新的时间，从而增加调度器的吞吐量。 12345678910111213141516func (sched *Scheduler) assume(logger klog.Logger, assumed *v1.Pod, host string) error &#123; assumed.Spec.NodeName = host if err := sched.Cache.AssumePod(logger, assumed); err != nil &#123; logger.Error(err, &quot;Scheduler cache AssumePod failed&quot;) return err &#125; ... return nil&#125;// kubernetes/pkg/scheduler/internal/cache/cache.gofunc (cache *cacheImpl) AssumePod(logger klog.Logger, pod *v1.Pod) error &#123; ... return cache.addPod(logger, pod, true)&#125; 继续如 调度框架 所示，在 sched.schedulingCycle 中执行 Reserve 和 Permit 插件，插件执行通过后调度周期返回 Pod 的调度结果。 接着，进入绑定周期。 绑定周期是一个异步的 goroutine，负责将调度到节点的 Pod 发送给 kube-apiserver。进入绑定周期查看绑定逻辑的实现。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// kubernetes/pkg/scheduler/schedule_one.gofunc (sched *Scheduler) scheduleOne(ctx context.Context) &#123; ... // 调度周期返回调度结果 scheduleResult, assumedPodInfo, status := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate) if !status.IsSuccess() &#123; sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, status, scheduleResult.nominatingInfo, start) return &#125; // 绑定周期绑定调度结果 go func() &#123; ... status := sched.bindingCycle(bindingCycleCtx, state, fwk, scheduleResult, assumedPodInfo, start, podsToActivate) if !status.IsSuccess() &#123; sched.handleBindingCycleError(bindingCycleCtx, state, fwk, assumedPodInfo, start, scheduleResult, status) return &#125; ... &#125;()&#125;func (sched *Scheduler) bindingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, scheduleResult ScheduleResult, assumedPodInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate) *framework.Status &#123; ... // 运行 Permit 插件 if status := fwk.WaitOnPermit(ctx, assumedPod); !status.IsSuccess() &#123; ... &#125; // 运行 PreBind 插件 if status := fwk.RunPreBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost); !status.IsSuccess() &#123; ... &#125; // 运行 Bind 插件 if status := sched.bind(ctx, fwk, assumedPod, scheduleResult.SuggestedHost, state); !status.IsSuccess() &#123; return status &#125; // 运行 PostBind 插件 fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) ...&#125; 可以看到，绑定周期运行一系列插件进行绑定，进入 Bind 插件查看绑定的行为。 123456789101112131415161718192021222324252627282930313233343536373839func (sched *Scheduler) bind(ctx context.Context, fwk framework.Framework, assumed *v1.Pod, targetNode string, state *framework.CycleState) (status *framework.Status) &#123; ... return fwk.RunBindPlugins(ctx, state, assumed, targetNode)&#125;// kubernetes/pkg/scheduler/framework/runtime/framework.gofunc (f *frameworkImpl) RunBindPlugins(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (status *framework.Status) &#123; ... for _, pl := range f.bindPlugins &#123; status = f.runBindPlugin(ctx, pl, state, pod, nodeName) if status.IsSkip() &#123; continue &#125; ... &#125; ...&#125;func (f *frameworkImpl) runBindPlugin(ctx context.Context, bp framework.BindPlugin, state *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status &#123; ... status := bp.Bind(ctx, state, pod, nodeName) ... return status&#125;// kubernetes/pkg/scheduler/plugins/defaultbinder/default_binder.gofunc (b DefaultBinder) Bind(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) *framework.Status &#123; ... logger.V(3).Info(&quot;Attempting to bind pod to node&quot;, &quot;pod&quot;, klog.KObj(p), &quot;node&quot;, klog.KRef(&quot;&quot;, nodeName)) binding := &amp;v1.Binding&#123; ObjectMeta: metav1.ObjectMeta&#123;Namespace: p.Namespace, Name: p.Name, UID: p.UID&#125;, Target: v1.ObjectReference&#123;Kind: &quot;Node&quot;, Name: nodeName&#125;, &#125; err := b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions&#123;&#125;) if err != nil &#123; return framework.AsStatus(err) &#125; return nil&#125; 在 Bind 插件中调用 ClientSet 的 Bind 方法将 Pod 和 node 绑定的结果发给 kube-apiserver，实现绑定操作。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://falser101.github.io/categories/k8s/"}],"tags":[{"name":"Kubernetes Scheduler","slug":"Kubernetes-Scheduler","permalink":"https://falser101.github.io/tags/Kubernetes-Scheduler/"}],"author":"falser101"},{"title":"Gradle相关操作及问题","slug":"2024/01.Gradle相关操作及问题","date":"2024-07-07T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2024/07/08/2024/01.Gradle相关操作及问题/","permalink":"https://falser101.github.io/2024/07/08/2024/01.Gradle%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C%E5%8F%8A%E9%97%AE%E9%A2%98/","excerpt":"","text":"无法启动任务报错信息如下： 1Starting a Gradle Daemon, 5 busy and 1 incompatible Daemons could not be reused, use --status for details 可以删除~&#x2F;.gradle&#x2F;daemon&#x2F;x.x 文件夹","categories":[{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"}],"tags":[{"name":"Gradle","slug":"Gradle","permalink":"https://falser101.github.io/tags/Gradle/"}],"author":"falser101"},{"title":"Grafana Loki Promtail","slug":"2024/Grafana Loki Promtail日志收集","date":"2024-06-27T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2024/06/28/2024/Grafana Loki Promtail日志收集/","permalink":"https://falser101.github.io/2024/06/28/2024/Grafana%20Loki%20Promtail%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86/","excerpt":"","text":"Grafana Loki Promtail传统的日志收集框架如ELK（Logstash + Elasticsearch + Kibana）太过重量级，如果需求复杂，服务器资源不受限制，推荐使用ELK（Logstash + Elasticsearch + Kibana）方案；如果需求仅是将不同服务器上的日志采集上来集中展示，且需要一个轻量级的框架，那使用PLG（Promtail + Loki + Grafana）最合适不过了 各组件官网地址 Loki promtail grafana 下载安装包Loki安装包，Promtail安装包 12unzip loki-linux-amd64.zipunzip promtail-linux-amd64.zip 配置Loki配置，可以参考官网配置，有很多例子，这里我使用的是本地配置 1234567891011121314151617181920212223242526auth_enabled: falseserver: http_listen_port: 3100 ## 服务监听端口common: ring: instance_addr: 127.0.0.1 kvstore: store: inmemory replication_factor: 1 path_prefix: /tmp/lokischema_config: configs: - from: 2020-05-15 store: tsdb object_store: filesystem schema: v13 index: prefix: index_ period: 24hstorage_config: filesystem: directory: /tmp/loki/chunks Promtail配置，也可以参考官网配置，本次演示的配置如下 123456789101112131415server: http_listen_port: 9080 grpc_listen_port: 0positions: filename: /home/zhangjf/positions.yamlclients: - url: http://localhost:3100/loki/api/v1/push ## loki推送log地址scrape_configs:- job_name: system static_configs: - targets: - localhost ## 目标设置本机 labels: job: loki __path__: /home/zhangjf/loki.log ## loki进程的日志文件地址 启动12nohup ./loki-linux-amd64 -config.file=loki.yaml &gt;/dev/null 2&gt;loki.log 2&gt;&amp;1 &amp;nohup ./promtail-linux-amd64 -config.file=promtail.yaml &gt; promtail.log 2&gt;&amp;1 &amp; 可视化在Grafana原生支持Loki数据源，在web页面添加 然后添加一个dashboard，通过Import 的方式添加，id：13639 导入成功后可以看到日志数据成功展示出来了","categories":[{"name":"k8s","slug":"k8s","permalink":"https://falser101.github.io/categories/k8s/"}],"tags":[{"name":"监控告警","slug":"监控告警","permalink":"https://falser101.github.io/tags/%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/"}],"author":"falser101"},{"title":"在前后端分离的项目中接入oauth2","slug":"2024/oauth2前后端分离","date":"2024-02-21T16:00:00.000Z","updated":"2025-06-18T07:00:26.642Z","comments":true,"path":"2024/02/22/2024/oauth2前后端分离/","permalink":"https://falser101.github.io/2024/02/22/2024/oauth2%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB/","excerpt":"","text":"OAuth2OAuth2的概念不再说明了，可查阅官网OAuth 2.0。 使用Authorization Code Flow模式来实现1. 创建OAuth2客户端在OAuth2服务器端创建客户端，并设置好secret和client_id。以及回调地址。 2. 后端重定向接口java 使用com.nimbusds:oauth2-oidc-sdk来实现OAuth2，重定向接口如下： 1234567891011121314151617181920212223242526272829303132@RequestMapping(&quot;/authorize&quot;)public String authorize(HttpServletResponse response) throws URISyntaxException &#123; ClientID clientID = new ClientID(clientId); // The client callback URL URI callbackUri = new URI(callback); // Generate random state string to securely pair the callback to this request State state = new State(); // Generate nonce for the ID token Nonce nonce = new Nonce(); // Compose the OpenID authentication request (for the code flow) AuthenticationRequest request = new AuthenticationRequest.Builder( new ResponseType(&quot;code&quot;), new Scope(&quot;openid&quot;, &quot;email&quot;, &quot;profile&quot;), clientID, callbackUri) .endpointURI(opMetadata.getAuthorizationEndpointURI()) .state(state) .nonce(nonce) .build(); String url = request.toURI().toString(); Cookie stateCookie = new Cookie(&quot;state&quot;, state.getValue()); stateCookie.setHttpOnly(true); stateCookie.setMaxAge((int) Timer.ONE_HOUR); response.addCookie(stateCookie); Cookie nonceCookie = new Cookie(&quot;nonce&quot;, nonce.getValue()); nonceCookie.setHttpOnly(true); nonceCookie.setMaxAge((int) Timer.ONE_HOUR); response.addCookie(nonceCookie); return &quot;redirect:&quot;+ url;&#125; 3. 前端跳转1234567891011121314router.beforeEach((to, from, next) =&gt; &#123; NProgress.start() if (getToken()) &#123; ..... &#125; else &#123; /* has no token*/ if (whiteList.indexOf(to.path) !== -1) &#123; // 在免登录白名单，直接进入 next() &#125; else &#123; // 重定向到登录页面 window.location.href = &quot;/oauth2/authorize&quot; &#125; &#125;&#125;) 4. 前端的callback处理登录成功后重定向到以下页面，请求后端获取token并设置到localStorage中 12345678910111213141516&lt;script&gt;export default &#123; name: &#x27;Index&#x27;, created() &#123; //(钩子函数) const query = this.$route.query; const code = query.code; const state = query.code; this.$store.dispatch(&#x27;getToken&#x27;, code, state).then(() =&gt; &#123; this.$router.push(&#123; path: &#x27;/management/tenants&#x27; &#125;) &#125;).catch(() =&gt; &#123; console.log(&#x27;login error!!&#x27;) &#125;) &#125;,&#125;&lt;/script&gt; 5. 后端获取token12345678910111213141516171819202122232425@PostMapping(&quot;/token&quot;)@ResponseBodypublic ResponseEntity&lt;Map&lt;String, Object&gt;&gt; callback(@RequestBody OAuth2GetTokenVO vo) throws ParseException, URISyntaxException, IOException &#123; AuthorizationCode code = new AuthorizationCode(vo.getCode()); AuthorizationGrant codeGrant = new AuthorizationCodeGrant(code, new URI(callback)); ClientID clientID = new ClientID(clientId); Secret secret = new Secret(clientSecret); ClientAuthentication clientAuth = new ClientSecretPost(clientID, secret); // Make the token request TokenRequest tokenRequest = new TokenRequest(opMetadata.getTokenEndpointURI(), clientAuth, codeGrant); TokenResponse tokenResponse = OIDCTokenResponseParser.parse(tokenRequest.toHTTPRequest().send()); HTTPResponse httpResponse; Map&lt;String, Object&gt; result = new HashMap&lt;&gt;(); if (!tokenResponse.indicatesSuccess()) &#123; // We got an error response... TokenErrorResponse errorResponse = tokenResponse.toErrorResponse(); result.put(&quot;error&quot;, errorResponse.getErrorObject().getDescription()); &#125; else &#123; OIDCTokenResponse successResponse = (OIDCTokenResponse)tokenResponse.toSuccessResponse(); result.put(&quot;access_token&quot;, successResponse.getTokens().getAccessToken().getValue()); result.put(&quot;refresh_token&quot;, successResponse.getTokens().getRefreshToken().getValue()); result.put(&quot;id_token&quot;, successResponse.getOIDCTokens().getIDToken().getParsedString()); &#125; return ResponseEntity.ok(result);&#125;","categories":[{"name":"frontend","slug":"frontend","permalink":"https://falser101.github.io/categories/frontend/"}],"tags":[{"name":"OAuth2","slug":"OAuth2","permalink":"https://falser101.github.io/tags/OAuth2/"}],"author":"falser101"},{"title":"Pulsar 自定义拦截器","slug":"2024/Broker自定义拦截器","date":"2024-01-15T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2024/01/16/2024/Broker自定义拦截器/","permalink":"https://falser101.github.io/2024/01/16/2024/Broker%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8B%A6%E6%88%AA%E5%99%A8/","excerpt":"","text":"概述Pulsar暴露了BrokerInterceptor接口，允许用户自定义拦截器，拦截器提供了多个切入点，我们可以在这些切入点上完成自定义的逻辑，切入点如表所示: 切入点名 描述 beforeSendMessage 当Broker读取完消息，还未发送给Consumer时触发 onPulsarCommand Pulsar收到任何客户端命令时触发 onConnectionClosed 当有连接关闭时触发 onWebserviceRequest 当有管理流相关的HTTP请求时触发 onWebserviceResponse 当有管理流相关的HTTP响应时触发 onFilter 管理流HTTP的Filter，当作Serverlet Filter使用即可 messageProduced 消息发送成功后触发 自定义拦截器我们实现messageProduced这个切入点，定义一个AtomicInteger，在消息发送成功后执行incrementAndGet方法，然后打印出Message produced count。 实现接口123456789101112@Slf4jpublic class CounterBrokerInterceptor implements BrokerInterceptor &#123; private final AtomicInteger messageCount = new AtomicInteger(); @Override public void messageProduced(ServerCnx cnx, Producer producer, long startTimeNs, long ledgerId, long entryId, Topic.PublishContext publishContext) &#123; messageCount.incrementAndGet(); log.info(&quot;Message produced count &#123;&#125;&quot;, messageCount.get()); &#125;&#125; 编写项目描述文件然后还需要在项目的resources/META-INF/services/broker_interceptor.yml文件中添加以下内容： 123name: CounterBrokerInterceptor ## 名称description: CounterBrokerInterceptorinterceptorClass: org.apache.pulsar.interceptor.CounterBrokerInterceptor ## 自定义拦截器的全限定名 添加依赖由于Pulsar在启动broker时是加载nar包，所以我们需要添加maven依赖，将broker依赖和构建nar包的插件引入 1234567891011121314151617181920212223242526272829303132&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.pulsar&lt;/groupId&gt; &lt;artifactId&gt;pulsar-broker&lt;/artifactId&gt; &lt;version&gt;3.3.0-SNAPSHOT&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.nifi&lt;/groupId&gt; &lt;artifactId&gt;nifi-nar-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;extensions&gt;true&lt;/extensions&gt; &lt;configuration&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;-$&#123;project.version&#125;&lt;/finalName&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;default-nar&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;nar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 修改配置打包成功后，将target目录下的nar包复制到pulsar的interceptors目录下,并且修改broker.conf的配置以启用拦截器 12345## The directory to locate broker interceptorsbrokerInterceptorsDirectory=./interceptors## resources/META-INF/services/broker_interceptor.yml文件中name定义的名称brokerInterceptors=CounterBrokerInterceptor 启动并测试启动broker成功后，我们进行测试，创建生产者并发送消息 123456789101112131415161718192021222324252627282930313233343536373839package consumerimport ( &quot;context&quot; &quot;fmt&quot; &quot;log&quot; &quot;time&quot; &quot;github.com/apache/pulsar-client-go/pulsar&quot;)func Produce() &#123; client, err := pulsar.NewClient(pulsar.ClientOptions&#123; URL: &quot;pulsar://localhost:6650&quot;, OperationTimeout: 30 * time.Second, ConnectionTimeout: 30 * time.Second, &#125;) if err != nil &#123; log.Fatalf(&quot;Could not instantiate Pulsar client: %v&quot;, err) &#125; defer client.Close() producer, err := client.CreateProducer(pulsar.ProducerOptions&#123; Topic: &quot;public/default/test&quot;, &#125;) var data = make([]byte, 1024) for i := 0; i &lt; 100; i++ &#123; producer.Send(context.Background(), &amp;pulsar.ProducerMessage&#123; Payload: data, &#125;) &#125; if err != nil &#123; log.Fatal(err) &#125; defer producer.Close() fmt.Println(&quot;Published message&quot;)&#125; 调用Produce()方法，发送100条消息，查看broker的日志如下图:","categories":[],"tags":[],"author":"falser101"},{"title":"pulsar broker组件启动流程","slug":"2024/broker启动流程","date":"2024-01-09T16:00:00.000Z","updated":"2025-06-18T07:00:30.038Z","comments":true,"path":"2024/01/10/2024/broker启动流程/","permalink":"https://falser101.github.io/2024/01/10/2024/broker%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/","excerpt":"","text":"Broker组件启动流程在收发消息前，我们得启动Broker服务，本章将介绍Broker在启动阶段都做了什么。各个模块之前的关系，服务启动不同模块的初始化顺序。 服务启动使用bin目录下名为pulsar的CLI工具，执行命令时传入broker参数，会调用PulsarBrokerStarter这个类来启动Pulsar Broker服务。Broker总体启动流程如下图所示: 加载配置文件。Broker启动时会读取conf&#x2F;broker.conf文件，并将其中的配置信息全部转化为Key&#x2F;Value的形式，通过反射创建一个ServiceConfigration对象。并且把值设置到这个对象的属性中。 注册ShutdownHook和OOM监听器。ShutdownHook在服务关闭时执行清理💰，OOMListener在内存溢出时打印异常信息。 启动BookieStatsProvider服务,BookieStatsProvider是Bookie服务的统计信息提供者，提供Bookie服务的统计信息。这一步通常只在Standalone模式下执行。 启动BookieServer,让Bookie和Broker一起启动，主要用于开发测试 启动PulsarService服务，PulsarService是启动Broker的主入口，内部还会启动一个BrokerService，这两个Service的分工不同，PulsarService范围更大，包括负载管理、缓存、Schema、Admin相关的Web服务等，属于管理流。而BrokerService则专注于消息的收发，创建Netty EventLoopGroup、创建内置调度器等，属于数据流。 什么是管理流和数据流？ 增删改查Broker的配置，或者一个Topic，通过Pulsar-Admin来操作，这些操作都是管理流。收发消息，这些操作都是数据流。执行操作之前，需要通过PulsarClient来创建Producer和Consumer，然后才能执行收发消息的操作。 主要代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041public static void main(String[] args) throws Exception &#123; // ... BrokerStarter starter = new BrokerStarter(args); Runtime.getRuntime().addShutdownHook(...); PulsarByteBufAllocator.registerOOMListener(oomException -&gt; &#123;...&#125;); try &#123; starter.start(); &#125; ...&#125;## 加载配置文件private static ServiceConfiguration loadConfig(String configFile) throws Exception &#123; try (InputStream inputStream = new FileInputStream(configFile)) &#123; ServiceConfiguration config = create(inputStream, ServiceConfiguration.class); // it validates provided configuration is completed isComplete(config); return config; &#125;&#125;## 启动服务public void start() throws Exception &#123; if (bookieStatsProvider != null) &#123; bookieStatsProvider.start(bookieConfig); log.info(&quot;started bookieStatsProvider.&quot;); &#125; if (bookieServer != null) &#123; bookieStartFuture = ComponentStarter.startComponent(bookieServer); log.info(&quot;started bookieServer.&quot;); &#125; if (autoRecoveryMain != null) &#123; autoRecoveryMain.start(); log.info(&quot;started bookie autoRecoveryMain.&quot;); &#125; pulsarService.start(); log.info(&quot;PulsarService started.&quot;);&#125; PulsarService.java主要代码如下（省略了一些不重要的代码）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147public void start() throws PulsarServerException &#123; mutex.lock(); try &#123; ...... localMetadataSynchronizer = StringUtils.isNotBlank(config.getMetadataSyncEventTopic()) ? new PulsarMetadataEventSynchronizer(this, config.getMetadataSyncEventTopic()) : null; localMetadataStore = createLocalMetadataStore(localMetadataSynchronizer); localMetadataStore.registerSessionListener(this::handleMetadataSessionEvent); // 创建coordinationService，用于分布式锁、Broker选主等 coordinationService = new CoordinationServiceImpl(localMetadataStore); // 创建pulsarResources，用于元数据管理，元数据包括本地元数据和配置文件元数据，保存在zookeeper中 // 主要实现方式为调用配置中心存储对于资源的元数据 pulsarResources = newPulsarResources(); // 创建orderedExecutor线程池，后续用于ZK的操作，拆分bundle等 orderedExecutor = newOrderedExecutor(); // 创建protocolHandlers，这个服务用于管理pulsar的扩展点，pulsar中可以通过*.nar文件实现自定义扩展点。 // Pulsar启动时会默认尝试扫描./protocols目录或系统环境变量java.io.tmpdir里设置的路径下的所有nar文件。 // 如KOP、AOP等都是通过这个扩展点实现的 protocolHandlers = ProtocolHandlers.load(config); protocolHandlers.initialize(config); // Now we are ready to start services this.bkClientFactory = newBookKeeperClientFactory(); // 创建ManagedLedgerClientFactory，用于管理ManagedLedger的客户端。 // Pulsar是计算和存储分离的架构，managedLedger是存储的抽象，ManagedLedgerClient用来请求BookKeeper保存数据 managedLedgerClientFactory = newManagedLedgerClientFactory(); // 创建BrokerService, 数据流相关的服务都在这里启动 this.brokerService = newBrokerService(this); // 创建loadManager，用于管理broker的负载，不管有没有打开自动负载管理都会创建 // 默认管理器是org.apache.pulsar.broker.loadbalance.impl.ModularLoadManagerImpl this.loadManager.set(LoadManager.create(this)); // needs load management service and before start broker service, this.startNamespaceService(); schemaStorage = createAndStartSchemaStorage(); // 用于管理Topic的Schema，如Schema.Type.JSON schemaRegistryService = SchemaRegistryService.create( schemaStorage, config.getSchemaRegistryCompatibilityCheckers(), this.executor); OffloadPoliciesImpl defaultOffloadPolicies = OffloadPoliciesImpl.create(this.getConfiguration().getProperties()); OrderedScheduler offloaderScheduler = getOffloaderScheduler(defaultOffloadPolicies); offloaderStats = LedgerOffloaderStats.create(config.isExposeManagedLedgerMetricsInPrometheus(), exposeTopicMetrics, offloaderScheduler, interval); // 创建OffloaderManager，当有大量的消息堆积，这些消息要求保留很长时间，我们不想让这些冷数据占用线上服务的硬盘空间。 // 这些冷数据可以使用OffloaderManager移动到其他磁盘上，以节省空间。 this.defaultOffloader = createManagedLedgerOffloader(defaultOffloadPolicies); setBrokerInterceptor(newBrokerInterceptor()); // use getter to support mocking getBrokerInterceptor method in tests BrokerInterceptor interceptor = getBrokerInterceptor(); if (interceptor != null) &#123; brokerService.setInterceptor(interceptor); interceptor.initialize(this); &#125; // 启动brokerService brokerService.start(); // Load additional servlets this.brokerAdditionalServlets = AdditionalServlets.load(config); // 创建管理流WebService，并启动 this.webService = new WebService(this); createMetricsServlet(); this.addWebServerHandlers(webService, metricsServlet, this.config); this.webService.start(); ...... // 启动Leader选举服务 startLeaderElectionService(); // 启动LoadManagementService this.startLoadManagementService(); // Initialize namespace service, after service url assigned. Should init zk and refresh self owner info. this.nsService.initialize(); // 主题策略服务，让用户可以设置主题级别的策略 if (config.isTopicLevelPoliciesEnabled() &amp;&amp; config.isSystemTopicEnabled()) &#123; this.topicPoliciesService = new SystemTopicBasedTopicPoliciesService(this); &#125; this.topicPoliciesService.start(); // Register heartbeat and bootstrap namespaces. this.nsService.registerBootstrapNamespaces(); // 事务相关服务 if (config.isTransactionCoordinatorEnabled()) &#123; MLTransactionMetadataStoreProvider.initBufferedWriterMetrics(getAdvertisedAddress()); MLPendingAckStoreProvider.initBufferedWriterMetrics(getAdvertisedAddress()); this.transactionBufferSnapshotServiceFactory = new TransactionBufferSnapshotServiceFactory(getClient()); this.transactionTimer = new HashedWheelTimer(new DefaultThreadFactory(&quot;pulsar-transaction-timer&quot;)); transactionBufferClient = TransactionBufferClientImpl.create(this, transactionTimer, config.getTransactionBufferClientMaxConcurrentRequests(), config.getTransactionBufferClientOperationTimeoutInMills()); transactionMetadataStoreService = new TransactionMetadataStoreService(TransactionMetadataStoreProvider .newProvider(config.getTransactionMetadataStoreProviderClassName()), this, transactionBufferClient, transactionTimer); transactionBufferProvider = TransactionBufferProvider .newProvider(config.getTransactionBufferProviderClassName()); transactionPendingAckStoreProvider = TransactionPendingAckStoreProvider .newProvider(config.getTransactionPendingAckStoreProviderClassName()); &#125; // 初始化metricsGenerator，用于将生成Broker级别的metrics暴露给Prometheus this.metricsGenerator = new MetricsGenerator(this); // Initialize the message protocol handlers. // start the protocol handlers only after the broker is ready, // so that the protocol handlers can access broker service properly. this.protocolHandlers.start(brokerService); Map&lt;String, Map&lt;InetSocketAddress, ChannelInitializer&lt;SocketChannel&gt;&gt;&gt; protocolHandlerChannelInitializers = this.protocolHandlers.newChannelInitializers(); this.brokerService.startProtocolHandlers(protocolHandlerChannelInitializers); acquireSLANamespace(); // 创建并启动function worker服务 this.startWorkerService(brokerService.getAuthenticationService(), brokerService.getAuthorizationService()); // 创建并启动packages management服务,用于管理Function的用户上传的代码包 if (config.isEnablePackagesManagement()) &#123; this.startPackagesManagementService(); &#125; ...... &#125;&#125; BrokerService.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public BrokerService(PulsarService pulsar, EventLoopGroup eventLoopGroup) throws Exception &#123; ...... // 创建topicOrderedExecutor，这是一个线程池，由bookkeeper封装，保证同一个Topic名， // 永远使用同一个线程来执行，从而保证同一个topic的操作是有序的 this.topicOrderedExecutor = OrderedExecutor.newBuilder() .numThreads(pulsar.getConfiguration().getTopicOrderedExecutorThreadNum()) .name(&quot;broker-topic-workers&quot;).build(); // 使用Netty创建acceptorGroup处理I/O事件 this.acceptorGroup = EventLoopUtil.newEventLoopGroup( pulsar.getConfiguration().getNumAcceptorThreads(), false, acceptorThreadFactory); // 使用Netty创建workerGroup处理业务逻辑 this.workerGroup = eventLoopGroup; ...... // Pulsar权限校验，支持Produce，Consume、Function三种权限 this.authorizationService = new AuthorizationService( pulsar.getConfiguration(), pulsar().getPulsarResources()); // 创建各种监视器 // 定期检查长期未使用的主题并删除 this.inactivityMonitor = OrderedScheduler.newSchedulerBuilder() .name(&quot;pulsar-inactivity-monitor&quot;) .numThreads(1) .build(); // 定期检查过期消息并删除 this.messageExpiryMonitor = ... // 定期压缩主题 this.compactionMonitor = ... this.consumedLedgersMonitor = ... this.backlogQuotaManager = new BacklogQuotaManager(pulsar); // backlog检查器 this.backlogQuotaChecker = ... // 认证服务 this.authenticationService = new AuthenticationService(pulsar.getConfiguration()); ...... // 创建动态配置的监听器 updateConfigurationAndRegisterListeners(); // 一些信号量 this.lookupRequestSemaphore = new AtomicReference&lt;Semaphore&gt;( new Semaphore(pulsar.getConfiguration().getMaxConcurrentLookupRequest(), false)); this.topicLoadRequestSemaphore = new AtomicReference&lt;Semaphore&gt;( new Semaphore(pulsar.getConfiguration().getMaxConcurrentTopicLoadRequest(), false)); if (pulsar.getConfiguration().getMaxUnackedMessagesPerBroker() &gt; 0 &amp;&amp; pulsar.getConfiguration().getMaxUnackedMessagesPerSubscriptionOnBrokerBlocked() &gt; 0.0) &#123; // 最大未确认消息数 this.maxUnackedMessages = pulsar.getConfiguration().getMaxUnackedMessagesPerBroker(); this.maxUnackedMsgsPerDispatcher = (int) (maxUnackedMessages * pulsar.getConfiguration().getMaxUnackedMessagesPerSubscriptionOnBrokerBlocked()); &#125; else &#123; this.maxUnackedMessages = 0; this.maxUnackedMsgsPerDispatcher = 0; &#125; // Pulsar的延迟消息的实现类会通过这个工厂创建出来 this.delayedDeliveryTrackerFactory = DelayedDeliveryTrackerLoader .loadDelayedDeliveryTrackerFactory(pulsar); // Entry元数据的拦截器，用于用户自定义扩展 this.brokerEntryMetadataInterceptors = BrokerEntryMetadataUtils .loadBrokerEntryMetadataInterceptors(pulsar.getConfiguration().getBrokerEntryMetadataInterceptors(), BrokerService.class.getClassLoader()); 动态配置在启动Broker Service前会遍历ServiceConfiguration类中的所有成员变量（包括私有成员变量） 如果变量不是null，并且它具有FieldContext注解，那么设置其可访问性（以便在后续步骤中访问其值） 如果变量具有dynamic()属性，使用new ConfigField(field)创建一个新的ConfigField对象，并且将该对象添加到dynamicConfigurationMap 12345678910111213private static ConcurrentOpenHashMap&lt;String, ConfigField&gt; prepareDynamicConfigurationMap() &#123; ConcurrentOpenHashMap&lt;String, ConfigField&gt; dynamicConfigurationMap = ConcurrentOpenHashMap.&lt;String, ConfigField&gt;newBuilder().build(); for (Field field : ServiceConfiguration.class.getDeclaredFields()) &#123; if (field != null &amp;&amp; field.isAnnotationPresent(FieldContext.class)) &#123; field.setAccessible(true); if (field.getAnnotation(FieldContext.class).dynamic()) &#123; dynamicConfigurationMap.put(field.getName(), new ConfigField(field)); &#125; &#125; &#125; return dynamicConfigurationMap;&#125; 然后通过下面的方法进行注册 1234public &lt;T&gt; void registerConfigurationListener(String configKey, Consumer&lt;T&gt; listener) &#123; validateConfigKey(configKey); configRegisteredListeners.put(configKey, listener);&#125; 示例： 12345678910111213registerConfigurationListener(&quot;maxPublishRatePerTopicInMessages&quot;, maxPublishRatePerTopicInMessages -&gt; updateMaxPublishRatePerTopicInMessages());private void updateMaxPublishRatePerTopicInMessages() &#123; this.pulsar().getExecutor().execute(() -&gt; forEachTopic(topic -&gt; &#123; if (topic instanceof AbstractTopic) &#123; ((AbstractTopic) topic).updateBrokerPublishRate(); ((AbstractTopic) topic).updatePublishRateLimiter(); &#125; &#125;));&#125;","categories":[],"tags":[],"author":"falser101"},{"title":"admissionWebhook","slug":"2024/admissionWebhook","date":"2023-11-20T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2023/11/21/2024/admissionWebhook/","permalink":"https://falser101.github.io/2023/11/21/2024/admissionWebhook/","excerpt":"","text":"原文链接 什么是准入 Webhook？准入 Webhook 是一种用于接收准入请求并对其进行处理的 HTTP 回调机制。 可以定义两种类型的准入 Webhook， 即验证性质的准入 Webhook 和变更性质的准入 Webhook。 变更性质的准入 Webhook 会先被调用。它们可以修改发送到 API 服务器的对象以执行自定义的设置默认值操作。 在完成了所有对象修改并且 API 服务器也验证了所传入的对象之后， 验证性质的 Webhook 会被调用，并通过拒绝请求的方式来强制实施自定义的策略。 Kubernetes 中的准入控制 webhook 的执行流程如下： 当用户尝试创建、修改或删除 Kubernetes 资源时，API 服务器会将请求发送到准入控制器。 准入控制器会将请求发送到 webhook。 webhook 会对请求进行处理，并返回一个响应。 API 服务器会根据 webhook 的响应来处理请求。 准入控制 webhook 可以分为两种类型： 变更准入控制器（MutatingAdmissionWebhook）：在资源存储之前对资源进行修改。 验证准入控制器（ValidatingAdmissionWebhook）：在资源存储之前对资源进行验证。 webhook返回不同响应 Webhook 允许请求的最简单响应示例：12345678910&#123; &quot;apiVersion&quot;: &quot;admission.k8s.io/v1&quot;, &quot;kind&quot;: &quot;AdmissionReview&quot;, &quot;response&quot;: &#123; &quot;uid&quot;: &quot;&lt;value from request.uid&gt;&quot;, &quot;allowed&quot;: true, &quot;patchType&quot;: &quot;JSONPatch&quot;, &quot;patch&quot;: &quot;W3sib3AiOiAiYWRkIiwgInBhdGgiOiAiL3NwZWMvcmVwbGljYXMiLCAidmFsdWUiOiAzfV0=&quot; &#125;&#125; 当允许请求时，mutating准入 Webhook 也可以选择修改传入的对象。 这是通过在响应中使用 patch 和 patchType 字段来完成的。 当前唯一支持的 patchType 是 JSONPatch，对于 patchType: JSONPatch，patch 字段包含一个以 base64 编码的 JSON patch 操作数组 例如：设置deployment的 spec.replicas 的单个补丁操作将是 [{“op”: “add”, “path”: “&#x2F;spec&#x2F;replicas”, “value”: 3}]，如果以 Base64 形式编码，结果将是 W3sib3AiOiAiYWRkIiwgInBhdGgiOiAiL3NwZWMvcmVwbGljYXMiLCAidmFsdWUiOiAzfV0&#x3D; Webhook 禁止请求的最简单响应示例：123456789101112&#123; &quot;apiVersion&quot;: &quot;admission.k8s.io/v1&quot;, &quot;kind&quot;: &quot;AdmissionReview&quot;, &quot;response&quot;: &#123; &quot;uid&quot;: &quot;&lt;value from request.uid&gt;&quot;, &quot;allowed&quot;: false, &quot;status&quot;: &#123;// 可自定义 &quot;code&quot;: 403, &quot;message&quot;: &quot;You cannot do this because it is Tuesday and your name starts with A&quot; &#125; &#125;&#125; 当拒绝请求时，Webhook 可以使用 status 字段自定义 http 响应码和返回给用户的消息。 有关状态类型的详细信息，请参见 API 文档。 禁止请求的响应示例，它定制了向用户显示的 HTTP 状态码和消息 配置准入WebHook你可以通过 ValidatingWebhookConfiguration 或者 MutatingWebhookConfiguration 动态配置哪些资源要被哪些准入 Webhook 处理。 以下是一个 MutatingWebhookConfiguration 示例，ValidatingWebhookConfiguration Webhook 配置与此类似。 有关每个配置字段的详细信息，请参阅 Webhook 配置部分。 1234567891011121314151617181920212223apiVersion: admissionregistration.k8s.io/v1kind: MutatingWebhookConfigurationmetadata: name: &quot;pod-policy.example.com&quot;webhooks:- name: &quot;pod-policy.example.com&quot; objectSelector: matchLabels: hello: &quot;true&quot; rules: - apiGroups: [&quot;&quot;] apiVersions: [&quot;v1&quot;] operations: [&quot;CREATE&quot;] resources: [&quot;pods&quot;] scope: &quot;Namespaced&quot; clientConfig: service: namespace: &quot;example-namespace&quot; name: &quot;example-service&quot; path: /mutate admissionReviewVersions: [&quot;v1&quot;] sideEffects: None timeoutSeconds: 5 当一个 API 服务器收到与 rules 相匹配的请求时， 该 API 服务器将按照 clientConfig 中指定的方式向 Webhook 发送一个 admissionReview 请求。 创建 Webhook 配置后，系统将花费几秒钟使新配置生效。 接下来演示如何开发一个自定义Webhook API并使用 开发Go API 服务端代码来自Github仓库 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394func (app *App) HandleMutate(w http.ResponseWriter, r *http.Request) &#123; admissionReview := &amp;admissionv1.AdmissionReview&#123;&#125; // read the AdmissionReview from the request json body err := readJSON(r, admissionReview) if err != nil &#123; app.HandleError(w, r, err) return &#125; // unmarshal the pod from the AdmissionRequest pod := &amp;corev1.Pod&#123;&#125; if err := json.Unmarshal(admissionReview.Request.Object.Raw, pod); err != nil &#123; app.HandleError(w, r, fmt.Errorf(&quot;unmarshal to pod: %v&quot;, err)) return &#125; // add the volume to the pod pod.Spec.Volumes = append(pod.Spec.Volumes, corev1.Volume&#123; Name: &quot;hello-volume&quot;, VolumeSource: corev1.VolumeSource&#123; ConfigMap: &amp;corev1.ConfigMapVolumeSource&#123; LocalObjectReference: corev1.LocalObjectReference&#123; Name: &quot;hello-configmap&quot;, &#125;, &#125;, &#125;, &#125;) // add volume mount to all containers in the pod for i := 0; i &lt; len(pod.Spec.Containers); i++ &#123; pod.Spec.Containers[i].VolumeMounts = append(pod.Spec.Containers[i].VolumeMounts, corev1.VolumeMount&#123; Name: &quot;hello-volume&quot;, MountPath: &quot;/etc/config&quot;, &#125;) &#125; containersBytes, err := json.Marshal(&amp;pod.Spec.Containers) if err != nil &#123; app.HandleError(w, r, fmt.Errorf(&quot;marshall containers: %v&quot;, err)) return &#125; volumesBytes, err := json.Marshal(&amp;pod.Spec.Volumes) if err != nil &#123; app.HandleError(w, r, fmt.Errorf(&quot;marshall volumes: %v&quot;, err)) return &#125; // build json patch patch := []JSONPatchEntry&#123; &#123; OP: &quot;add&quot;, Path: &quot;/metadata/labels/hello-added&quot;, Value: []byte(`&quot;OK&quot;`), &#125;, &#123; OP: &quot;replace&quot;, Path: &quot;/spec/containers&quot;, Value: containersBytes, &#125;, &#123; OP: &quot;replace&quot;, Path: &quot;/spec/volumes&quot;, Value: volumesBytes, &#125;, &#125; patchBytes, err := json.Marshal(&amp;patch) if err != nil &#123; app.HandleError(w, r, fmt.Errorf(&quot;marshall jsonpatch: %v&quot;, err)) return &#125; patchType := admissionv1.PatchTypeJSONPatch // build admission response admissionResponse := &amp;admissionv1.AdmissionResponse&#123; UID: admissionReview.Request.UID, Allowed: true, Patch: patchBytes, PatchType: &amp;patchType, &#125; respAdmissionReview := &amp;admissionv1.AdmissionReview&#123; TypeMeta: metav1.TypeMeta&#123; Kind: &quot;AdmissionReview&quot;, APIVersion: &quot;admission.k8s.io/v1&quot;, &#125;, Response: admissionResponse, &#125; jsonOk(w, &amp;respAdmissionReview)&#125; 上述代码主要做了以下事情： 将来自 Http 请求中的 AdmissionReview json 输入反序列化。 读取 Pod 的 spec 信息。 将 hello-configmap 作为数据源，添加 hello-volume 卷到 Pod。 挂载卷至 Pod 容器中。 以 JSON PATCH 的形式记录变更信息，包括卷的变更，卷挂载信息的变更。顺道为容器添加一个“hello-added&#x3D;true”的标签。 构建 json 格式的响应结果，结果中包含了这次请求中的被修改的部分。 创建k8s资源在webhook项目下执行 1234567891011$ kubectl apply -k k8s/deployment## 日志configmap/hello-configmap createdservice/hello-webhook-service createdmutatingwebhookconfiguration.admissionregistration.k8s.io/hello-webhook.leclouddev.com created$ kubectl apply -k k8s/other## 日志configmap/hello-configmap createdservice/hello-webhook-service createdmutatingwebhookconfiguration.admissionregistration.k8s.io/hello-webhook.leclouddev.com created 生成TLS证书Webhook API 服务器需要通过 TLS 方式通信。如果想将其部署至 Kubernetes 集群内，我们还需要证书。原作者的仓库使用的kubectl版本较低，原作者是通过job生成pod来生成相关证书的，我们可以直接执行脚本来生成，脚本内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190#!/usr/bin/env shset -eusage() &#123; cat &lt;&lt;EOFGenerate certificate suitable for use with any Kubernetes Mutating Webhook.This script uses k8s&#x27; CertificateSigningRequest API to a generate acertificate signed by k8s CA suitable for use with any Kubernetes Mutating Webhook service pod.This requires permissions to create and approve CSR. Seehttps://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster fordetailed explantion and additional instructions.The server key/cert k8s CA cert are stored in a k8s secret.usage: $&#123;0&#125; [OPTIONS]The following flags are required. --service Service name of webhook. --webhook Webhook config name. --namespace Namespace where webhook service and secret reside. --secret Secret name for CA certificate and server certificate/key pair.The following flags are optional. --webhook-kind Webhook kind, either MutatingWebhookConfiguration or ValidatingWebhookConfiguration (defaults to MutatingWebhookConfiguration)EOF exit 1&#125;while [ $## -gt 0 ]; do case $&#123;1&#125; in --service) service=&quot;$2&quot; shift ;; --webhook) webhook=&quot;$2&quot; shift ;; --secret) secret=&quot;$2&quot; shift ;; --namespace) namespace=&quot;$2&quot; shift ;; --webhook-kind) kind=&quot;$2&quot; shift ;; *) usage ;; esac shiftdone[ -z &quot;$&#123;service&#125;&quot; ] &amp;&amp; echo &quot;ERROR: --service flag is required&quot; &amp;&amp; exit 1[ -z &quot;$&#123;webhook&#125;&quot; ] &amp;&amp; echo &quot;ERROR: --webhook flag is required&quot; &amp;&amp; exit 1[ -z &quot;$&#123;secret&#125;&quot; ] &amp;&amp; echo &quot;ERROR: --secret flag is required&quot; &amp;&amp; exit 1[ -z &quot;$&#123;namespace&#125;&quot; ] &amp;&amp; echo &quot;ERROR: --namespace flag is required&quot; &amp;&amp; exit 1fullServiceDomain=&quot;$&#123;service&#125;.$&#123;namespace&#125;.svc&quot;## THE CN has a limit of 64 characters. We could remove the namespace and svc## and rely on the Subject Alternative Name (SAN), but there is a bug in EKS## that discards the SAN when signing the certificates.### https://github.com/awslabs/amazon-eks-ami/issues/341if [ $&#123;#fullServiceDomain&#125; -gt 64 ] ; then echo &quot;ERROR: common name exceeds the 64 character limit: $&#123;fullServiceDomain&#125;&quot; exit 1fiif [ ! -x &quot;$(command -v openssl)&quot; ]; then echo &quot;ERROR: openssl not found&quot; exit 1ficsrName=$&#123;service&#125;.$&#123;namespace&#125;tmpdir=$(mktemp -d)echo &quot;creating certs in tmpdir $&#123;tmpdir&#125; &quot;cat &lt;&lt;EOF &gt;&gt; &quot;$&#123;tmpdir&#125;/csr.conf&quot;[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentextendedKeyUsage = serverAuthsubjectAltName = @alt_names[alt_names]DNS.1 = $&#123;service&#125;DNS.2 = $&#123;service&#125;.$&#123;namespace&#125;DNS.3 = $&#123;fullServiceDomain&#125;DNS.4 = $&#123;fullServiceDomain&#125;.cluster.localEOFecho &quot;/CN=$&#123;fullServiceDomain&#125;&quot;openssl genrsa -out &quot;$&#123;tmpdir&#125;/server-key.pem&quot; 2048#openssl req -new -key &quot;$&#123;tmpdir&#125;/server-key.pem&quot; -subj &quot;/CN=$&#123;fullServiceDomain&#125;&quot; -out &quot;$&#123;tmpdir&#125;/server.csr&quot; -config &quot;$&#123;tmpdir&#125;/csr.conf&quot;openssl req -new -key &quot;$&#123;tmpdir&#125;/server-key.pem&quot; -subj &quot;/CN=system:node:$&#123;fullServiceDomain&#125;;/O=system:nodes&quot; -out &quot;$&#123;tmpdir&#125;/server.csr&quot; -config &quot;$&#123;tmpdir&#125;/csr.conf&quot;set +e## clean-up any previously created CSR for our service. Ignore errors if not present.if kubectl delete csr &quot;$&#123;csrName&#125;&quot;; then echo &quot;WARN: Previous CSR was found and removed.&quot;fiset -e## create server cert/key CSR and send it to k8s apicat &lt;&lt;EOF | kubectl create -f -apiVersion: certificates.k8s.io/v1kind: CertificateSigningRequestmetadata: name: $&#123;csrName&#125;spec: #signerName: kubernetes.io/kube-apiserver-client signerName: kubernetes.io/kubelet-serving groups: - system:authenticated request: $(base64 &lt; &quot;$&#123;tmpdir&#125;/server.csr&quot; | tr -d &#x27;\\n&#x27;) usages: - server auth - digital signature - key enciphermentEOFset +e## verify CSR has been createdwhile true; do if kubectl get csr &quot;$&#123;csrName&#125;&quot;; then echo &quot;CertificateSigningRequest create succsee&quot; break fidoneset -e## approve and fetch the signed certificate . !! not working with k8s 1.19.1, running the command separately outside of the container / nodeset +ewhile true; do if kubectl certificate approve &quot;$&#123;csrName&#125;&quot;; then echo &quot;$&#123;csrName&#125; certificate approve&quot; break fidoneset -eset +e## verify certificate has been signedi=1while [ &quot;$i&quot; -ne 10 ]do serverCert=$(kubectl get csr &quot;$&#123;csrName&#125;&quot; -o jsonpath=&#x27;&#123;.status.certificate&#125;&#x27;) if [ &quot;$&#123;serverCert&#125;&quot; != &#x27;&#x27; ]; then break fi sleep 5 i=$((i + 1))doneset -eif [ &quot;$&#123;serverCert&#125;&quot; = &#x27;&#x27; ]; then echo &quot;ERROR: After approving csr $&#123;csrName&#125;, the signed certificate did not appear on the resource. Giving up after 10 attempts.&quot; &gt;&amp;2 exit 1fiecho &quot;$&#123;serverCert&#125;&quot; | openssl base64 -d -A -out &quot;$&#123;tmpdir&#125;/server-cert.pem&quot;## create the secret with CA cert and server cert/keykubectl create secret tls &quot;$&#123;secret&#125;&quot; \\ --key=&quot;$&#123;tmpdir&#125;/server-key.pem&quot; \\ --cert=&quot;$&#123;tmpdir&#125;/server-cert.pem&quot; \\ --dry-run -o yaml | kubectl -n &quot;$&#123;namespace&#125;&quot; apply -f -#caBundle=$(base64 &lt; /run/secrets/kubernetes.io/serviceaccount/ca.crt | tr -d &#x27;\\n&#x27;)caBundle=$(cat $&#123;tmpdir&#125;/server-cert.pem)set +e## Patch the webhook adding the caBundle. It uses an `add` operation to avoid errors in OpenShift because it doesn&#x27;t set## a default value of empty string like Kubernetes. Instead, it doesn&#x27;t create the caBundle key.## As the webhook is not created yet (the process should be done manually right after this job is created),## the job will not end until the webhook is patched.while true; do echo &quot;INFO: Trying to patch webhook adding the caBundle.&quot; if kubectl patch &quot;$&#123;kind:-mutatingwebhookconfiguration&#125;&quot; &quot;$&#123;webhook&#125;&quot; --type=&#x27;json&#x27; -p &quot;[&#123;&#x27;op&#x27;: &#x27;add&#x27;, &#x27;path&#x27;: &#x27;/webhooks/0/clientConfig/caBundle&#x27;, &#x27;value&#x27;:&#x27;$&#123;serverCert&#125;&#x27;&#125;]&quot;; then break fi echo &quot;INFO: webhook not patched. Retrying in 5s...&quot; sleep 5done 执行脚本生成证书： –service：对应webhook api服务的service name –webhook：我们创建的MutatingWebhookConfiguration的webhooks的name –secret：我们的webhook api服务pod所需要挂载的secret的名称 –namespace：命名空间 1./generate_certificate.sh --service hello-webhook-service --webhook hello-webhook.leclouddev.com --secret hello-tls-secret --namespace default 1234567891011121314## 输出日志creating certs in tmpdir /var/folders/m6/11gz2m9x1m11lkts0h83x8800000gn/T/tmp.ebrOVBA0 /CN=hello-webhook-service.default.svcError from server (NotFound): certificatesigningrequests.certificates.k8s.io &quot;hello-webhook-service.default&quot; not foundcertificatesigningrequest.certificates.k8s.io/hello-webhook-service.default createdNAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITIONhello-webhook-service.default 0s kubernetes.io/kubelet-serving kubernetes-admin &lt;none&gt; PendingCertificateSigningRequest create succseecertificatesigningrequest.certificates.k8s.io/hello-webhook-service.default approvedhello-webhook-service.default certificate approveW1122 10:40:41.297220 55111 helpers.go:692] --dry-run is deprecated and can be replaced with --dry-run=client.secret/hello-tls-secret configuredINFO: Trying to patch webhook adding the caBundle.mutatingwebhookconfiguration.admissionregistration.k8s.io/hello-webhook.leclouddev.com patched 创建一个带hello&#x3D;true标签的pod1kubectl run busybox-1 --image=busybox --restart=Never -l=app=busybox,hello=true -- sleep 3600 查看webhook api的pod日志 1kubectl logs hello-webhook-deployment-5957bbb8bf-mkrzv 输出日志 12Listening on port 80002023/11/22 02:46:16 [hello-webhook-deployment-5957bbb8bf-mkrzv/JbYpz7vK1i-000001] &quot;POST https://hello-webhook-service.default.svc:443/mutate?timeout=10s HTTP/1.1&quot; from 10.244.235.192:46335 - 200 1393B in 1.844861ms 查看busybox-1的volume,可以看到cm已经挂载进去了 12345volumes:- configMap: defaultMode: 420 name: hello-configmap name: hello-volume 创建一个不带hello&#x3D;true标签的则不会挂载1kubectl run busybox-2 --image=busybox --restart=Never -l=app=busybox -- sleep 3600 查看busybox-2的yaml并没有对应的volume","categories":[{"name":"k8s","slug":"k8s","permalink":"https://falser101.github.io/categories/k8s/"}],"tags":[{"name":"admissionWebhook","slug":"admissionWebhook","permalink":"https://falser101.github.io/tags/admissionWebhook/"}],"author":"falser101"},{"title":"sed中使用环境变量","slug":"2023/sed中使用环境变量","date":"2023-10-16T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2023/10/17/2023/sed中使用环境变量/","permalink":"https://falser101.github.io/2023/10/17/2023/sed%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/","excerpt":"","text":"如何在Sed命令中使用环境变量 https://cn.linux-console.net/?p=15424 有一个test.conf文件内容如下 1key=value 我们执行sed命令进行普通字符串的替换，可以看到能成功的替换 123[root@k8s-node1 conf]# sed -i &quot;s/^key=.*/key=1/g&quot; test.conf[root@k8s-node1 conf]# cat test.confkey=1 我们再创建一个test.sh脚本在sed命令中使用环境变量 123#!/bin/shMY_VALUE=/home/testsed -i &#x27;s/^key=.*/key=$MY_VALUE/g&#x27; test.conf 执行脚本 12chmod +x test.sh./test.sh 执行后会发现报错，这是因为我们的环境变量中有&#x2F;会干扰“s”命令sed：-e 表达式 #1，字符 23：未终止的“s”命令 可以使用其他字符作为s命令的分隔符 我们修改test.sh脚本,使用#作为分隔符 123#!/bin/shMY_VALUE=/home/testsed -i &#x27;s#^key=.*#key=$MY_VALUE#g&#x27; test.conf 这时没有报错，但是替换的内容不正确，$MY_VALUE被当成了普通字符串进行替换了 12[root@k8s-node1 conf]# cat test.confkey=$MY_VALUE 再次修改test.sh,将单引号修改为双引号 123#!/bin/shMY_VALUE=/home/testsed -i &quot;s#^key=.*#key=$MY_VALUE#g&quot; test.conf 再次执行脚本,可以看到成功用环境变量替换了文件中的内容 123[root@k8s-node1 conf]# ./test.sh[root@k8s-node1 conf]# cat test.confkey=/home/test","categories":[{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/tags/linux/"}],"author":"falser101"},{"title":"go1.21新增日志库slog详解","slug":"2023/go1.21新增日志库slog详解","date":"2023-10-06T16:00:00.000Z","updated":"2025-06-16T07:17:39.756Z","comments":true,"path":"2023/10/07/2023/go1.21新增日志库slog详解/","permalink":"https://falser101.github.io/2023/10/07/2023/go1.21%E6%96%B0%E5%A2%9E%E6%97%A5%E5%BF%97%E5%BA%93slog%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"slog 结构化日志记录 Go 1.21 中的新 log&#x2F;slog 软件包为标准库带来了结构化日志记录。结构化日志使用键值对，因此可以快速可靠地解析、过滤、搜索和分析它们。对于服务器来说，日志记录是开发人员观察系统详细行为的重要方式，通常是他们调试系统的第一个地方。因此，日志往往数量庞大，快速搜索和过滤它们的能力至关重要 快速使用12345678910package mainimport &quot;log/slog&quot;func main() &#123; slog.Info(&quot;hello, world&quot;)&#125;# 输出内容如下2023/010/07 16:09:19 INFO hello, world 除Info之外，还有用于其他三个级别的函数 Debug 、 Warn 和 Error以及一个将级别作为参数的更通用 Log 的函数。在slog中，级别只是整数，因此不限于四个命名级别。例如，Info为0且Warn为4，因此，如果您的日志记录系统具有介于两者之间的级别，则可以使用2。 与 log 包不同，我们可以轻松地将键值对添加到我们的输出中，方法是将它们写入消息之后： 1234slog.Info(&quot;hello, world&quot;, &quot;user&quot;, os.Getenv(&quot;USER&quot;))# 输出2023/10/07 16:27:19 INFO hello, world user=falser slog 顶级函数使用默认logger。我们可以显式获取此logger，并调用其方法 12logger := slog.Default()logger.Info(&quot;hello, world&quot;, &quot;user&quot;, os.Getenv(&quot;USER&quot;)) 我们可以通过更改logger使用的处理程序来更改输出。 slog 带有两个内置处理程序。TextHandler以key&#x3D;value的形式发出所有日志信息。此程序使用创建一个 TextHandler新的logger，并对Info该方法进行相同的调用： 12345logger := slog.New(slog.NewTextHandler(os.Stdout, nil))logger.Info(&quot;hello, world&quot;, &quot;user&quot;, os.Getenv(&quot;USER&quot;))# 输出time=2023-10-07T16:56:03.786-04:00 level=INFO msg=&quot;hello, world&quot; user=falser JsonHandler: 12345logger := slog.New(slog.NewJSONHandler(os.Stdout, nil))logger.Info(&quot;hello, world&quot;, &quot;user&quot;, os.Getenv(&quot;USER&quot;))# 输出&#123;&quot;time&quot;:&quot;2023-10-07T16:58:02.939245411-04:00&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;msg&quot;:&quot;hello, world&quot;,&quot;user&quot;:&quot;falser&quot;&#125; 也可以自己实现slog.Handler接口 对于频繁执行的日志语句，使用该 Attr 类型和调用 LogAttrs 方法可能更有效。 1slog.LogAttrs(context.Background(), slog.LevelInfo, &quot;hello, world&quot;, slog.String(&quot;user&quot;, os.Getenv(&quot;USER&quot;))) 123slog.Info(&quot;message&quot;, &quot;k1&quot;, v1, &quot;k2&quot;, v2)# 这种方式可读性更高不易出错slog.Info(&quot;message&quot;, slog.Int(&quot;k1&quot;, v1), slog.String(&quot;k2&quot;, v2))","categories":[{"name":"golang","slug":"golang","permalink":"https://falser101.github.io/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://falser101.github.io/tags/golang/"}],"author":"falser101"},{"title":"Operator开发","slug":"2024/Operator开发","date":"2023-10-06T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2023/10/07/2024/Operator开发/","permalink":"https://falser101.github.io/2023/10/07/2024/Operator%E5%BC%80%E5%8F%91/","excerpt":"","text":"基于Operatorframework sdk进行Operator的开发 使用OperatorSdk开发环境要求 安装Operator SDK CLI git go，环境变量（export GO111MODULE&#x3D;on） docker kubectl 开始创建并初始化项目123mkdir memcached-operatorcd memcached-operatoroperator-sdk init --domain example.com --repo github.com/example/memcached-operator 创建新的 API 和控制器创建一个新的自定义资源定义 （CRD） API，其中包含组 cache 版本 v1alpha1 和内存缓存类型。出现提示时，输入yes以创建资源和控制器。 12345$ operator-sdk create api --group cache --version v1alpha1 --kind Memcached --resource --controllerWriting scaffold for you to edit...api/v1alpha1/memcached_types.gocontrollers/memcached_controller.go... 定义API123456789101112131415161718192021222324// 添加 +kubebuilder:subresource:status 标记以将状态子资源添加到 CRD 清单，以便控制器可以在不更改 CR 对象的其余部分的情况下更新 CR 状态//+kubebuilder:subresource:statustype MemcachedSpec struct &#123; // 校验 可查阅https://book.kubebuilder.io/reference/markers/crd-validation.html // +kubebuilder:validation:Minimum=1 // +kubebuilder:validation:Maximum=5 // +kubebuilder:validation:ExclusiveMaximum=false // 实例数 // +operator-sdk:csv:customresourcedefinitions:type=spec Size int32 `json:&quot;size,omitempty&quot;` // 容器的端口 // +operator-sdk:csv:customresourcedefinitions:type=spec ContainerPort int32 `json:&quot;containerPort,omitempty&quot;`&#125;// Memcached的状态type MemcachedStatus struct &#123; // 存储Memcached实例的状态条件 // +operator-sdk:csv:customresourcedefinitions:type=status Conditions []metav1.Condition `json:&quot;conditions,omitempty&quot; patchStrategy:&quot;merge&quot; patchMergeKey:&quot;type&quot; protobuf:&quot;bytes,1,rep,name=conditions&quot;`&#125; 定义完成后，请运行以下命令以更新为该资源类型生成的代码 1make generate 上面的 makefile 目标将调用controller-gen来更新 api/v1alpha1/zz_generated.deepcopy.go 文件，以确保我们的 API 的 Go 类型定义实现所有 Kind 类型必须实现的 runtime.Object 接口。 生成 CRD 清单使用规范&#x2F;状态字段和 CRD 验证标记定义 API 后，可以使用以下命令生成和更新 CRD 清单 1make manifests 此makefile目标将调用controller-gen以生成config/crd/bases/cache.example.com_memcacheds.yaml CRD 清单。 实现Controller协调循环协调功能负责对系统的实际状态强制实施所需的 CR 状态。每当在监视的 CR 或资源上发生事件时，它都会运行，并且会根据这些状态是否匹配返回一些值 每个控制器都有一个协调器对象，其中包含一个 Reconcile() 实现协调循环的方法。协调循环传递参数， Request 该参数是用于从缓存中查找主要资源对象 Memcached 的命名空间&#x2F;名称键 12345678910111213import ( ctrl &quot;sigs.k8s.io/controller-runtime&quot; cachev1alpha1 &quot;github.com/example/memcached-operator/api/v1alpha1&quot; ...)func (r *MemcachedReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) &#123; // Lookup the Memcached instance for this reconcile request memcached := &amp;cachev1alpha1.Memcached&#123;&#125; err := r.Get(ctx, req.NamespacedName, memcached) ...&#125; 以下是协调程序的一些可能的返回选项： With the error: 1return ctrl.Result&#123;&#125;, err Without an error: 1return ctrl.Result&#123;Requeue: true&#125;, nil Therefore, to stop the Reconcile, use: 1return ctrl.Result&#123;&#125;, nil Reconcile again after X time: 1return ctrl.Result&#123;RequeueAfter: 5 * time.Minute&#125;, nil 指定权限并生成 RBAC 清单控制器需要某些 RBAC 权限才能与其管理的资源进行交互。这些标记通过如下所示的 RBAC 标记指定： 12345678910//+kubebuilder:rbac:groups=cache.example.com,resources=memcacheds,verbs=get;list;watch;create;update;patch;delete//+kubebuilder:rbac:groups=cache.example.com,resources=memcacheds/status,verbs=get;update;patch//+kubebuilder:rbac:groups=cache.example.com,resources=memcacheds/finalizers,verbs=update//+kubebuilder:rbac:groups=core,resources=events,verbs=create;patch//+kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;create;update;patch;delete//+kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watchfunc (r *MemcachedReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) &#123; ...&#125; ClusterRole 清单 at config&#x2F;rbac&#x2F;role.yaml 是使用以下命令通过控制器生成从上述标记生成的： 1make manifests 构建镜像可修改Makefil 的镜像名称 12## IMG ?= controller:latestIMG ?= $(IMAGE_TAG_BASE):$(VERSION) 修改后执行以下命令 1make docker-build docker-push 运行operator本地在集群外部执行1make install run 在群集内作为Deployment运行1make deploy 验证 memcached-operator是否已启动并正在运行 123$ kubectl get deployment -n memcached-operator-systemNAME READY UP-TO-DATE AVAILABLE AGEmemcached-operator-controller-manager 1/1 1 1 8m 创建Memcached CR资源更新 config&#x2F;samples&#x2F;cache_v1alpha1_memcached.yaml，并定义 spec 如下： 1234567apiVersion: cache.example.com/v1alpha1kind: Memcachedmetadata: name: memcached-samplespec: size: 3 containerPort: 11211 创建CR: 1kubectl apply -f config/samples/cache_v1alpha1_memcached.yaml 清除12kubectl delete -f config/samples/cache_v1alpha1_memcached.yamlmake undeploy","categories":[{"name":"k8s","slug":"k8s","permalink":"https://falser101.github.io/categories/k8s/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/tags/linux/"}],"author":"falser101"},{"title":"nginx 配置https访问云服务器","slug":"2023/nginx 配置https访问云服务器","date":"2023-07-11T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2023/07/12/2023/nginx 配置https访问云服务器/","permalink":"https://falser101.github.io/2023/07/12/2023/nginx%20%E9%85%8D%E7%BD%AEhttps%E8%AE%BF%E9%97%AE%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/","excerpt":"","text":"第一步首先自己购买一个服务器 第二步购买一个域名挂载到服务器，各个域名服务商有详细的文档这里就不再写了 第三步申请ssl证书并且将证书和需要部署的静态页面上传到服务器如下图腾讯SSL证书域名验证指引 编写default.conf下面是我自己的证书和域名 123456789101112131415161718192021222324252627282930server &#123; #SSL 访问端口号为 443 listen 443 ssl; #填写绑定证书的域名 server_name falser.top; #证书文件名称 ssl_certificate 1_falser.top_bundle.crt; #私钥文件名称 ssl_certificate_key 2_falser.top.key; ssl_session_timeout 5m; #请按照以下协议配置 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #请按照以下套件配置，配置加密套件，写法遵循 openssl 标准。 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE; ssl_prefer_server_ciphers on; location / &#123; #网站主页路径。此路径仅供参考，具体请您按照实际目录操作。 root html; index index.html index.htm; &#125;&#125;server &#123; listen 80; #填写绑定证书的域名 server_name falser.top; #把http的域名请求转成https return 301 https://$host$request_uri;&#125; 编写Dockerfile1234567891011# 设置基础镜像FROM nginx# 定义作者MAINTAINER falser &lt;1023535569@qq.com&gt;# 将dist文件中的内容复制到 /etc/nginx/html/ 这个目录下面COPY dist/ /etc/nginx/html/#用本地的 default.conf 配置来替换nginx镜像里的默认配置COPY default.conf /etc/nginx/conf.d/default.conf#将证书拷贝到docker镜像中的/etc/nginx/目录下COPY 1_falser.top_bundle.crt /etc/nginx/COPY 2_falser.top.key /etc/nginx/ 创建镜像并启动1docker build -t falser . 1docker run -d -p 443:443 -p 80:80 --name fblog falser 启动成功","categories":[{"name":"frontend","slug":"frontend","permalink":"https://falser101.github.io/categories/frontend/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://falser101.github.io/tags/nginx/"}],"author":"falser101"},{"title":"JAVA中的代理模式","slug":"2022/01.JAVA中的代理模式","date":"2023-01-08T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2023/01/09/2022/01.JAVA中的代理模式/","permalink":"https://falser101.github.io/2023/01/09/2022/01.JAVA%E4%B8%AD%E7%9A%84%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"代理模式 静态代理在Java中手动实现代理模式非常简单，定义一个接口，一个实现类和一个持有实现类的代理类，下面以短信发送为例 短信发送接口 12345package proxy;public interface SmsService &#123; void send(String str);&#125; 短信发送实现类 12345678package proxy;public class SmsServiceImpl implements SmsService &#123; @Override public void send(String str) &#123; System.out.println(&quot;impl send &quot; + str); &#125;&#125; 短信发送代理类 12345678910111213141516package proxy;public class SmsProxy implements SmsService &#123; private final SmsService smsService; public SmsProxy(SmsService smsService) &#123; this.smsService = smsService; &#125; @Override public void send(String str) &#123; System.out.println(&quot;before send&quot;); this.smsService.send(str); System.out.println(&quot;after send&quot;); &#125;&#125; 总结不够灵活，每个目标类都需要创建一个代理类，接口一旦变更，目标类和代理类都需要进行修改 动态代理就 Java 来说，动态代理的实现方式有很多种，比如 JDK 动态代理、CGLIB 动态代理等等 JDK动态代理主要类和接口，代理类需要实现InvocationHandler的invoke方法，再通过Proxy.newProxyInstance(ClassLoader loader, Class&lt;?&gt;[]interfaces, InvocationHandler h);方法获取代理类的实例，从而完成目标类的增强；还是以短信发送为例子 短信发送接口 12345package proxy;public interface SmsService &#123; void send(String str);&#125; 实现类 12345678package proxy;public class JdkSmsServiceImpl implements SmsService &#123; @Override public void send(String str) &#123; System.out.printf(&quot;impl send %s&quot;, str); &#125;&#125; 代理类 12345678910111213141516171819202122232425package proxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;public class JdkSmsProxy implements InvocationHandler &#123; /** * 代理类中的真实对象 */ private final Object target; public JdkSmsProxy(Object target) &#123; this.target = target; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //调用方法之前，我们可以添加自己的操作 System.out.println(&quot;before method &quot; + method.getName()); Object result = method.invoke(target, args); //调用方法之后，我们同样可以添加自己的操作 System.out.println(&quot;after method &quot; + method.getName()); return result; &#125;&#125; 获取代理类实例 12345678910111213141516171819public class JdkProxyFactory &#123; public static Object getProxy(Object target) &#123; return Proxy.newProxyInstance( // 目标类的类加载 target.getClass().getClassLoader(), // 代理需要实现的接口，可指定多个 target.getClass().getInterfaces(), // 代理对象对应的自定义 InvocationHandler new JdkSmsProxy(target) ); &#125;&#125;public class Main &#123; public static void main(String[] args) throws NoSuchMethodException &#123; SmsService smsService = (SmsService) JdkProxyFactory.getProxy(new JdkSmsServiceImpl()); smsService.send(&quot;proxy&quot;); &#125;&#125; 总结JDK 动态代理有一个最致命的问题是其只能代理实现了接口的类 CGLIB动态代理CGLIBopen in new window(Code Generation Library)是一个基于ASMopen in new window的字节码生成库，它允许我们在运行时对字节码进行修改和动态生成。CGLIB通过继承方式实现代理Spring 中的 AOP 模块中：如果目标对象实现了接口，则默认采用 JDK 动态代理，否则采用 CGLIB 动态代理。 在 CGLIB 动态代理机制中 MethodInterceptor 接口和 Enhancer 类是核心 目标类 12345678package proxy;public class CgLibService &#123; public void send(String str) &#123; System.out.println(&quot;send &quot; + str); &#125;&#125; 目标类拦截器 123456789101112131415161718192021222324package proxy;import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import java.lang.reflect.Method;public class CgLibInterceptor implements MethodInterceptor &#123; /** * @param o 被代理的对象（需要增强的对象） * @param method 被拦截的方法（需要增强的方法） * @param args 方法入参 * @param methodProxy 用于调用原始方法 */ @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; //调用方法之前，我们可以添加自己的操作 System.out.println(&quot;before method &quot; + method.getName()); Object object = methodProxy.invokeSuper(o, args); //调用方法之后，我们同样可以添加自己的操作 System.out.println(&quot;after method &quot; + method.getName()); return object; &#125;&#125; 通过Enhancer创建代理类 12345678910111213141516171819package proxy;import net.sf.cglib.proxy.Enhancer;public class CglibProxyFactory &#123; public static Object getProxy(Class&lt;?&gt; clazz) &#123; // 创建动态代理增强类 Enhancer enhancer = new Enhancer(); // 设置类加载器 enhancer.setClassLoader(clazz.getClassLoader()); // 设置被代理类 enhancer.setSuperclass(clazz); // 设置方法拦截器 enhancer.setCallback(new CgLibInterceptor()); // 创建代理类 return enhancer.create(); &#125;&#125; 获取代理类并执行 123456public class Main &#123; public static void main(String[] args) throws NoSuchMethodException &#123; CgLibService smsService = (CgLibService) CglibProxyFactory.getProxy(CgLibService.class); smsService.send(&quot;test&quot;); &#125;&#125; 总结JDK 动态代理只能代理实现了接口的类或者直接代理接口，而 CGLIB 可以代理未实现任何接口的类。 另外， CGLIB动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为 final 类型的类和方法。就二者的效率来说，大部分情况都是JDK 动态代理更优秀，随着 JDK 版本的升级，这个优势更加明显。","categories":[{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://falser101.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"author":"falser101"},{"title":"垃圾回收概述","slug":"2021/08.垃圾回收概述","date":"2021-03-03T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2021/03/04/2021/08.垃圾回收概述/","permalink":"https://falser101.github.io/2021/03/04/2021/08.%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%A6%82%E8%BF%B0/","excerpt":"","text":"垃圾回收概述 什么是垃圾什么是垃圾( Garbage) 呢? 垃圾是指在运行程序中没有任何指针指向的对象，这个对象就是需要被回收的垃圾。 外文: An object is considered garbage when it can no longer be reached from any pointer in the running program. 如果不及时对内存中的垃圾进行清理，那么，这些垃圾对象所占的内存空间会一直保留到应用程序结束，被保留的空间无法被其他对象使用。甚至可能导致内存溢出。 为什么需要GC 对于高级语言来说，一个基本认知是如果不进行垃圾回收，内存迟早都会被消耗完，因为不断地分配内存空间而不进行回收，就好像不停地生产生活垃圾而从来不打扫一样。 除了释放没用的对象，垃圾回收也可以清除内存里的记录碎片。碎片整理将所占用的堆内存移到堆的一端，以便JVM将整理出的内存分配给新的对象。 随着应用程序所应付的业务越来越庞大、复杂，用户越来越多，没有GC就不能保证应用程序的正常进行。而经常造成STW的GC又跟不上实际的需求，所以才会不断地尝试对GC进行优化。 早期垃圾回收在早期的C&#x2F;C+ +时代，垃圾回收基本上是手工进行的。开发人员可以使用new关键字进行内存申请，并使用delete关键字进行内存释放。比如以下代码: 1234MibBridge *pBridge = new cmBaseGroupBridge ();//如果注册失败，使用Delete释放该对象所占内存区域if (pBridge-&amp;gt;Register (kDestroy) != NO_ ERROR)delete pBridge; 这种方式可以灵活控制内存释放的时间，但是会给开发人员带来频繁申请和释放内存的管理负担。倘若有一处内存区间由于程序员编码的问题忘记被回收，那么就会产生内存泄漏，垃圾对象永远无法被清除，随着系统运行时间的不断增长，垃圾对象所耗内存可能持续上升，直到出现内存溢出并造成应用程序崩溃。 Java垃圾回收机制 自动内存管理，无需开发人员手动参与内存的分配与回收，这样降低内存泄漏和内存溢出的风险 没有垃圾回收器，java也会和cpp-样，各种悬垂指针，野指针，泄露问题让你头疼不已。 自动内存管理机制，将程序员从繁重的内存管理中释放出来，可以更专心地专注于业务开发 垃圾回收器可以对年轻代回收，也可以对老年代回收，甚至是全堆和方法区的回收。 其中，Java堆是垃圾收集器的工作重点。 从次数上讲: 频繁收集Young区 较少收集Old区 基本不动Perm区(或元空间) 缺点 对于Java开发人员而言，自动内存管理就像是一个黑匣子，如果过度依赖于自动”，那么这将会是一一场灾难，最严重的就会弱化Java开发人员在程序出现内存溢出时定位问题和解决问题的能力。 此时，了解JVM的自动内存分配和内存回收原理就显得非常重要，只有在真正了解JVM是如何管理内存后，我们才能够在遇见OutOfMemoryError时,快速地根据错误异常日志定位问题和解决问题。 当需要排查各种内存溢出、内存泄漏问题时，当垃圾收集成为系统达到更高并发量的瓶颈时，我们就必须对这些“自动化”的技术实施必要的监控和调节。","categories":[{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://falser101.github.io/tags/jvm/"}]},{"title":"方法区","slug":"2021/07.方法区","date":"2021-02-14T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2021/02/15/2021/07.方法区/","permalink":"https://falser101.github.io/2021/02/15/2021/07.%E6%96%B9%E6%B3%95%E5%8C%BA/","excerpt":"","text":"方法区概述 栈、堆、方法区的交互关系 方法区的理解方法区在那儿？《Java虚拟机规范》中明确说明： “尽管所有的方法区在逻辑上是属于堆的一部分，但一些简单的实现可能不会选择去进行垃圾收集或者进行压缩。”但对于HotSpotJVM而言，方法区还有一个别名叫做Non—Heap （非堆）， 目的就是要和堆分开。所以，方法区看作是一块独立于Java堆的内存空间。 方法区的基本理解 方法区（Method Area）与Java堆一样，是各个线程共享的内存区域。 方法区在JVM启动的时候被创建，并且它的实际的物理内存空间中和Java堆区一样都可以是不连续的。 方法区的大小，跟堆空间一样，可以选择固定大小或者可扩展。 方法区的大小决定了系统可以保存多少个类，如果系统定义了太多的类，导致方法区溢出，虚拟机同样会抛出内存溢出错误： java.lang .OutofMemoryError： PermGen space或者java. lang.OutofMemoryError: Metaspace 关闭JVM就会释放这个区域的内存。 元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永夕代最大的区别在于：元空间不在虚拟机设置的内存中，而是使用本地内存。 永久代、元空间二者并不只是名字变了，内部结构也调整了。 根据《Java虚拟机规范》的规定，如果方法区无法满足新的内存分配需求时，将抛出OOM异常。 设置方法区大小与OOMJDK7方法区的大小不必是固定的， jvm可以根据应用的需要动态调整。 通过—xx： Permsize来设置永久代初始分配空间。默认值是20.75M xx： MaxPermsize来设定永久代最大可分配空间。32位机器默认是64M， 64位机器模式是82M 当JVM加载的类信息容量超过了这个值，会报异常outofMemoryError ： PermGenspace 。 JDK8 元数据区大小可以使用参数-XX：Metaspacesize和-XX：MaxMetaspacesize指定，替代上述原有的两个参数。 默认值依赖于平台.windows下,-XX:Metaspacesize是21M, -XX:MaxMetaspacesize的值是-1，即没有限制。 与永久代不同，如果不指定大小，默认情况下，虚拟机会耗尽所有的可用系统内存。如果元数据区发生溢出，虚拟机一样会抛出异常outOfMemoryError： Metaspace -XX：Metaspacesize：设置初始的元空间大小。对于一个64位的服务器端JVM来说，其默认的—Xx： MetaspaceSize值为21MB.这就是初始的高水位线，一旦触及这个水位线， Full GC将会被触发并卸载没用的类（即这些类对应的类加载器不再存活）然后这个高水位线将会重置。新的高水位线的值取决于Gc后释放了多少元空间。如果释放的空间不足，那么在不超过MaxMetaspacesize时，适当提高该值。如果释放空间过多，则适当降低该值。 如果初始化的高水位线设置过低，上述高水位线调整情况会发生很多次。通过垃圾回收器的日志可以观察到FULL GC多次调用。为了避免频繁地GC ，建议将-XX：Metaspacesize设置为一个相对较高的值。 方法区的内部结构 方法区存储什么《深入理解Java虚拟机》书中对方法区（Method Area）存储内容描述如下：它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等。 小结：常量池，可以看做是一张表，虚拟机指令根据这张常量表找到要执行的类名、方法名、参数类型、字面量等类型。 运行时常量池 运行时常量池（Runtime Constant Pool）是方法区的一部分。 常量池表（Constant Pool Table）是Class文件的一部分，用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 运行时常量池，在加载类和接口到虚拟机后，就会创建对应的运行时常量池。 JVM为每个已加载的类型（类或接口）都维护一个常量池。池中的数据项像数组项一样，是通过索引访问的。 运行时常量池中包含多种不同的常量，包括编译期就已经明确的数值字面量，也包括到运行期解析后才能够获得的方法或者字段引用。此时不再是常量池中的符号地址了，这里换为真实地址。 运行时常量池，相对于Class文件常量池的另一重要特征是：具备动态性。string.intern () 运行时常量池类似于传统编程语言中的符号表（symbol table） ，但是它所包含的数据却比符号表要更加丰富一些 当创建类或接口的运行时常量池时，如果构造运行时常量池所需的内存空间超过了方法区所能提供的最大值，则JVM会抛outofMemoryError异常。 方法区的演进细节 永久代为什么被元空间代替 随着Java8的到来， HotSpot vM中再也见不到永久代了。但是这并不意味着类的元数据信息也消失了。这些数据被移到了一个与堆不相连的本地内存区域，这个区域叫做元空间( Metaspace） 由于类的元数据分配在本地内存中，元空间的最大可分配空间就是系统可用内存空间。 这项改动是很有必要的，原因有： 永久代设置空间大小是很难确定的。 如果动态加载类过多，容易产生Perm区的00M。比如某个实际Web工程中，因为功能点比较多，在运行过程中，要不断动态加载很多类，经常出现致命错误。”Exception in thread ‘dubbo client x.x connector’ java.lang.OutOMemoryError: PermGen space而元空间和永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制。 永久代进行调优是很困难的。 方法区的垃圾回收StringTable为什么要调整？ jdk7中将stringmable放到了堆空间中。因为永久代的回收效率很低，在full gc的时候才会触发。而full gc是老年代的空间不足、永久代不足时才会触发。 这就导致stringTable回收效率不高。而我们开发中会有大量的字符串被创建，回收效率低，导致永久代内存不足。放到堆里，能及时回收内存。 总结","categories":[{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://falser101.github.io/tags/jvm/"}]},{"title":"运行时数据区——堆","slug":"2021/06.运行时数据区——堆","date":"2021-02-13T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2021/02/14/2021/06.运行时数据区——堆/","permalink":"https://falser101.github.io/2021/02/14/2021/06.%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E2%80%94%E2%80%94%E5%A0%86/","excerpt":"","text":"1. 堆的核心概述 一个JVM实例只存在一个堆内存，堆也是Java内存管理的核心区域。 Java 堆区在JVM启动的时候即被创建，其空间大小也就确定了。是JVM管理的最大一块内存空间。 堆内存的大小是可以调节的。 《Java虚拟机规范》规定，堆可以处于物理上不连续的内存空间中，但在逻辑上它应该被视为连续的。 所有的线程共享Java堆，在这里还可以划分线程私有的缓冲区(Thread Local Allocation Buffer, TLAB) 。 《Java虛拟机规范》中对Java堆的描述是:所有的对象实例以及数组都应当在运行时分配在堆上。(The heap is the run-time data area from which memory for all cla3s instances and arrays is allocated ) 我要说的是: “几乎”所有的对象实例都在这里分配内存。一从实际使用角度看的。 数组和对象可能永远不会存储在栈上，因为栈帧中保存引用，这个引用指向对象或者数组在堆中的位置。 在方法结束后，堆中的对象不会马上被移除，仅仅在垃圾收集的时候才会被移除。 堆，是GC ( Garbage Collection， 垃圾收集器**)执行垃圾回收的重点区域**。 内存细分现代垃圾收集器大部分都基于分代收集理论设计,堆空间细分为: Java 7及之前堆内存逻辑上分为三部分:新生区+老年区+永久区 Young Generation Space 新生区 Young&#x2F;New 又被划分为Eden区和Survivor区 Tenure generation space 老年区 Old&#x2F; Tenure Permanent Space 永久区 Perm Java 8及之后堆内存逻辑上分为三部分:新生区+老年区+元空间 Young Generation Space 新生区 Young&#x2F;New 又被划分为Eden区和Survivor区 Tenure generation space 老年区 Old&#x2F; Tenure Meta Space 元空间 Meta 2. 设置堆内存大小与OOM-XX:+PrintGCDetails：打印GC详情jps：查看java进程jstat -gc java进程号：查看内存使用情况 Java堆区用于存 储Java对象实例，那么堆的大小在JVM启动时就已经设定好了，大家可以通过选项” -Xmx”和”-xms”来进行设置。 -Xms用来设置堆空间(年轻代+老年代)的初始内存大小， 等价于-XX: InitialHeapSize -X是jvm的运行参数 ms是memory start -Xmx用来设置堆空间(年轻代+老年代)的最大内存大小， 等价于-XX: MaxHeapSize : 一旦堆区中的内存大小超过”-Xmx”所指定的最大内存时，将会抛出OutOfMemoryError异常。 通常会将-Xms和-Xmx两个参数配置相同的值，其目的是为了能够在java垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小，从而提高性能。 默认情况下，初始内存大小:物理电脑内存大小&#x2F; 64 最大内存大小:物理电脑内存大小&#x2F; 4 3. 年轻代与老年代存储在JVM中的Java对象可以被划分为两类： 一类是生命周期较短的瞬时对象，这类对象的创建和消亡都非常迅速 另外一类对象的生命周期却非常长，在某些极端的情况下还能够与JVM的生命周期保持一致。 Java堆区进一步细分的话，可以划分为年轻代（YoungGen）和老年代（OldGen） 其中年轻代又可以划分为Eden空间、Survivor0空间和survivor1空间（有时也叫做from区、to区） 配置新生代与老年代在堆结构的占比(默认1:2)。 默认**-XX: NewRatio&#x3D;2**,表示新生代占1,**老年代占2,**新生代占整个堆的1&#x2F;3 可以修改**-XX: NewRatio&#x3D;4**,表示新生代占1,**老年代占4,**新生代占整个堆的1&#x2F;5 -XX:-UseAdaptivesizepoli 关闭自适应的内存分配策略 在 Hotspot中,Eden空间和另外两个 Survivor空间缺省所占的比例是8:1:1 当然开发人员可以通过选项“-xx: Survivorratio”调整这个空间比例。XX: SurvivorRatio&#x3D;8 几乎所有的Java对象都是在Eden区被new出来的 绝大部分的Java对象的销毁都在新生代进行了 IBM公司的专门研究表明,新生代中80%的对象都是“朝生夕死”的 可以使用选项**”-Xmn”**设置新生代最大内存大小 这个参数一般使用默认值就可以了 4. 图解对象分配过程为新对象分配内存是一件非 常严谨和复杂的任务，JVM的设计者们不仅需要考虑内存如何分配、在哪里分配等问题，并且由于内存分配算法与内存回收算法密切相关，所以还需要考虑GC执行完内存回收后是否会在内存空间中产生内存碎片。 new的对象先放伊甸园区。此区有大小限制。 当伊甸园的空间填满时，程序又需要创建对象，JVM的垃圾回收器将对伊甸园区进行垃圾回收(Minor GC)，将伊甸园区中的不再被其他对象所引用的对象进行销毁。再加载新的对象放到伊甸园区 然后将伊甸园中的剩余对象移动到幸存者0区。 如果再次触发垃圾回收，此时上次幸存下来的放到幸存者0区的，如果没有回收，就会放到幸存者1区。 如果再次经历垃圾回收，此时会重新放回幸存者0区，接着再去幸存者1区。 啥时候能去养老区呢?可以设置次数。默认是15次。 可以设置参数: -XX:MaxTenuringThreshold&#x3D;N 进行设置。 总结: 针对幸存者s0,s1区的总结：复制之后有交换，谁空谁是to 关于垃圾回收：频繁在新生区收集，很少在养老区收集，几乎不在永久区&#x2F;元空间收集。 5. Minor GC、Major GC、Full GCJVM在进行GC时，并非每次都对上面三个内存(新生代、老年代;方法区)区域一起回收的，大部分时候回收的都是指新生代。针对HotSpot VM的实现，它里面的GC按照回收区域又分为两大种类型:一 种是部分收集(Partial GC)，一种是整堆收集(Full GC) 部分收集:不是完整收集整个Java堆的垃圾收集。其中又分为: 新生代收集(Minor GC &#x2F; Young GC) :只是新生代(Eden\\Se,S1)的垃圾收集 老年代收集(MajorGC&#x2F; OldGC):只是老年代的垃圾收集。 目前，只有CMS GC会有单独收集老年代的行为。. 注意，很多时候Major GC会和Full GC混淆使用，需要具体分辨是老年代回收还是整堆回收。 混合收集(Mixed GC): 收集整个新生代以及部分老年代的垃圾收集。 目前，只有G1 GC会有这种行为 整堆收集(Fu11 GC):收集整个java堆和方法区的垃圾收集。 最简单的分代式GC策略的触发条件年轻代GC(Minor GC)触发机制: 当年轻代空间不足时， 就会触发Minor GC，这里的年轻代满指的是Eden区满，Survivor满不会引发GC（要么晋升，要么等待下一次Eden满进行GC）。(每次 Minor GC会清理年轻代的内存。) 因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。这一定义既清晰又易于理解。 Minor GC会引发STW， 暂停其它用户的线程，等垃圾回收结束，用户线程才恢复运行。 threshold &#x3D; 阈值默认为15 老年代GC (Major GC&#x2F;Fu11 GC)触发机制: 指发生在老年代的GC，对象从老年代消失时，我们说“Major GC”或“Fu1l GC”发生了。 出现了Major GC，经常会伴随至少一次的Minor GC (但非绝对的，在ParallelScavenge收集器的收集策略里就有直接进行MajorGC的策略选择过程) 也就是在老年代空间不足时，会先尝试触发Minor GC。如果之后空间还不足，则触发Major GC Major GC的速度一般会比Minor GC慢10倍以上，STW的时间更长。 如果Major GC后，内存还不足，就报OOM了。 Full GC触发机制:触发Full GC执行的情况有如下五种: 调用System. gc()时，系统建议执行Fu11 GC，但是不必然执行 老年代空间不足 方法区空间不足， 通过Minor GC后进入老年代的平均大小大于老年代的可用内存 由Eden区、survivor space日(From Space) 区向survivor space1 (To Space)区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小.说明: full gc是开发或调优中尽量要避免的。这样暂停时间会短一些。 6. 堆空间分代思想为什么需要把Java堆分代?不分代就不能正常工作了吗?其实不分代完全可以，分代的唯一理由就是优化Gc性能。如果没有分代，那所有的对象都在一块，就如同把一个学校的人都关在一个教室。GC的时候要找到哪些对象没用，这样就会对堆的所有区域进行扫描。而很多对象都是朝生夕死的，如果分代的话，把新创建的对象放到某一地方，当GC的时候先把这块存储“朝生夕死”对象的区域进行回收，这样就会腾出很大的空间出来。 7. 内存分配策略如果对象在Eden出生并经过第一次MinorGC 后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并将对 象年龄设为1。对象在Survivor区中每熬过一次MinorGC，年龄就增加1岁， 当它的年龄增加到一定程度(默认为15岁，其实每个JVM、每个GC都有所不同)时，就会被晋升到老年代中。对象晋升老年代的年龄阈值，可以通过选项-XX:MaxTenuringThreshold来设置。 针对不同年龄段的对象分配原则如下所示: 优先分配到Eden 大对象直接分配到老年代 尽量避免程序中出现过多的大对象 长期存活的对象分配到老年代 动态对象年龄判断 如果Survivor 区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代，无须等到MaxTenuri ngThreshold中要求的年龄。 空间分配担保（Eden和幸存者区放不下直接放到老年区） -XX: HandlePromotionFailure 8. 为对象分配内存: TLAB为什么有TLAB ( Thread Local Allocation Buffer ) ? 堆区是线程共享区域，任何线程都可以访问到堆区中的共享数据 由于对象实例的创建在JVM中非常频繁，因此在并发环境下从堆区中划分内存空间是线程不安全的 为避免多个线程操作同一地址，需要使用加锁等机制，进而影响分配速度。 什么是TLAB ? 从内存模型而不是垃圾收集的角度，对Eden区域继续进行划分，JVM为每个线程分配了一个私有缓存区域，它包含在Eden空间内。 多线程同时分配内存时，使用TLAB可以避免一系列的非线程安全问题，同时还能够提升内存分配的吞吐量，因此我们可以将这种内存分配方式称之为快速分配策略。 据我所知所有OpenJDK衍生出来的JVM都提供了TLAB的设计。 TLAB再说明 尽管不是所有的对象实例都能够在TLAB中成功分配内存，但JVM确实是将TLAB作为内存分配的首选。 在程序中，开发人员可以通过选项-XX:TLAB设置是否开启TLAB空间。 默认情况下，TLAB空间的内存非常小，**仅占有整个Eden空间的1%**，当然我们可以通过选项-XX:TLABWasteTargetPercent 设置TLAB空间所占用Eden空间的百分比大小。 一旦对象在TLAB空间分配内存失败时，JVM就会尝试着通过使用加锁机制确保数据操作的原子性，从而直接在Eden空间中分配内存。 9. 小结堆空间的参数设置Oracle官网 -XX:+PrintFlagsInitial :查看所有的参数的默认初始值 -XX:+PrintFlagsFinal:查看所有的参数的最终值(可能会存在修改,不再是初始值) 具体查看某个参数的指令: jps: 查看当前运行中的进程 jinfo -flag 参数名 进程id -Xms:初始堆空间内存( 默认为物理内存的1&#x2F;64) -Xmx:最大堆空间内存(默认为物理内存的1&#x2F;4) -Xmn:设置新生代的大小。(初始值及最大值) -XX:NewRatio：配置新生代与老年代在堆结构的占比 -XX:SurvivorRatio:设置新生代中Eden和sO&#x2F;S1空间的比例 -XX:MaxTenuringThreshold:设置新生代垃圾的最大年龄 -XX:+PrintGCDetails:输出详细的GC处理日志 打印gc简要信息:1.-XX:+PrintGC 2.-verbose:gc -XX:HandlePromotionFailure:是否设置空间分配担保 10 堆是分配对象的唯一选择吗在《深入理解Java虚拟机》中关于Java堆内存有这样一段描述:随着JIT编译期的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。 在Java虚拟机中，对象是在Java堆中分配内存的，这是一个普遍的常识。但是，有一种特殊情况，那就是如果经过逃逸分析(Escape Analysis) 后发现，一个对象并没有逃逸出方法的话，那么就可能被优化成栈上分配。这样就无需在堆上分配内存，也无须进行垃圾回收了。这也是最常见的堆外存储技术。 此外，前面提到的基于openJDK深度定制的TaoBaoVM，其中创新的GCIH (GCinvisible heap) 技术实现off-heap，将生命周期较长的Java对象从heap中移至heap外，并且GC不能管理GCIH内部的Java对象，以此达到降低GC的回收频率和提升GC的回收效率的目的。 如何将堆上的对象分配到栈，需要使用逃逸分析手段。 这是一种可以有效减少Java程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。 通过逃逸分析，Java Hotspot编译器能够分析出一个新的对象的引用的使用范围从而决定是否要将这个对象分配到堆上。 逃逸分析的基本行为就是分析对象动态作用域: 当一个对象在方法中被定义后，对象只在方法内部使用，则认为没有发生逃逸。 当一个对象在方法中被定义后，它被外部方法所引用，则认为发生逃逸。例如作为调用参数传递到其他地方中。 没有发生逃逸的对象，则可以分配到栈上，随着方法执行的结束，栈空间就被移除。 **结论:**开发中能使用局部变量的，就不要使用在方法外定义。 代码优化同步省略 线程同步的代价是相当高的，同步的后果是降低并发性和性能。在动态编译同步块的时候，JIT编译器可以借助逃逸分析来判断同步块所 使用的锁对象是否只能够被一个线程访问而没有被发布到其他线程。如果没有，那么JIT编译器在编译这个同步块的时候就会取消对这部分代码的同步。这样就能大大提高并发性和性能。这个取消同步的过程就叫同步省略，也叫锁消除。 标量替换 标量(Scalar)是指一个无法再分解成更小的数据的数据。Java中的原始数据类型就是标量 相对的，那些还可以分解的数据叫做聚合量(Aggregate) ，Java中的对象就是聚合量，因为他可以分解成其他聚合量和标量。 在JIT阶段，如果经过逃逸分析，发现一个对象不会被外界访问的话，那么经过JIT优化，就会把这个对象拆解成若干个其中包含的若干个成员变量来代替。这个过程就是标量替换。 标量替换参数设置: 参数-XX: +EliminateAllocations:开启了标量替换(默认打开)，允许将对象打散分配在栈上 小结 年轻代是对象的诞生、成长、消亡的区域，-一个对象在这里产生、应用，最后被垃圾回收器收集、结束生命。 老年代放置长生命周期的对象，通常都是从Survivor区域筛选拷贝过来的Java对象。当然，也有特殊情况，我们知道普通的对象会被分配在TLAB_上;如果对象较大，JVM会试图直接分配在Eden其他位置上;如果对象太大，完全无法在新生代找到足够长的连续空闲空间，JVM就会直接分配到老年代。 当GC只发生在年轻代中，回收年轻代对象的行为被称为MinorGC。当GC发生在老年代时则被称为MajorGC或者Ful1GC。- .般的，MinorGC 的发生频率要比MajorGC高很多，即老年代中垃圾回收发生的频率将大大低于年轻代。","categories":[{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://falser101.github.io/tags/jvm/"}]},{"title":"本地方法以及本地方法栈","slug":"2021/05.本地方法以及本地方法栈","date":"2021-02-12T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2021/02/13/2021/05.本地方法以及本地方法栈/","permalink":"https://falser101.github.io/2021/02/13/2021/05.%E6%9C%AC%E5%9C%B0%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E6%9C%AC%E5%9C%B0%E6%96%B9%E6%B3%95%E6%A0%88/","excerpt":"","text":"本地方法以及本地方法栈 什么是本地方法?简单地讲，一个Native Method就是一个Java调用非Java代码的接口。一个Native Method是 这样- -一个Java方法:该方法的实现由非Java语言实现，比如C。这个特征并非Java所特有，很多其它的编程语言都有这一机制，比如在C++中，你可以用extern “C” 告知C++编译器去调用一个C的函数。 “A native method is a Java method whose implementation isprovided by non-java code.” 在定义一个native method时，并不提供实现体(有些像定义一个Javainterface)，因为其实现体是由非java语言在外面实现的。本地接口的作用是融合不同的编程语言为Java所用，它的初衷是融合C&#x2F;C++程序。 为什么要使用Java使用起来非常方便，然而有些层次的任务用Java实现起来不容易，或者我们对程序的效率很在意时，问题就来了。 与Java环境外交互: 有时Java应用需要与Java外面的环境交互，这是本地方法存在的主要原因。你可以想想Java需要与一些底层 系统，如操作系统或某些硬件交换信息时的情况。本地方法正是这样一种交流机制: 它为我们提供了一个非常简洁的接口,而且我们无需去了解Java应用之外的繁琐的细节。 本地方法栈 Java虚拟机栈用于管理Java方法的调用， 而本地方法栈用于管理本地方法的调用。 本地方法栈，也是线程私有的。 允许被实现成固定或者是可动态扩展的内存大小。 (在内存溢出方面是相同的) 如果线程请求分配的栈容量超过本地方法栈允许的最大容量，Java虚拟机将会抛出一个stackoverflowError异常。 如果本地方法栈可以动态扩展，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程时没有足够的内存去创建对应的本地方法栈，那么Java虛拟机将会抛出一个outofMemoryError 异常。 本地方法是使用c语言实现的。 它的具体做法是Native Method Stack中登记native方法，在Execution Engine执行时加载本地方法库。","categories":[{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://falser101.github.io/tags/jvm/"}]},{"title":"虚拟机栈","slug":"2021/04.虚拟机栈","date":"2021-02-11T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2021/02/12/2021/04.虚拟机栈/","permalink":"https://falser101.github.io/2021/02/12/2021/04.%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%A0%88/","excerpt":"","text":"虚拟机栈概述 虚拟机栈概述虚拟机出现的背景由于跨平台性的设计，Java的指令都是根据栈来设计的。不同平台CPU架构不同，所以不能设计为基于寄存器的。优点：跨平台，指令集小，编译器容易实现缺点：性能下降，实现同样的功能需要更多的指令。 基本内容 Java虚拟机栈是什么? Java虚拟机栈(Java Virtual Machine Stack) ，早期也叫Java栈。每个线程在创建时都会创建一个虚拟机栈，其内部保存一个个的栈帧(Stack Frame) ，对应着一次次的Java方法调用。 是线程私有的 生命周期： 生命周期和线程一致。 作用： 主管Java程序的运行，它保存方法的局部变量、部分结果，并参与方法的调用和返回。 优点： 栈是一种快速有效的分配存储方式，访问速度仅次于程序计数器 对于栈来说不存在垃圾回收问题，存在OOM JVM直接对Java栈的操作只有两个: - 每个方法执行，伴随着进栈(入栈、压栈) - 执行结束后的出栈工作 栈中可能出现的异常Java虚拟机规范允许Java栈的大小是动态的或者是固定不变的。 如果采用固定大小的Java虚拟机栈，那每一个线程的Java虚拟机栈容量可以在线程创建的时候独立选定。如果线程请求分配的栈容量超过Java虚拟机栈允许的最大容量，Java虚拟机将会抛出一个StackOverflowError 异常。 如果Java虚拟机栈可以动态扩展，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程时没有足够的内存去创建对应的虚拟机栈，那Java虚拟机将会抛出一个OutOfMemoryError 异常。 栈是运行时的单位，而堆是存储的单位。栈解决程序的运行问题，即程序如何执行，或者说如何处理数据。堆解决的是数据存储的问题，即数据怎么放、放在哪儿。 设置虚拟机栈的大小-Xss256m 栈的存储单位栈中存储什么？ 每个线程都有自己的栈，栈中的数据都是以栈帧(stack Frame) 的格式存在。 在这个线程上正在执行的每个方法都各自对应一个栈帧(Stack Frame) 。 栈帧是一个内存区块，是一个数据集，维系着方法执行过程中的各种数据信息。 栈运行原理 JVM直接对Java栈的操作只有两个，就是对栈帧的压栈和出栈，遵循“先进后出”&#x2F;“后进先出”原则。 在一条活动线程中，一个时间点上，只会有一个活动的栈帧。即只有当前正在执行的方法的栈帧(栈顶栈帧)是有效的，这个栈帧被称为当前栈帧(Current Frame) ，与当前栈帧相对应的方法就是当前方法(Current Method)，定义这个方法的类就是当前类(Current Class) 。 执行引擎运行的所有字节码指令只针对当前栈帧进行操作。 如果在该方法中调用了其他方法，对应的新的栈帧会被创建出来，放在栈的顶端，成为新的当前帧。 不同线程中所包含的栈帧是不允许存在相互引用的，即不可能在一个栈帧之中引用另外一个线程的栈帧。 如果当前方法调用了其他方法，方法返回之际，当前栈帧会传回此方法的执行结果给前一个栈帧，接着，虚拟机会丢弃当前栈帧，使得前一个栈帧重新成为当前栈帧。 Java方法有两种返回函数的方式，一种是正常的函数返回，使用return指令;另外一种是抛出异常。不管使用哪种方式，都会导致栈帧被弹出。 栈帧内部结构每个栈帧中存储着: 局部变量表（Local variables) 操作数栈（operand stack）（或表达式栈） 动态链接(Dynamic Linking)（或指向运行时常量池的方法引用） 方法返回地址（Return Address）(或方法正常退出或者异常退出的定义) 一些附加信息 局部变量表 局部变量表也被称之为局部变量数组或本地变量表 定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量，这些数据类型包括各类基本数据类型、对象引用(reference） ，以及returnAddress类型。 由于局部变量表是建立在线程的栈上，是线程的私有数据，因此不存在数据安全问题 局部变量表所需的容量大小是在编译期确定下来的，并保存在方法的Code属性的maximum local variables数据项中。在方法运行期间是不会改变局部变量表的大小的。 方法嵌套调用的次数由栈的大小决定。一般来说，栈越大，方法嵌套调用次数越多。对一个函数而言，它的参数和局部变量越多，使得局部变量表膨胀，它的栈帧就越大，以满足方法调用所需传递的信息增大的需求。进而函数调用就会占用更多的栈空间，导致其嵌套调用次数就会减少。 局部变量表中的变量只在当前方法调用中有效。在方法执行时，虚拟机通过使用局部变量表完成参数值到参数变量列表的传递过程。当方法调用结束后，随着方法栈帧的销毁，局部变量表也会随之销毁。 Slot 参数值的存放总是在局部变量数组的index0开始，到数组长度-1的索引结束。 局部变量表，最基本的存储单元是Slot (变量槽) 局部变量表中存放编译期可知的各种基本数据类型(8种)，引用类型(reference)，returnAddress 类型的变量。 在局部变量表里，32位以内的类型只占用一个slot (包括returnAddress类型)，64位的类型(long和double)占用两个slot。 byte、short、char在存储前被转换为int, boolean也被转换为int，0表示false，非0表示true。 long和double则占据两个Slot。 当一个实例方法被调用的时候，它的方法参数和方法体内部定义的局部变量将会按照顺序被复制到局部变量表中的每一个Slot上 如果当前帧是由构造方法或者实例方法创建的，那么该对象引用this将会存放在index为0的slot处，其余的参数按照参数表顺序继续排列。 如果需要访问局部变量表中一个64bit的局部变量值时，只需要使用前一个索引即可。(比如:访问long或double类型变量) 栈帧中的局部变量表中的槽位是可以重用的，如果一个局部变量过了其作用域，那么在其作用域之后申明的新的局部变量就很有可能会复用过期局部变量的槽位，从而达到节省资源的目的。 补充 在栈帧中，与性能调优关系最为密切的部分就是前面提到的局部变量表。在方法执行时，虚拟机使用局部变量表完成方法的传递。 局部变量表中的变量也是重要的垃圾回收根节点，只要被局部变量表中直接或间接引用的对象都不会被回收。 操作数栈 每一个独立的栈帧中除了包含局部变量表以外，还包含一个后进先出(Last-In-First-Out)的操作数栈，也可以称之为表达式栈(Expression Stack)。 操作数栈，在方法执行过程中，根据字节码指令，往栈中写入数据或提取数据，即入栈(push) &#x2F;出栈(pop)。 某些字节码指令将值压入操作数栈，其余的字节码指令将操作数取出栈。使用它们后再把结果压入栈。 比如:执行复制、交换、求和等操作 操作数栈，主要用于保存计算过程的中间结果，同时作为计算过程中变量临时的存储空间。 操作数栈就是JVM执行引擎的一个工作区，当一个方法刚开始执行的时候，一个新的栈帧也会随之被创建出来，这个方法的操作数栈是空的。 每一个操作数栈都会拥有一个明确的栈深度用于存储数值，其所需的最大深度在编译期就定义好了，保存在方法的Code属性中，为max_stack的值。 栈中的任何一个元素都是可以任意的Java数据类型。 32bit的类型占用一个栈单位深度 64bit的类型占用两个栈单位深度 操作数栈并非采用访问索引的方式来进行数据访问的，而是只能通过标准的入栈(push)和出栈(pop)操作来完成一次数据访问。 如果被调用的方法带有返回值的话，其返回值将会被压入当前栈帧的操作数栈中，并更新PC寄存器中下一条需要执行的字节码指令。 操作数栈中元素的数据类型必须与字节码指令的序列严格匹配，这由编译器在编译器期间进行验证，同时在类加载过程中的类检验阶段的数据流分析阶段要再次验证。 另外，我们说Java虚拟机的解释引擎是基于栈的执行引擎，其中的栈指的就是操作数栈。 问题i++ 和 ++i的区别？i++ 先赋值再+1，++i 先+1再赋值 栈顶缓存技术动态链接（指向运行时常量池的方法引用） 每一个栈帧内部都包含一个指向运行时常量池中该栈帧所属方法的引用。包含这个引用的目的就是为了支持当前方法的代码能够实现动态链接（Dynamic Linking） 。比如： invokedynamic指令 在Java源文件被编译到字节码文件中时，所有的变量和方法引用都作为符号引用（Symbolic Reference）保存在class文件的常量池里。比如：描述一个方法调用了另外的其他方法时，就是通过常量池中指向方法的符号引用来表示的，那么动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用。 方法的调用:解析与分派在JVM中，将符号引用转换为调用方法的直接引用与方法的绑定机制相关。 静态链接： 当一个字节码文件被装载进JVM内部时，如果被调用的目标方法在编译期可知，且运行期保持不变时。这种情况下将调用方法的符号引用转换为直接引用的过程称之为静态链接。 动态链接： 如果被调用的方法在编译期无法被确定下来，也就是说，只能够在程序运行期将调用方法的符号引用转换为直接引用，由于这种引用转换过程具备动态性，因此也就被称之为动态链接。 对应的方法的绑定机制为：早期绑定（Early Binding）和晚期绑定（Late Binding） 。绑定是一个字段、方法或者类在符号引用被替换为直接引用的过程，这仅仅发生一次。 早期绑定： 早期绑定就是指被调用的目标方法如果在编译期可知，且运行期保持不变时，即可将这个方法与所属的类型进行绑定，这样一来，由于明确了被调用的目标方法究竟是哪一个，因此也就可以使用静态链接的方式将符号引用转换为直接引用。 晚期绑定： 如果被调用的方法在编译期无法被确定下来，只能够在程序运行期根据实际的类型绑定相关的方法，这种绑定方式也就被称之为晚期绑定。 虚方法和非虚方法虚拟机中提供了以下几条方法调用指令： 普通调用指令： invokestatic：调用静态方法，解析阶段确定唯一方法版本 invokespecial：调用init()方法，私有及父类方法，解析阶段确定唯一方法版本 invokevirtual：调用所有虚方法 invokeinterface：调用接口方法 动态调用指令： invokedynamic：动态解析出需要调用的方法，然后执行 前四条指令固化在虚拟机内部，方法的调用执行不可人为干预，而invokedynamic指令则支持由用户确定方法版本。其中invokestatic指令和invokespecial指令调用的方法称为非虚方法，其余的（final修饰的除外）称为虚方法。 虚方法表在面向对象的编程中，会很频繁的使用到动态分派，如果在每次动态分派的过程中都要重新在类的方法元数据中搜索合适的目标的话就可能影响到执行效率。因此，为了提高性能， JVM采用在类的方法区建立一个虚方法表（virtual method table） （非虚方法不会出现在表中）来实现。使用索引表来代替查找。每个类中都有一个虚方法表，表中存放着各个方法的实际入口。那么虚方法表什么时候被创建？虚方法表会在类加载的链接阶段被创建并开始初始化，类的变量初始值准备完成之后， JVM会把该类的方法表也初始化完毕。 方法返回地址 存放调用该方法的pc寄存器的值。 一个方法的结束，有两种方式: ➢正常执行完成 ➢出现未处理的异常，非正常退出 无论通过哪种方式退出，在方法退出后都返回到该方法被调用的位置。方法正常退出时，调用者的pc计数器的值作为返回地址，即调用该方法的指令的下一条指令的地址。而通过异常退出的，返回地址是要通过异常表来确定，栈帧中一般不会保存这部分信息。 当一个方法开始执行后，只有两种方式可以退出这个方法: 执行引擎遇到任意一个方法返回的字节码指令(return)，会有返回值传递给，上层的方法调用者，简称正常完成出口; ➢一个方法在正常调用完成之后究竟需要使用哪一个返回指令还需要根据方法返回值的实际数据类型而定。 ➢在字节码指令中，返回指令包含ireturn (当返回值是boolean、 byte、char.short和int类型时使用)、lreturn、 freturn、 dreturn以及areturn，另外还有一个return指令供声明为void的方法、实例初始化方法、类和接0的初始化方法使用。 在方法执行的过程中遇到了异常(Exception) ，并且这个异常没有在方法内进行处理，也就是只要在本方法的异常表中没有搜索到匹配的异常处理器，就会导致方法退出。简称异常完成出口。方法执行过程中抛出异常时的异常处理，存储在一个 异常处理表，方便在发生异常的时候找到处理异常的代码。 一些附加信息栈帧中还允许携带与Java虛拟机实现相关的一些附加信息。例如 对程序调试提供支持的信息。 栈的相关面试题 举例栈溢出的情况? (StackOverflowError) 通过-Xss设置栈的大小，如果申请不到内存时，内存溢出OOM 调整栈大小，就能保证不出现溢出吗? 不能 分配的栈内存越大越好吗? 不是，整个内存是有限的，可能会造成可用线程数减少 垃圾回收是否会涉及到虚拟机栈? 不会 方法中定义的局部变量是否线程安全? 12345678//s1的声明方式是线程安全的public static void method1()&#123; //StringBuilder:线程不安全 StringBuilder s1 = new StringBuilder(); s1.append(&quot;a&quot;); s1.append(&quot;b&quot;);//...&#125; 12345//sBuilder的操作过程:是线程不安全的public static void method2(StringBuilder sBuilder)&#123; sBuilder.append(&quot;a&quot;); sBuilder.append(&quot;b&quot;);//... 1234567//s1的操作:是线程不安全的public static StringBuilder method3()&#123; StringBuilder s1 = new StringBuilder(); s1.append(&quot;a&quot;); s1.append(&quot;b&quot;); return s1;&#125; 1234567//s1的操作:是线程安全的public static String method3()&#123; StringBuilder s1 = new StringBuilder(); s1.append(&quot;a&quot;); s1.append(&quot;b&quot;); return s1.toString();&#125;","categories":[{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://falser101.github.io/tags/jvm/"}]},{"title":"程序计数器","slug":"2021/03.程序计数器","date":"2021-02-10T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2021/02/11/2021/03.程序计数器/","permalink":"https://falser101.github.io/2021/02/11/2021/03.%E7%A8%8B%E5%BA%8F%E8%AE%A1%E6%95%B0%E5%99%A8/","excerpt":"","text":"介绍 JVM中的程序计数寄存器(Program Counter Register) 中，Register的命名源于CPU的寄存器，寄存器存储指令相关的现场信息。CPU 只有把数据装载到寄存器才能够运行。这里并非是广义上所指的物理寄存器，或许将其翻译为PC计数器(或指令计数器)会更加贴切(也称为程序钩子)，并且也不容易引起一些不必要的误会。JVM中的PC寄存器是对物理PC寄存器的一种抽象模拟。 它是一块很小的内存空间，几乎可以忽略不记。也是运行速度最快的存储区域。 在JVM规范中，每个线程都有它自己的程序计数器，是线程私有的，生命周期与线程的生命周期保持一致。 任何时间一个线程都只有一个方法在执行，也就是所谓的当前方法。程序计数器会存储当前线程正在执行的Java方法的JVM指令地址;或者, 如果是在执行native方法，则是未指定值(undefined)。 它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令。 它是唯一一个在Java虚拟机规范中没有规定任何OutOtMemoryError情况的区域。 作用 PC寄存器用来存储指向下一条指令的地址，也即将要执行的指令代码。由执行引擎读取下一条指令。 常见面试题使用PC寄存器存储字节码指令地址有什么用呢?为什么使用PC寄存器记录当前线程的执行地址呢？答：因为CPU需要不停的切换各个线程，这时候切换回来以后，就得知道接着从哪开始继续执行JVM的字节码解释器就需要通过改变PC寄存器的值来明确下一条应该执行什么样的字节码指令。 PC寄存器为什么设定为线程私有 答：我们都知道所谓的多线程在-一个特定的时间段内只会执行其中某一个线程的方法，CPU会不停地做任务切换，这样必然导致经常中断或恢复，如何保证分毫无差呢?为了能够准确地记录各个线程正在执行的当前字节码指令地址，最好的办法自然是为每一个线程都分配一个PC寄存器，这样-来各个线程之间便可以进行独立计算，从而不会出现相互干扰的情况。 由于CPU时间片轮限制，众多线程在并发执行过程中，任何一个确定的时刻，一个处理器或者多核处理器中的一个内核，只会执行某个线程中的一条指令。 这样必然导致经常中断或恢复，如何保证分毫无差呢?每个线程在创建后，都会产生自己的程序计数器和栈帧，程序计数器在各个线程之间互不影响。","categories":[{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://falser101.github.io/tags/jvm/"}]},{"title":"运行时数据区概述及线程","slug":"2021/02.运行时数据区概述及线程","date":"2021-02-09T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2021/02/10/2021/02.运行时数据区概述及线程/","permalink":"https://falser101.github.io/2021/02/10/2021/02.%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E6%A6%82%E8%BF%B0%E5%8F%8A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"概述内存是非常重要的系统资源，是硬盘和CPU的中间仓库及桥梁，承载着操作系统和应用程序的实时运行。JVM内存布局规定了Java在运行过程中内存申请、分配、管理的策略，保证了JVM的高效稳定运行。不同的JVM对于内存的划分方式和管理机制存在着部分差异。结合JVM虚拟机规范，来探讨一下经典的JVM内存布局。 Java虚拟机定义了若干种程序运行期间会使用到的运行时数据区，其中有一些会随着虚拟机启动而创建，随着虚拟机退出而销毁。另外一些则是与线程一一对应的，这些与线程对应.的数据区域会随着线程开始和结束而创建和销毁。 ➢每个线程：独立包括程序计数器、栈、本地栈。➢线程间共享：堆、堆外内存(永久代或元空间、代码缓存) 线程 线程是一个程序里的运行单元。JVM允许一个应用有多个线程并行的执行。 在Hotspot JVM里， 每个线程都与操作系统的本地线程直接映射。 当一个Java线程准备好执行以后，此时一个操作系统的本地线程也同时创建。Java线程执行终止后，本地线程也会回收。 操作系统负责所有线程的安排调度到任何一个可用的CPU上。一旦本地线程初始化成功，它就会调用Java线程中的run()方法。 JVM系统线程如果你使用jconsole或者是任何一个调试工具，都能看到在后台有许多线程在运行。这些后台线程不包括调用public static void main (String[])的main线程以及所有这个main线程自己创建的线程。 这些主要的后台系统线程在Hotspot JVM里 主要是以下几个: 虚拟机线程:这种线程的操作是需要JVM达到安全点才会出现。这些操作必须在不同的线程中发生的原因是他们都需要JVM达到安全点，这样堆才不会变化。这种线程的执行类型包括”stop-the-world”的垃圾收集，线程栈收集，线程挂起以及偏向锁撤销。 周期任务线程:这种线程是时间周期事件的体现(比如中断)，他们一般用于周期性操作的调度执行。 GC线程:这种线程对在JVM里不同种类的垃圾收集行为提供了支持。 编译线程:这种线程在运行时会将字节码编译成到本地代码。 信号调度线程:这种线程接收信号并发送给JVM，在它内部通过调用适当的方法进行处理。","categories":[{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://falser101.github.io/tags/jvm/"}]},{"title":"类加载子系统","slug":"2021/01.类加载子系统","date":"2021-02-08T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2021/02/09/2021/01.类加载子系统/","permalink":"https://falser101.github.io/2021/02/09/2021/01.%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%AD%90%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"类的加载器和类加载过程 类加载器子系统负责从文件系统或者网络中加载Class文件，class文件在文件开头有特定的文件标识。 ClassLoader之负责Class文件的加载，至于它是否可以运行，则由Execution Engine决定 加载的类信息，存放于一块称为方法区的内存空间，除了类的信息外，方法区中还会存放运行时常量池信息，可能还包括字符串字面量和数字常量（这部分常量信息是Class文件中常量池部分的内存映射） 类加载器ClassLoader角色 class file存放在本地硬盘上，可以理解为设计师画在纸上的模板，而最终这个模板在执行的时候是要加载到JVM中来根据这个文件实例化个一模一样的实例。 class file加载到JVM中，被称为DNA元数据模板，放在方法区 在.class文件-&gt; JVM-&gt; 最终成为元数据模板，这个过程需要一个运输工具（类装载器 Class Loader），扮演一个快递员的角色。 类的加载过程 加载： 通过一个类的全限定名获取定义此类的二进制字节流 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口 加载.class文件的方式有 本地系统 jar包 运行时计算生成，使用最多的是：动态代理 其他文件生成：jsp …… 链接（Linking)： 验证： 目的在于确保Class文件的字节流中包含信息符合当前虚拟机要求，保证被加载类的正确性，不会危害虚拟机自身安全 主要包括四种验证：文件格式验证，元数据验证，字节码验证，符号引用验证（CA FE BA BE） 准备： 为类变量分配内存并且设置该类变量的默认初始值，即零值 这里不包含用final修饰的static，因为final在编译的时候就会分配，准备阶段会显式初始化 不会为实力变量分配初始化，类变量会分配在方法区中，而实例变量是会随着对象一起分配到Java堆中。 解析： 将常量池内的符号引用转换为直接引用的过程。 事实上，解析操作往往会伴随着JVM在执行完初始化之后再执行。 符号引用就是一组符号来描述所引用的目标，符号引用的字面量形式明确定义在Class文件格式中。直接引用就是直接指向目标的指针，相对偏移量或一个间接定位到目标的句柄 解析动作主要针对类或接口，字段，类方法，接口方法，方法类型等。对应常量池中的CONSTANT_Class_info，CONSTANT_Fieldref_info，CONSTANT_Methodref_info等。 初始化： 初始化阶段就是执行类构造器方法&lt;clinit&gt;()的过程 此方法不需要定义，是javac编译器自动收集类中的所有类变量的赋值动作和静态代码块中的语句合并而来。（无静态变量和静态方法，静态代码块等就不会执行&lt;clinit&gt;()） 构造器方法中指令按语句在源文件中出现的顺序执行。 &lt;clinit&gt;()不同于类的构造器。（关联：构造器是虚拟机视角下的&lt;init&gt;()） 若该类有父类，JVM会保证子类的&lt;clinit&gt;()执行前，父类的&lt;clinit&gt;()已经执行完毕 虚拟机必须保证一个类的&lt;clinit&gt;()方法在多线程下被同步加锁(保证只加载一次) 类加载器的分类 JVM支持两种类型的类加载器，分别为引导类加载器（Bootstrap ClassLoader）和自定义类加载器（User-Defined ClassLoader） 从概念上来讲，自定义类加载器一般指的是程序中有开发人员自定义的一类类加载器，但是Java虚拟机规范却没有这么定义，而是将所有派生于抽象类ClassLoader的类加载器都划分为自定义类加载器。 无论类加载器的类型如何划分，在程序中我们最常见的类加载器始终只有3个，如下所示：这里四者的关系是包含关系，不是上下级关系，也不是父子继承关系。 虚拟机自带的加载器 启动类加载器（引导类加载器，Bootstrap classLoader) 这个类加载使用c&#x2F;C++语言实现的，嵌套在JVM内部。 它用来加载Java的核心库（JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;rt.jar、resources.jar或sun. boot.class.path路径下的内容），用于提供JVM自身需要的类 并不继承自java.lang.classLoader，没有父加载器。加载扩展类和应用程序类加载器，并指定为他们的父类加载器。 出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类 用户自定义的类加载器 为什么要自定义类加载器? 隔离加载类 修改类加载的方式 扩展加载源 防止源码泄漏 用户自定义类加载器实现步骤： 开发人员可以通过继承抽象类java.lang.classLoader类的方式，实现自己的类加载器，以满足一些特殊的需求 在JDK1.2之前，在自定义类加载器时，总会去继承classLoader类并重写loadclass()方法，从而实现自定义的类加载类，但是在JDK1.2之后已不再建议用户去覆盖loadclass()方法，而是建议把自定义的类加载逻辑写在findclass()方法中 在编写自定义类加载器时，如果没有太过于复杂的需求，可以直接继承URLClassLoader类，这样就可以避免自己去编写findclass()方法及其获取字节码流的方式，使自定义类加载器编写更加简洁。 获取ClassLoader的途径 方式一：获取当前类的ClassLoaderclass.getClassLoader() 方式二：获取当前线程上下文的ClassLoaderThread。currentThread().getContextClassLoader() 方式三：获取系统的ClassLoaderClassLoader.getSystemClassLoader() 方式四：获取调用者的ClassLoaderDriverManager.getCallerClassLoader() 双亲委派机制工作原理 如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行; 如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归,请求最终将到达顶层的启动类加载器; 如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。 优势 避免类的重复加载 保护程序安全，防止核心API被随意篡改 自定义类: java.lang.string 自定义类: java.lang.shkstart 抛出异常: java.lang.securityException:Prohibited package name: java.lang 沙箱安全机制自定义String类，但是在加载自定义string类的时候会率先使用引导类加载器加载，而引导类加载器在加载的过程中会先加载jdk自带的文件(rt.jar包中java\\lang\\String.class)，报错信息说没有main方法，就是因为加载的是rt.jar包中的String类。这样可以保证对java核心源代码的保护，这就是沙箱安全机制。 在JVM中表示两个class对象是否为同一个类存在两个必要条件: 类的完整类名必须一致，包括包名。 加载这个类的classLoader(指classLoader实例对象)必须相同。 换句话说，在JVM中，即使这两个类对象(class对象)来源同一个class文件，被同一个虚拟机所加载，但只要加载它们的classLoader实例对象不同，那么这两个类对象也是不相等的。","categories":[{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://falser101.github.io/tags/jvm/"}]},{"title":"JVM相关参数","slug":"2021/09.JVM相关参数","date":"2020-12-07T16:00:00.000Z","updated":"2025-06-16T07:17:39.757Z","comments":true,"path":"2020/12/08/2021/09.JVM相关参数/","permalink":"https://falser101.github.io/2020/12/08/2021/09.JVM%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0/","excerpt":"","text":"随便整理的一些JVM相关 Oracle的JVM官方文档 远程debug参数：1-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8000 指定虚拟机堆内存大小：123-Xmx20m Java Heap最大值,默认值为物理内存的1/4-Xms20m Java Heap初始值,Server端JVM最好将-Xms和-Xmx设为相同值,开发测试机JVM可以保留默认值-Xmn20m Java Heap 新生代大小 指定虚拟机栈内存大小：1-Xss20m 每个线程的Stack大小,不熟悉最好保留默认值 常见的GC日志分析工具 GCViewer GCEasy GCLogViewer GCHisto Hpjmeter 参数及其默认值 描述 -XX:LargePageSizeInBytes&#x3D;4m 设置用于Java堆的大页面尺寸 -XX:MaxHeapFreeRatio&#x3D;70 GC后java堆中空闲量占的最大比例 -XX:MaxNewSize&#x3D;size 新生成对象能占用内存的最大值 -XX:MaxPermSize&#x3D;64m 老年代对象能占用内存的最大值 -XX:MinHeapFreeRatio&#x3D;40 GC后java堆中空闲量占的最小比例 -XX:NewRatio&#x3D;2 新生代内存容量与老生代内存容量的比例 -XX:NewSize&#x3D;2.125m 新生代对象生成时占用内存的默认值 -XX:ReservedCodeCacheSize&#x3D;32m 保留代码占用的内存容量 -XX:ThreadStackSize&#x3D;512 设置线程栈大小，若为0则使用系统默认值 -XX:+UseLargePages 使用大页面内存 -XX:+PrintCommandLineFlags 查看命令行相关参数(包含使用的垃圾收集器) jinfo -flag 相关垃圾回收器参数 进程ID 查看默认的垃圾收集器 -XX:+UseSerialGC 指定年轻代和老年代都使用串行收集器 -XX:ParallelGCThreads 限制线程数量，默认开启和cpu数据相同的线程数，cpu大于8时，其值等于 3+[5*CPU_COUNT]&#x2F;8 -XX:+UseParallelGC 手动指定年轻代使用Parallel并行收集器执行内存回收任务（默认开启年轻代使用ParallelOld GC） -XX:+UseParallelOldGC 手动指定老年代使用并行回收收集器（默认开启年轻代使用Parallel GC） -XX:MaxGCPauseMillis 设置垃圾收集器最大的停顿时间（即STW时间）单位为毫秒（慎用）,默认200ms -XX:GCTimeRatio 垃圾收集时间占总时间的比例（1&#x2F;(N+1)）,用于衡量吞吐量的大小 -XX:+UseAdaptiveSizePolicy 设置Parallel Scavenge收集器具有自适应调节机制（各代比例，晋升老年代年龄会自动调整） -XX:+UseConcMarkSweepGC 老年代使用CMS 收集器，年轻代使用parNew 收集器 -XX:CMSlnitiatingOccupanyFraction 设置堆内存使用率的阈值，一旦达到该阈值，便开始进行回收。 -XX:+UseCMSCompactAtFullCollection 指定执行完Full GC后对内存空间进行压缩整理，避免内存碎片的产生 -XX:CMSFullGCsBeforeCompaction 设置执行多少次Full GC后进行内存压缩整理 -XX:ParallelCMSThreads 设置CMS线程数量（默认启动(ParallelCMSThreads+3)&#x2F;4） -XX:G1HeapRegionSize 设置每个Region的大小。值是2的幂，范围是1MB -XX:InitiatingHeapOccupancyPercent 设置触发并发GC周期的Java堆占用率阈值。超过就触发GC，默认45 -Xloggc:&#x2F;path&#x2F;to&#x2F;gc.log 把gc日志放到指定目录","categories":[{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://falser101.github.io/tags/jvm/"}]}],"categories":[{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/categories/linux/"},{"name":"k8s","slug":"k8s","permalink":"https://falser101.github.io/categories/k8s/"},{"name":"java","slug":"java","permalink":"https://falser101.github.io/categories/java/"},{"name":"frontend","slug":"frontend","permalink":"https://falser101.github.io/categories/frontend/"},{"name":"golang","slug":"golang","permalink":"https://falser101.github.io/categories/golang/"}],"tags":[{"name":"kde","slug":"kde","permalink":"https://falser101.github.io/tags/kde/"},{"name":"linux","slug":"linux","permalink":"https://falser101.github.io/tags/linux/"},{"name":"监控告警","slug":"监控告警","permalink":"https://falser101.github.io/tags/%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/"},{"name":"waydroid","slug":"waydroid","permalink":"https://falser101.github.io/tags/waydroid/"},{"name":"iptables","slug":"iptables","permalink":"https://falser101.github.io/tags/iptables/"},{"name":"Kubernetes controller-manager","slug":"Kubernetes-controller-manager","permalink":"https://falser101.github.io/tags/Kubernetes-controller-manager/"},{"name":"Kubernetes Scheduler","slug":"Kubernetes-Scheduler","permalink":"https://falser101.github.io/tags/Kubernetes-Scheduler/"},{"name":"Gradle","slug":"Gradle","permalink":"https://falser101.github.io/tags/Gradle/"},{"name":"OAuth2","slug":"OAuth2","permalink":"https://falser101.github.io/tags/OAuth2/"},{"name":"admissionWebhook","slug":"admissionWebhook","permalink":"https://falser101.github.io/tags/admissionWebhook/"},{"name":"golang","slug":"golang","permalink":"https://falser101.github.io/tags/golang/"},{"name":"nginx","slug":"nginx","permalink":"https://falser101.github.io/tags/nginx/"},{"name":"设计模式","slug":"设计模式","permalink":"https://falser101.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"jvm","slug":"jvm","permalink":"https://falser101.github.io/tags/jvm/"}]}